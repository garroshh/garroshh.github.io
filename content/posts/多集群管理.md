---
title: "多集群管理"
date: 2020-05-14T11:47:35+08:00
draft: false

tags: ["Istio", "实验"]
categories: ["Istio"]
---

在多个集群中部署和管理应用，能带来更好的故障隔离性和扩展性。Istio 的多集群模型主要分为两类：多控制面模型和单控制面模型。

由于多控制面模型存在配置规则复杂等问题，而在集群间使用 VPN 直连的单控制面模型对网络的连通性又有较高的要求。所以如下实验

演示单控制平面服务路由的感知方案，这种方案不要求网络扁平，只要求 Pilot 可以访问所有集群的 Kube-apiserver。

![image-20200514114822145](https://cdn.jsdelivr.net/gh/garroshh/figurebed/img/image-20200514114822145.png)

# 1. 实验目标

在两个不同的集群中分别部署同一个服务 helloworld 的不同实例：helloworld-v1 和 helloworld-v2，从客户端程序 sleep 内访问 helloworld

服务的流量能够被路由到两个集群的不同实例。

# 2. 实验演练：

（一）环境准备

（1）准备两个 kubernetes 集群：cluster1 和 cluster2，其中 cluster1 是主集群，即部署 Pilot 等组件的集群：

```bash
kubectl config get-contexts
```

![image-20200514135806109](https://cdn.jsdelivr.net/gh/garroshh/figurebed/2020/image-20200514135806109.png)

（2）存储集群名称到环境变量：

```bash
export CTX_CLUSTER1=$(kubectl config view -o jsonpath='{.contexts[0].name}')
export CTX_CLUSTER2=$(kubectl config view -o jsonpath='{.contexts[1].name}')
echo CTX_CLUSTER1 = ${CTX_CLUSTER1}, CTX_CLUSTER2 = ${CTX_CLUSTER2}
```

（二）主集群配置：

（1）部署 Istio 到 cluster1：

```bash
kubectl create --context=$CTX_CLUSTER1 ns istio-system
kubectl create --context=$CTX_CLUSTER1 secret generic cacerts -n istio-system --from-file=samples/certs/ca-cert.pem --from-file=samples/certs/ca-key.pem --from-file=samples/certs/root-cert.pem --from-file=samples/certs/cert-chain.pem
istioctl manifest apply --context=$CTX_CLUSTER1 \
-f install/kubernetes/operator/examples/multicluster/values-istio-multicluster-primary.yaml
```

查看部署：

```bash
kubectl get pods --context=$CTX_CLUSTER1 -n istio-system
```

![image-20200514135829046](https://cdn.jsdelivr.net/gh/garroshh/figurebed/2020/image-20200514135829046.png)

（2）创建一个入口网关使 cluster2 的服务可以进人 cluster1：

```bash
kubectl apply --context=$CTX_CLUSTER1 -f - <<EOF
apiVersion: networking.istio.io/v1alpha3
kind: Gateway
metadata:
name: cluster-aware-gateway
namespace: istio-system
spec:
selector:
istio: ingressgateway
servers:
- port:
number: 443
name: tls
protocol: TLS
tls:
mode: AUTO_PASSTHROUGH
hosts:
- "*.local"
EOF
```

（3）确定 cluster1 的入口 IP 和端口：

1.将kubectl的当前上下文设置为CTX_CLUSTER1

```bash
export ORIGINAL_CONTEXT=$(kubectl config current-context)
kubectl config use-context $CTX_CLUSTER1
```

2.确定入口 IP 和 端口：

```bash
export INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.spec.ports[?(@.name=="http2")].nodePort}')
export SECURE_INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.spec.ports[?(@.name=="https")].nodePort}')
export INGRESS_HOST=$(kubectl get po -l istio=ingressgateway -n istio-system -o jsonpath='{.items[0].status.hostIP}')
```

3.恢复以前的kubectl上下文：

```bash
kubectl config use-context $ORIGINAL_CONTEXT
unset ORIGINAL_CONTEXT
```

4.打印 INGRESS_HOST 和 SECURE_INGRESS_PORT 的值：

```bash
echo The ingress gateway of cluster1: address=$INGRESS_HOST, port=$SECURE_INGRESS_PORT
```

![image-20200514135859383](https://cdn.jsdelivr.net/gh/garroshh/figurebed/2020/image-20200514135859383.png)

（4）在网状网络配置中更新网关地址。编辑istio ConfigMap：注意，该地址显示在两个位置，第二个位于values.yaml：

```bash
kubectl edit cm -n istio-system --context=$CTX_CLUSTER1 istio
```

![image-20200514135925286](https://cdn.jsdelivr.net/gh/garroshh/figurebed/2020/image-20200514135925286.png)

（三）部署 cluster2：

（1）导出 cluster1 网关地址：

```bash
export LOCAL_GW_ADDR=$(kubectl get po -l istio=ingressgateway -n istio-system -o jsonpath='{.items[0].status.hostIP}') && echo ${LOCAL_GW_ADDR}
```

（2）部署 Istio 到 cluster2：

```bash
kubectl create --context=$CTX_CLUSTER2 ns istio-system
kubectl create --context=$CTX_CLUSTER2 secret generic cacerts -n istio-system --from-file=samples/certs/ca-cert.pem --from-file=samples/certs/ca-key.pem --from-file=samples/certs/root-cert.pem --from-file=samples/certs/cert-chain.pem
CLUSTER_NAME=$(kubectl --context=$CTX_CLUSTER2 config view --minify=true -o jsonpath='{.clusters[].name}')
istioctl manifest apply --context=$CTX_CLUSTER2 \
--set profile=remote \
--set values.global.mtls.enabled=true \
--set values.gateways.enabled=true \
--set values.security.selfSigned=false \
--set values.global.controlPlaneSecurityEnabled=true \
--set values.global.createRemoteSvcEndpoints=true \
--set values.global.remotePilotCreateSvcEndpoint=true \
--set values.global.remotePilotAddress=${LOCAL_GW_ADDR} \
--set values.global.remotePolicyAddress=${LOCAL_GW_ADDR} \
--set values.global.remoteTelemetryAddress=${LOCAL_GW_ADDR} \
--set values.gateways.istio-ingressgateway.env.ISTIO_META_NETWORK="network2" \
--set values.global.network="network2" \
--set values.global.multiCluster.clusterName=${CLUSTER_NAME} \
--set autoInjection.enabled=true
```

（3）等待除 istio-ingressgateway 外的cluster2上的Istio Pod准备就绪，注意：将 cluster1 中的 Istio 控制平面配置为监视cluster2之前，istio-ingressgateway才准备就绪。

```bash
kubectl get pods --context=$CTX_CLUSTER2 -n istio-system -l istio!=ingressgateway
```

![image-20200514140130635](https://cdn.jsdelivr.net/gh/garroshh/figurebed/2020/image-20200514140130635.png)

（4）确定cluster2的入口IP和端口。

1.将kubectl的当前上下文设置为CTX_CLUSTER2

```bash
export ORIGINAL_CONTEXT=$(kubectl config current-context)
kubectl config use-context $CTX_CLUSTER2
```

2.确定入口IP和端口

```bash
export INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.spec.ports[?(@.name=="http2")].nodePort}')
export SECURE_INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.spec.ports[?(@.name=="https")].nodePort}')
export INGRESS_HOST=$(kubectl get po -l istio=ingressgateway -n istio-system -o jsonpath='{.items[0].status.hostIP}')
```

3.恢复以前的 kubectl 上下文

```bash
kubectl config use-context $ORIGINAL_CONTEXT
unset ORIGINAL_CONTEXT
```

4.打印 INGRESS_HOST 和 SECURE_INGRESS_PORT 的值

```bash
echo The ingress gateway of cluster2: address=$INGRESS_HOST, port=$SECURE_INGRESS_PORT
```

![image-20200514140151384](https://cdn.jsdelivr.net/gh/garroshh/figurebed/2020/image-20200514140151384.png)

（5）更新网关地址。编辑istio ConfigMap：

```bash
kubectl edit cm -n istio-system --context=$CTX_CLUSTER1 istio
```

![image-20200514140254573](https://cdn.jsdelivr.net/gh/garroshh/figurebed/2020/image-20200514140254573.png)

（6）准备环境变量以为服务帐户istio-reader-service-account构建n2-k8s-config文件：

```bash
CLUSTER_NAME=$(kubectl --context=$CTX_CLUSTER2 config view --minify=true -o jsonpath='{.clusters[].name}')
SERVER=$(kubectl --context=$CTX_CLUSTER2 config view --minify=true -o jsonpath='{.clusters[].cluster.server}')
SECRET_NAME=$(kubectl --context=$CTX_CLUSTER2 get sa istio-reader-service-account -n istio-system -o jsonpath='{.secrets[].name}')
CA_DATA=$(kubectl get --context=$CTX_CLUSTER2 secret ${SECRET_NAME} -n istio-system -o jsonpath="{.data['ca\.crt']}")
TOKEN=$(kubectl get --context=$CTX_CLUSTER2 secret ${SECRET_NAME} -n istio-system -o jsonpath="{.data['token']}" | base64 --decode)
```

（7）在 Istio 工作目录中创建n2-k8s-config文件：

```yaml
cat <<EOF > n2-k8s-config
apiVersion: v1
kind: Config
clusters:
- cluster:
certificate-authority-data: ${CA_DATA}
server: ${SERVER}
name: ${CLUSTER_NAME}
contexts:
- context:
cluster: ${CLUSTER_NAME}
user: ${CLUSTER_NAME}
name: ${CLUSTER_NAME}
current-context: ${CLUSTER_NAME}
users:
- name: ${CLUSTER_NAME}
user:
token: ${TOKEN}
EOF
```

（8）执行以下命令以添加并标记 cluster2 Kubernetes 的秘钥。执行完这些命令后，cluster1 上的 Istio Pilot 将开始监视 cluster2 的服务和实例，就像对 cluster1 一样。

```bash
kubectl create --context=$CTX_CLUSTER1 secret generic n2-k8s-secret --from-file n2-k8s-config -n istio-system
kubectl label --context=$CTX_CLUSTER1 secret n2-k8s-secret istio/multiCluster=true -n istio-system
```

等待istio-ingressgateway准备就绪：

```bash
kubectl get pods --context=$CTX_CLUSTER2 -n istio-system -l istio=ingressgateway
```

发现一直未就绪：

![image-20200514140338390](https://cdn.jsdelivr.net/gh/garroshh/figurebed/2020/image-20200514140338390.png)

查看 cluster2 ingressgateway 日志：报错找不到 Pilot：

![image-20200514140418675](https://cdn.jsdelivr.net/gh/garroshh/figurebed/2020/image-20200514140418675.png)

查看 cluster1 Pilot 日志：P[ilot 可以感知到 cluser2 上 ingressgateway 的变化：

![image-20200514140517303](https://cdn.jsdelivr.net/gh/garroshh/figurebed/2020/image-20200514140517303.png)

//todo debug 很久很久，一直未发现原因，待后续深入理解交互流程及原理，再来实验。



# 3. 清理

清理 cluster2：

```bash
istioctl manifest generate --context=$CTX_CLUSTER2 \
  --set profile=remote \
  --set values.global.mtls.enabled=true \
  --set values.gateways.enabled=true \
  --set values.security.selfSigned=false \
  --set values.global.controlPlaneSecurityEnabled=true \
  --set values.global.createRemoteSvcEndpoints=true \
  --set values.global.remotePilotCreateSvcEndpoint=true \
  --set values.global.remotePilotAddress=${LOCAL_GW_ADDR} \
  --set values.global.remotePolicyAddress=${LOCAL_GW_ADDR} \
  --set values.global.remoteTelemetryAddress=${LOCAL_GW_ADDR} \
  --set values.gateways.istio-ingressgateway.env.ISTIO_META_NETWORK="network2" \
  --set values.global.network="network2" \
  --set autoInjection.enabled=true | kubectl --context=$CTX_CLUSTER2 delete -f -
kubectl delete --context=$CTX_CLUSTER2 ns sample
unset CTX_CLUSTER2 CLUSTER_NAME SERVER SECRET_NAME CA_DATA TOKEN INGRESS_HOST SECURE_INGRESS_PORT INGRESS_PORT LOCAL_GW_ADDR
```

清理 cluster1：

```bash
istioctl manifest generate --context=$CTX_CLUSTER1 \
  -f install/kubernetes/operator/examples/multicluster/values-istio-multicluster-primary.yaml | kubectl --context=$CTX_CLUSTER1 delete -f -
kubectl delete --context=$CTX_CLUSTER1 ns sample
unset CTX_CLUSTER1
rm n2-k8s-config
```

