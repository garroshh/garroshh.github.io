[{"categories":["Istio"],"content":"为了使系统不出现单点故障，服务需要有多个实例增加冗余来提高高可用性，这就需要负载均衡技术；为了请求处理的高效性，又要求有会话保持功能；重试是服务的容错处理机制；故障注入是对系统鲁棒性的测试方法；熔断限流用于保护服务端且提高整个系统的稳定性。Istio 在不侵入代码的情况下，可以提供以上这些流量治理的技术。 1. 流量负载均衡 在选择集群中的多个实例之间分配流量时，要考虑用最有效的方式利用资源。可以通过设置 DestinationRule 的 spec.trafficPolicy.loadBalance.simple 字段，来选择合适的负载均衡算法。 ","date":"2020-05-13","objectID":"/%E6%9C%8D%E5%8A%A1%E6%B2%BB%E7%90%86/:0:0","tags":["Istio","实验"],"title":"服务治理","uri":"/%E6%9C%8D%E5%8A%A1%E6%B2%BB%E7%90%86/"},{"categories":["Istio"],"content":"1.1. ROUND_ROBIN 模式 ","date":"2020-05-13","objectID":"/%E6%9C%8D%E5%8A%A1%E6%B2%BB%E7%90%86/:1:0","tags":["Istio","实验"],"title":"服务治理","uri":"/%E6%9C%8D%E5%8A%A1%E6%B2%BB%E7%90%86/"},{"categories":["Istio"],"content":"1.1.1. 实验目标 为 advertisement 服务配置 ROUND_ROBIN 算法，期望对 advertisement 服务的请求被平均分配到后端实例。 ","date":"2020-05-13","objectID":"/%E6%9C%8D%E5%8A%A1%E6%B2%BB%E7%90%86/:1:1","tags":["Istio","实验"],"title":"服务治理","uri":"/%E6%9C%8D%E5%8A%A1%E6%B2%BB%E7%90%86/"},{"categories":["Istio"],"content":"1.1.2. 实验演练 （1）将 advertisement 服务扩展到两个实例，确认如下： （2）为 advertisement 服务设置 ROUND_ROBIN 算法的负载均衡： kubectl apply -f chapter-files/traffic-management/dr-advertisement-round-robin.yaml -n weather （3）进入 frontend 容器，对 advertisement 服务发起 6 个请求： kubectl -n weather exec -it frontend-v1-75d4648dc6-fqx9c bash for i in seq 1 6; do curl http://advertisement.weather:3003/ad --silent -w \"Status: %{http_code}\\n\" -o dev/null; done （4）分别查看两个实例的 Proxy 日志，可以看到他们各自收到了 3 个请求，表示对 advertisement 服务发起的 6 个请求被平均分配到了两个后端实例 kubectl -n weather logs advertisement-v1-68d74cc5bd-4x62w -c istio-proxy kubectl -n weather logs advertisement-v1-68d74cc5bd-xr9m6 -c istio-proxy ","date":"2020-05-13","objectID":"/%E6%9C%8D%E5%8A%A1%E6%B2%BB%E7%90%86/:1:2","tags":["Istio","实验"],"title":"服务治理","uri":"/%E6%9C%8D%E5%8A%A1%E6%B2%BB%E7%90%86/"},{"categories":["Istio"],"content":"1.1.3. 工作原理 查看 advertisement 服务的 DestinationRule 配置： kubectl get dr advertisement-dr -oyaml -n weather ","date":"2020-05-13","objectID":"/%E6%9C%8D%E5%8A%A1%E6%B2%BB%E7%90%86/:1:3","tags":["Istio","实验"],"title":"服务治理","uri":"/%E6%9C%8D%E5%8A%A1%E6%B2%BB%E7%90%86/"},{"categories":["Istio"],"content":"1.2. RANDOM 模式 ","date":"2020-05-13","objectID":"/%E6%9C%8D%E5%8A%A1%E6%B2%BB%E7%90%86/:2:0","tags":["Istio","实验"],"title":"服务治理","uri":"/%E6%9C%8D%E5%8A%A1%E6%B2%BB%E7%90%86/"},{"categories":["Istio"],"content":"1.2.1. 实验目标 为 advertisement 服务配置 RANDOM 算法，期望对 advertisement 服务的请求被随机分配到后端实例。 ","date":"2020-05-13","objectID":"/%E6%9C%8D%E5%8A%A1%E6%B2%BB%E7%90%86/:2:1","tags":["Istio","实验"],"title":"服务治理","uri":"/%E6%9C%8D%E5%8A%A1%E6%B2%BB%E7%90%86/"},{"categories":["Istio"],"content":"1.2.2. 实验演练 （1）将 advertisement 服务扩展到两个实例，确认如下： （2）为 advertisement 服务设置 RANDOM 算法的负载均衡： kubectl apply -f chapter-files/traffic-management/dr-advertisement-random.yaml -n weather 查看下发的配置： （3）进入 frontend 容器，对 advertisement 服务发起 6 个请求： kubectl -n weather exec -it frontend-v1-75d4648dc6-fqx9c bash for i in seq 1 6; do curl http://advertisement.weather:3003/ad --silent -w \"Status: %{http_code}\\n\" -o dev/null; done （4）分别查看两个实例的 Proxy 日志，可以看到一个实例收到 2 个请求，另一个实例收到 4 个请求，请求分配不再均匀： kubectl -n weather logs advertisement-v1-68d74cc5bd-4x62w -c istio-proxy kubectl -n weather logs advertisement-v1-68d74cc5bd-xr9m6 -c istio-proxy 2. 会话保持 会话保持是将来自同一客户端的请求始终映射到同一个后端实例中，让请求具有记忆性。会话保持带来的好处是：如果在服务端的缓存中保存着客户端的请求结果， 且同一个客户端始终访问同一个后端实例，就可以一直从缓存中获取数据。Istio 利用一致性哈希算法提供了会话保持功能，也属于负载均衡算法，这种负载均衡只对 HTTP 连接有效。 ","date":"2020-05-13","objectID":"/%E6%9C%8D%E5%8A%A1%E6%B2%BB%E7%90%86/:2:2","tags":["Istio","实验"],"title":"服务治理","uri":"/%E6%9C%8D%E5%8A%A1%E6%B2%BB%E7%90%86/"},{"categories":["Istio"],"content":"2.1. 实验目标 对 advertisement 服务配置会话保持策略，期望将对 advertisement 服务的所有请求都被转发到同一个后端实例。 ","date":"2020-05-13","objectID":"/%E6%9C%8D%E5%8A%A1%E6%B2%BB%E7%90%86/:3:0","tags":["Istio","实验"],"title":"服务治理","uri":"/%E6%9C%8D%E5%8A%A1%E6%B2%BB%E7%90%86/"},{"categories":["Istio"],"content":"2.2. 实验演练 （1）将 advertisement 服务扩展到两个实例，确认如下： （2）为 advertisement 服务设置会话保持模式的负载均衡，根据 Cookie 中的 user 数据得到所使用的哈希值。 kubectl apply -f chapter-files/traffic-management/dr-advertisement-consistenthash.yaml -n weather 查看下发配置： （3）进入 frontend 容器，对 advertisement 服务发起在 Cookie 中携带 user 信息的 6 个请求： kubectl -n weather exec -it frontend-v1-75d4648dc6-fqx9c bash for i in seq 1 6; do curl http://advertisement.weather:3003/ad --cookie “user=tester” --silent -w \"Status: %{http_code}\\n\" -o dev/null; done （4）分别查看两个实例的 Proxy 日志，期望看到一个实例收到这6个请求，说明会话保持策略生效了；实际看到一个实例收到2个请求，另一个实例收到4个请求，反复几次实验，都没有得到期望的结果，实验失败。 kubectl -n weather logs advertisement-v1-68d74cc5bd-4x62w -c istio-proxy kubectl -n weather logs advertisement-v1-68d74cc5bd-xr9m6 -c istio-proxy 3. 故障注入 故障注入是一种软件测试方法，通过在代码中引入故障来发现系统软件中隐藏的 Bug，并且通常与压力测试一起用于验证软件的稳定性。目前， Istio 故障注入功能支持延迟注入和中断注入。 ","date":"2020-05-13","objectID":"/%E6%9C%8D%E5%8A%A1%E6%B2%BB%E7%90%86/:4:0","tags":["Istio","实验"],"title":"服务治理","uri":"/%E6%9C%8D%E5%8A%A1%E6%B2%BB%E7%90%86/"},{"categories":["Istio"],"content":"3.1. 延迟注入 延迟属于时序故障，模仿增加的网络延迟或过载的上游服务。故障配置可以设置在特定条件下对请求注入故障，也可以限制发生请求故障的百分比。 ","date":"2020-05-13","objectID":"/%E6%9C%8D%E5%8A%A1%E6%B2%BB%E7%90%86/:5:0","tags":["Istio","实验"],"title":"服务治理","uri":"/%E6%9C%8D%E5%8A%A1%E6%B2%BB%E7%90%86/"},{"categories":["Istio"],"content":"3.1.1. 实验目标 为 advertisement 服务注入 3 秒的延迟，期望访问 advertisement 服务的返回时间是 3 秒。 ","date":"2020-05-13","objectID":"/%E6%9C%8D%E5%8A%A1%E6%B2%BB%E7%90%86/:5:1","tags":["Istio","实验"],"title":"服务治理","uri":"/%E6%9C%8D%E5%8A%A1%E6%B2%BB%E7%90%86/"},{"categories":["Istio"],"content":"3.1.2. 实验演练 （1）在正常情况下，进入 frontend 容器访问 advertisement 服务，可以看到返回时间远远少于 3 秒，如下： （2）为 advertisement 服务注入 3 秒的延迟调用： kubectl apply -f chapter-files/traffic-management/vs-advertisement-fault-delay.yaml -n weather 查看配置： （3）进入frontend 容器访问 advertisement 服务，查询得到的返回时间是 3 秒，延迟注入成功： （4）验证完成后删除故障策略 kubectl delete -f chapter-files/traffic-management/vs-advertisement-fault-delay.yaml -n weather ","date":"2020-05-13","objectID":"/%E6%9C%8D%E5%8A%A1%E6%B2%BB%E7%90%86/:5:2","tags":["Istio","实验"],"title":"服务治理","uri":"/%E6%9C%8D%E5%8A%A1%E6%B2%BB%E7%90%86/"},{"categories":["Istio"],"content":"3.2. 中断注入 中断注入是模拟上游服务的崩溃失败，通常以 HTTP 地址错误或 TCP 连接失败的形式出现。 ","date":"2020-05-13","objectID":"/%E6%9C%8D%E5%8A%A1%E6%B2%BB%E7%90%86/:6:0","tags":["Istio","实验"],"title":"服务治理","uri":"/%E6%9C%8D%E5%8A%A1%E6%B2%BB%E7%90%86/"},{"categories":["Istio"],"content":"3.2.1. 实验目标 为 advertisement 服务注入 HTTP 500 错误，期望在访问 advertisement 服务时始终返回 “500” 状态码。 ","date":"2020-05-13","objectID":"/%E6%9C%8D%E5%8A%A1%E6%B2%BB%E7%90%86/:6:1","tags":["Istio","实验"],"title":"服务治理","uri":"/%E6%9C%8D%E5%8A%A1%E6%B2%BB%E7%90%86/"},{"categories":["Istio"],"content":"3.2.2. 实验演练 （1）为 advertisement 服务的调用注入 HTTP 500 错误： kubectl apply -f chapter-files/traffic-management/vs-advertisement-fault-abort.yaml -n weather 查看配置： （2）进入 frontend 容器访问 advertisement 服务，返回 “500” 状态码，说明故障注入成功： （3）验证完毕后删除故障策略 kubectl delete -f chapter-files/traffic-management/vs-advertisement-fault-abort.yaml -n weather 4. 超时 程序在长时间不能正常返回时，需要设置超时控制机制，即过了设置的时间就应该返回错误。如果长期处于等待状态，就会浪费资源，甚至引起级联错误导致整个系统不可用。虽然超时配置可以 通过修改程序的代码完成，但是这样不灵活，可以利用 Istio 设置超时参数达到上述目的。 ","date":"2020-05-13","objectID":"/%E6%9C%8D%E5%8A%A1%E6%B2%BB%E7%90%86/:6:2","tags":["Istio","实验"],"title":"服务治理","uri":"/%E6%9C%8D%E5%8A%A1%E6%B2%BB%E7%90%86/"},{"categories":["Istio"],"content":"4.1. 实验目标 如果 forecast 服务处理请求的时间超过 1 秒，则请求端收到超时错误。 ","date":"2020-05-13","objectID":"/%E6%9C%8D%E5%8A%A1%E6%B2%BB%E7%90%86/:7:0","tags":["Istio","实验"],"title":"服务治理","uri":"/%E6%9C%8D%E5%8A%A1%E6%B2%BB%E7%90%86/"},{"categories":["Istio"],"content":"4.2. 实验演练 （1）为 forecast 服务设置 1 秒的超时： kubectl apply -f chapter-files/traffic-management/vs-forecast-timeout.yaml -n weather 查看配置： （2）在浏览器查询天气信息，始终可以看到推荐的信息。服务调用关系如下： frontend v1 → forecast v2 → recommendation v1 为了使 forecast 服务的返回时间多于 1 秒触发超时，给 recommendation 服务注入一段 4 秒的延迟，使 forecast 服务在 1 秒内收不到 recommendation 的响应，不能向 frontend 服务及时返回信息， 从而导致超时报错。为 recommendation 服务注入 4 秒的延迟： kubectl apply -f chapter-files/traffic-management/vs-recommendation-fault-delay.yaml -n weather 查看配置： （3）进入 frontend 容器访问 forecast 服务，返回 “504” 超时错误： （4）验证完毕，删除超时策略 kubectl delete -f chapter-files/traffic-management/vs-recommendation-fault-delay.yaml -n weather kubectl delete -f chapter-files/traffic-management/vs-forecast-timeout.yaml -n weather 5. 重试 服务在网络不稳定的环境中经常会返回错误，这是需要增加重试机制，通过多次尝试返回正确结果。虽然也可以将重试逻辑写在业务代码中，但 Istio 可以让开发人员 通过简单配置就可以完成重试功能，不用去考虑这部分的代码实现，增强服务的鲁棒性。 ","date":"2020-05-13","objectID":"/%E6%9C%8D%E5%8A%A1%E6%B2%BB%E7%90%86/:8:0","tags":["Istio","实验"],"title":"服务治理","uri":"/%E6%9C%8D%E5%8A%A1%E6%B2%BB%E7%90%86/"},{"categories":["Istio"],"content":"5.1. 实验目标 当对 forecast 服务请求失败（返回码为 500）时，请求端自动重试 3 次。 ","date":"2020-05-13","objectID":"/%E6%9C%8D%E5%8A%A1%E6%B2%BB%E7%90%86/:9:0","tags":["Istio","实验"],"title":"服务治理","uri":"/%E6%9C%8D%E5%8A%A1%E6%B2%BB%E7%90%86/"},{"categories":["Istio"],"content":"5.2. 实验演练 （1）为 recommendation 服务注入故障： kubectl apply -f chapter-files/traffic-management/vs-recommendation-fault-abort.yaml -n weather 查看配置： （2）进入 frontend 容器访问一次 forecast 服务，由于 recommendation 被注入错误，导致 forecast 服务也返回 “5xx” 状态码。 （3）查看 forecast 服务 v2 版本的 Proxy 日志，期望看到同一时刻只要 1 次请求记录，实际看到同一时刻有 3 次请求记录，这块比较奇怪 kubectl logs -f forecast-v2-5668689589-fntwt -n weather -c istio-proxy | grep \"GET /weather\" （4）对 forecast 服务设置重试机制： 这里的 retries 表示：如果服务在 1 秒内没有得到正确的返回值，就认为这次请求失败，然后重试 3 次，重试条件是返回码为 “5xx”。 （5）进入 frontend 容器再次访问 forecast 服务： （6）查看 forecast 服务的 Proxy 日志，发现同一时刻有 4 次请求记录（有 3 次是重试的请求）： kubectl logs -f forecast-v2-5668689589-fntwt -n weather -c istio-proxy | grep \"GET /weather\" （7）验证完毕，删除重试策略 kubectl apply -f install/virtual-service-all.yaml -n weather 6. HTTP 重定向 HTTP 重定向能够让单个页面、表单或者整个 Web 应用都跳转到新的 URL 下，该操作可以应用于多种场景：网站维护期间的临时跳转，网站架构改变后为了保持 外部链接继续可用的永久重定向，上传文件时的进度页面等。 ","date":"2020-05-13","objectID":"/%E6%9C%8D%E5%8A%A1%E6%B2%BB%E7%90%86/:10:0","tags":["Istio","实验"],"title":"服务治理","uri":"/%E6%9C%8D%E5%8A%A1%E6%B2%BB%E7%90%86/"},{"categories":["Istio"],"content":"6.1. 实验目标 将对 advertisement 服务的路径 “/ad” 的请求重定向到 “http://advertisement.weather.svc.cluster.local/maintenanced”。 ","date":"2020-05-13","objectID":"/%E6%9C%8D%E5%8A%A1%E6%B2%BB%E7%90%86/:11:0","tags":["Istio","实验"],"title":"服务治理","uri":"/%E6%9C%8D%E5%8A%A1%E6%B2%BB%E7%90%86/"},{"categories":["Istio"],"content":"6.2. 实验演练 （1）设置重定向规则： kubectl apply -f chapter-files/traffic-management/redirect.yaml -n weather 查看配置： HTTP 重定向用来向下游服务发送 “301” 转向响应，并且能够用特定值来替换响应中的认证、主机及 URI 部分。上面的规则会将向 advertisement 服务的 “/ad” 路径 发送的请求重定向到 http://advertisement.weather.svc.cluster.local/maintenanced。 （2）进入 frontend 容器，对 advertisement 服务发起请求，返回 “301” 状态码： （3）清除规则 kubectl apply -f install/virtual-service-all.yaml -n weather 7. HTTP 重写 HTTP 重写用来在 HTTP 请求被转发到目标之前，对请求的内容进行部分改写。 ","date":"2020-05-13","objectID":"/%E6%9C%8D%E5%8A%A1%E6%B2%BB%E7%90%86/:12:0","tags":["Istio","实验"],"title":"服务治理","uri":"/%E6%9C%8D%E5%8A%A1%E6%B2%BB%E7%90%86/"},{"categories":["Istio"],"content":"7.1. 实验目标 在 访问 advertisement 服务时，期望对路径 “/demo” 的访问能够自动重写成对 advertisement 服务的请求。 ","date":"2020-05-13","objectID":"/%E6%9C%8D%E5%8A%A1%E6%B2%BB%E7%90%86/:13:0","tags":["Istio","实验"],"title":"服务治理","uri":"/%E6%9C%8D%E5%8A%A1%E6%B2%BB%E7%90%86/"},{"categories":["Istio"],"content":"7.2. 实验演练 （1）设置重定向规则： kubectl apply -f chapter-files/traffic-management/rewrite.yaml -n weather 查看配置： 在对 advertisement 服务的 API 进行调用之前，Istio 会将 URL 前缀 “/demo” 替换成 “/\"。 （2）进入 frontend 容器，对 advertisement 服务发起请求，如果请求路径不带 “/demo”，则返回 “404” 响应码；再次对 advertisement 服务发起请求，请求路径带 “/demo”，返回成功： （3）清除规则 kubectl apply -f install/virtual-service-all.yaml -n weather 8. 熔断 服务端的 Proxy 会记录调用发生错误的次数，然后根据配置决定是否继续提供服务或者立刻返回错误。使用熔断机制可以保护服务后端不会过载。 ","date":"2020-05-13","objectID":"/%E6%9C%8D%E5%8A%A1%E6%B2%BB%E7%90%86/:14:0","tags":["Istio","实验"],"title":"服务治理","uri":"/%E6%9C%8D%E5%8A%A1%E6%B2%BB%E7%90%86/"},{"categories":["Istio"],"content":"8.1. 实验目标 在对 forecast 服务发起多个并发请求的情况下，为了保护系统整体的可用性，Istio 根据熔断配置会对一部分请求直接返回 “503” 状态码，表示服务处于不可接收请求状态。 另外，在服务被检测到一段时间内发生了多次连续异常后，Istio 会对部分后端实例进行隔离。 ","date":"2020-05-13","objectID":"/%E6%9C%8D%E5%8A%A1%E6%B2%BB%E7%90%86/:15:0","tags":["Istio","实验"],"title":"服务治理","uri":"/%E6%9C%8D%E5%8A%A1%E6%B2%BB%E7%90%86/"},{"categories":["Istio"],"content":"8.2. 实验演练 （1）部署访问 forecast 服务的客户端 fortio，这个程序可以控制连接数、并发数及 HTTP 请求的延迟，安装文件在 Istio 安装包的 sample 目录中。 kubectl apply -f samples/httpbin/sample-client/fortio-deploy.yaml -n weather fortio 也被注入了 Proxy，可通过查看 fortio 客户端中 Proxy 的统计日志验证熔断效果 为了更好的演示效果，建议将 forecast 服务的实例扩展到 5 个 （2）为 forecast 服务配置熔断策略 kubectl apply -f chapter-files/traffic-management/circuit-breaking.yaml -n weather connectionPool 表示如果对 forecast 服务发起超过 3 个 HTTP/1.1 的连接，并且存在 5 个及以上的待处理请求，就会触发熔断机制。 （3）进入 fortio 容器，使用 10 个并发进行 100 次调用触发熔断： kubectl exec -it fortio-deploy-7cb865f87f-grnqg -n weather -c fortio /usr/bin/fortio -- load -c 10 -qps 0 -n 100 -loglevel Warning http://forecast.weather:3002/weather?locate=hangzhou 上面结果表示有 95% 的请求成功，其余部分则被熔断（“503 Service Unavailable” 表示服务处于不可接收请求状态，由 Proxy 直接返回此状态码）。 为了进一步验证测试结果，在 fortio 客户端的 Proxy 客户端的 Proxy 中查看统计信息 upstream_rq_pending_overflow 表明有 5 次调用被标记为熔断。 （4）接下来验证异常检测功能： outlierDetection 表示每 10 秒扫描一次 forecast 服务的后端实例，在连续返回两次网关错误（状态码为502，503 或 504，但不包括 500），实例中的 40% 就会被移出连接池两分钟。 为 forecast 服务设置超时并对 recommendation 服务注入延迟故障，导致所有访问 forecast 服务的请求都返回 “504” 错误 kubectl -n weather apply -f chapter-files/traffic-management/vs-forecast-timeout.yaml kubectl -n weather apply -f chapter-files/traffic-management/vs-recommendation-fault-delay.yaml （5）进入 fortio 容器，使用 5 个并发连接进行 20 次调用，触发连续网关故障异常检测： kubectl -n weather exec -it fortio-deploy-7cb865f87f-grnqg -c fortio /usr/bin/fortio -- load -c 5 -qps 0 -n 20 -loglevel Warning http://forecast.weather:3002/weather?locate=hangzhou 由于没有达到 connectionPool 的限制，不会触发熔断返回 “503”，所有请求都返回 “504 Gateway Timeout”。在 fortio 客户端的 Proxy 中查看异常检测结果： ejections_active: 2 表示有两个服务实例被移除了负载均衡池。两分钟的移除时间过后，再去查看统计结果，之前被移除的两个实例又被移回了负载均衡池。 （6）清除规则 kubectl apply -f install/virtual-service-all.yaml -n weather 9. 限流 限流是一种预防措施，在灾难发生前就对并发访问进行限制。Istio 的速率限制特性可以实现常见的限流功能，即防止来自外部服务的过度调用。衡量指标主要是 QPS，实现方式是计数器方式。Istio 支持 Memquota 适配器和 Redisquota 适配器， 为了方便，实验使用 Memquota 适配器。 ","date":"2020-05-13","objectID":"/%E6%9C%8D%E5%8A%A1%E6%B2%BB%E7%90%86/:16:0","tags":["Istio","实验"],"title":"服务治理","uri":"/%E6%9C%8D%E5%8A%A1%E6%B2%BB%E7%90%86/"},{"categories":["Istio"],"content":"9.1. 普通方式 ","date":"2020-05-13","objectID":"/%E6%9C%8D%E5%8A%A1%E6%B2%BB%E7%90%86/:17:0","tags":["Istio","实验"],"title":"服务治理","uri":"/%E6%9C%8D%E5%8A%A1%E6%B2%BB%E7%90%86/"},{"categories":["Istio"],"content":"9.1.1. 实验目标 对目标服务 advertisement 在上线前进行性能测试，找到此服务最大能承受的性能值，在上线时利用这个性能值设置限流规则，使得在请求达到限制的速率时触发限流。Istio 的限流是直接拒绝多出来的请求，对客户端返回 “429：RESOURCE_EXHAUSTED”。 ","date":"2020-05-13","objectID":"/%E6%9C%8D%E5%8A%A1%E6%B2%BB%E7%90%86/:17:1","tags":["Istio","实验"],"title":"服务治理","uri":"/%E6%9C%8D%E5%8A%A1%E6%B2%BB%E7%90%86/"},{"categories":["Istio"],"content":"9.1.2. 实验演练 （1）验证是否启用了 Istio 策略检查功能： kubectl -n istio-system get cm istio -o jsonpath=\"{@.data.mesh}\" | grep disablePolicyChecks disablePolicyChecks: false 表示开启了策略检查。如果没有开启，可以使用 helm upgrade 更新 disablePolicyChecks 参数，开启策略检查功能。 （2）为 advertisement 服务配置速率限制： kubectl apply -f chapter-files/traffic-management/ratelimiting.yaml vim chapter-files/traffic-management/ratelimiting.yaml apiVersion: config.istio.io/v1alpha2 kind: handler metadata: name: quotahandler namespace: istio-system spec: compiledAdapter: memquota params: quotas: - name: requestcount.instance.istio-system maxAmount: 200 validDuration: 1s overrides: - dimensions: destination: advertisement maxAmount: 4 validDuration: 5s --- apiVersion: config.istio.io/v1alpha2 kind: instance metadata: name: requestcount namespace: istio-system spec: compiledTemplate: quota params: dimensions: source: request.headers[\"x-forwarded-for\"] | \"unknown\" destination: destination.labels[\"app\"] | destination.service.host | \"unknown\" destinationVersion: destination.labels[\"version\"] | \"unknown\" --- apiVersion: config.istio.io/v1alpha2 kind: rule metadata: name: quota namespace: istio-system spec: actions: - handler: quotahandler instances: - requestcount --- apiVersion: config.istio.io/v1alpha2 kind: QuotaSpec metadata: name: request-count namespace: istio-system spec: rules: - quotas: - charge: 1 quota: requestcount --- apiVersion: config.istio.io/v1alpha2 kind: QuotaSpecBinding metadata: name: request-count namespace: istio-system spec: quotaSpecs: - name: request-count namespace: istio-system services: - name: advertisement namespace: weather 从上配置可以看出，Memquota 规定 advertisement 服务在 5 秒内最多能被访问 4 次。 （3）进入 frontend 容器，执行如下命令对 advertisement 服务在 5 秒内发起 5 个请求：可以看到 5 秒限制 4 次请求的速率并未完全匹配 （4）清除规则 kubectl delete -f chapter-files/traffic-management/ratelimiting.yaml ","date":"2020-05-13","objectID":"/%E6%9C%8D%E5%8A%A1%E6%B2%BB%E7%90%86/:17:2","tags":["Istio","实验"],"title":"服务治理","uri":"/%E6%9C%8D%E5%8A%A1%E6%B2%BB%E7%90%86/"},{"categories":["Istio"],"content":"9.2. 条件方式 ","date":"2020-05-13","objectID":"/%E6%9C%8D%E5%8A%A1%E6%B2%BB%E7%90%86/:18:0","tags":["Istio","实验"],"title":"服务治理","uri":"/%E6%9C%8D%E5%8A%A1%E6%B2%BB%E7%90%86/"},{"categories":["Istio"],"content":"9.2.1. 实验目标 Istio 速率限制还可以用于另一种场景：普通用户在使用 advertisement 服务时，只被提供免费的配额，若超过免费配额的请求，则被限制。对于付费的特殊用户会取消速率限制。 ","date":"2020-05-13","objectID":"/%E6%9C%8D%E5%8A%A1%E6%B2%BB%E7%90%86/:18:1","tags":["Istio","实验"],"title":"服务治理","uri":"/%E6%9C%8D%E5%8A%A1%E6%B2%BB%E7%90%86/"},{"categories":["Istio"],"content":"9.2.2. 实验演练 （1）执行如下命令： kubectl apply -f chapter-files/traffic-management/ratelimiting.yaml kubectl apply -f chapter-files/traffic-management/ratelimiting-conditional.yaml 查看配置： 上面的规则利用了 Rule 的 match 字段实现了有条件的速率限制：如果在请求的 cookie 信息中带有 user=tester，就不会执行限流策略。 （2）执行如下命令对 advertisement 服务发起多次请求。 结果如下：对 tester 用户没有配额限制，非 tester 用户受到了速率限制。 （3）清除规则： kubectl delete -f chapter-files/traffic-management/ratelimiting-conditional.yaml kubectl delete -f chapter-files/traffic-management/ratelimiting.yaml 10. 服务隔离 Sidecar 资源的配置支持定义 Sidecar 可访问的服务范围，让用户能够更精确控制 Sidecar 行为。 ","date":"2020-05-13","objectID":"/%E6%9C%8D%E5%8A%A1%E6%B2%BB%E7%90%86/:18:2","tags":["Istio","实验"],"title":"服务治理","uri":"/%E6%9C%8D%E5%8A%A1%E6%B2%BB%E7%90%86/"},{"categories":["Istio"],"content":"10.1. 实验目标 配置 Sidecar 资源使 weather 命名空间下的 httpbin 服务对 default 命名空间下的 sleep 服务不可见，即 sleep.default 只能访问 httpbin.default，不能访问 httpbin.weather。 ","date":"2020-05-13","objectID":"/%E6%9C%8D%E5%8A%A1%E6%B2%BB%E7%90%86/:19:0","tags":["Istio","实验"],"title":"服务治理","uri":"/%E6%9C%8D%E5%8A%A1%E6%B2%BB%E7%90%86/"},{"categories":["Istio"],"content":"10.2. 实验演练 （1）在 default 命名空间下部署 Istio 安装包中的 sleep 和 httpbin 两个服务： kubectl label ns default istio-injection=enabled kubectl apply -f samples/sleep/sleep.yaml kubectl apply -f samples/httpbin/httpbin.yaml 在 weather 命名空间下部署 Istio 安装包中的 httpbin 服务： kubectl apply -f samples/httpbin/httpbin.yaml -n weather （2）进入 sleep.defautl 的 Proxy 查看 clusters，可以看到 httpbin.default 和 httpbin.weather 的信息： 从 sleep.default 容器中分别访问 httpbin.default 和 httpbin.weather，请求均成功： （3）为命名空间 default 配置 Sidecar 资源对象： kubectl apply -f chapter-files/traffic-management/sidecar-demo.yaml 查看配置： 其规则表示在 default 命名空间下 sleep 工作负载对外只能访问 default 和 istio-system 两个命名空间下的服务。 （4）进入 sleep.default 的 Proxy 查看 clusters，只能看到 httpbin.default 的信息，http.weather 信息不见了： 再次在 sleep.default 的容器中访问不同命名空间下的两个服务，httpbin.weather 期望返回 “404”，实际返回 “200”，httpbin.default 仍然能正常响应： 11. 影子测试 查找新代码错误的最佳方法是在实际环境中进行测试。影子测试可以将生产流量复制到目标服务中进行测试，在处理中产生的任何错误都不会对真个系统的性能和可靠性造成影响。 ","date":"2020-05-13","objectID":"/%E6%9C%8D%E5%8A%A1%E6%B2%BB%E7%90%86/:20:0","tags":["Istio","实验"],"title":"服务治理","uri":"/%E6%9C%8D%E5%8A%A1%E6%B2%BB%E7%90%86/"},{"categories":["Istio"],"content":"11.1. 实验目标 测试 forecast 服务的 v2 版本在真实用户访问下的表现，但同时不想影响到终端用户，这时就需要复制一份 forecast 服务的 v1 版本的流量给 forecast 服务的 v2 版本，观察 v2 版本在复制的数据流下的行为 和影响指标。 ","date":"2020-05-13","objectID":"/%E6%9C%8D%E5%8A%A1%E6%B2%BB%E7%90%86/:21:0","tags":["Istio","实验"],"title":"服务治理","uri":"/%E6%9C%8D%E5%8A%A1%E6%B2%BB%E7%90%86/"},{"categories":["Istio"],"content":"11.2. 实验演练 （1）部署 forecast 服务的 v1 和 v2 版本，设置策略使访问 forecast 服务的流量都被路由到 forecast 服务的 v1 版本。 kubectl apply -f install/virtual-service-v1.yaml -n weather 在浏览器查询天气信息，看不到推荐信息。这时查询 forecast 服务的 v2 实例的 Proxy 日志，或者通过可视化工具，可以确定服务的 v2 版本的实例上没有任何流量。 （2）执行如下命令设置影子策略，复制 v1 版本流量给 v2 版本： kubectl apply -f chapter-files/traffic-management/vs-forecast-mirroring.yaml -n weather 在浏览器中查询天气信息，我们没有看到推荐信息，说明 forecast 服务的 v2 版本没有将结果返回给 frontend 服务，那么 forecast 服务的 v2 版本有没有收到流量？分别查看 forecast 服务的 v1 版本 和 v2 版本 Proxy 日志，可以看到两个实例同时收到了请求，forecast 服务的 v1 版本流量被 Proxy 复制了一份发给 v2 版本的实例： 通过可视化工具可以看到，虽然 frontend 服务没有调用 forecast 服务的 v2 版本，但这时 recommendation 服务的实例有流量出现，说明在 forecast 服务的 v2 版本上存在隐藏的流量： ","date":"2020-05-13","objectID":"/%E6%9C%8D%E5%8A%A1%E6%B2%BB%E7%90%86/:22:0","tags":["Istio","实验"],"title":"服务治理","uri":"/%E6%9C%8D%E5%8A%A1%E6%B2%BB%E7%90%86/"},{"categories":["Istio"],"content":"11.3. 工作原理 查看配置： 上面配置的策略将全部流量都发送到 forecast 服务的 v1 版本，其中的 mirror 字段指定将流量复制到 forecast 服务的 v2 版本。当流量被复制时，会在请求的 HOST 或 Authority 头中添加 -shadow 后缀（如 forecast-shadow），并将请求发送到 forecast 服务的 v2 版本以示它是影子流量。这些被复制的请求引发的响应会被丢弃，不会影响终端客户。 ","date":"2020-05-13","objectID":"/%E6%9C%8D%E5%8A%A1%E6%B2%BB%E7%90%86/:23:0","tags":["Istio","实验"],"title":"服务治理","uri":"/%E6%9C%8D%E5%8A%A1%E6%B2%BB%E7%90%86/"},{"categories":["Gloo"],"content":"使用 Gloo Proxy API 自动为存在的 kubernetes 服务创建路由器。 1. 为什么编写自定义代理控制器？ Building a Proxy controller allows you to add custom Gloo operational logic to your setup. In this example, we will write a proxy controller that creates and manages a second Gloo Proxy (my-cool-proxy) alongside the Gloo-managed one (gateway-proxy). The my-cool-proxy envoy proxy will route to Gloo-discovered kubernetes services using the host header alone, relieving the Gloo admin from creating virtual services or route tables to route to each discovered service. Other common use cases that can be solved with custom proxy controllers include: automatically creating http routes that redirect to equivalent https routes automatically routing to services based on service name and removing the service name prefix from the request path 2. 工作原理 A custom Proxy controller takes any inputs (in our case, Gloo custom resources in kubernetes) and writes the desired output to managed Proxy custom resource(s). In our case, we will write a controller that takes Upstreams and Proxys as inputs and outputs a new Proxy. Then we will deploy the new controller to create and manage our new my-cool-proxy Proxy custom resource. Finally, we will deploy a second envoy proxy to kubernetes, have it register to Gloo with its role configured to match the name of our managed Proxy custom resource (my-cool-proxy), and configure it to receive configuration from Gloo. 3. 编码步骤 前置准备 依赖 module proxycontroller go 1.13 require ( github.com/solo-io/gloo v1.2.12 // change to update Gloo version to build against github.com/solo-io/go-utils v0.11.5 github.com/solo-io/solo-kit v0.11.15 k8s.io/client-go v11.0.0+incompatible ) replace ( github.com/Azure/go-autorest =\u003e github.com/Azure/go-autorest v13.0.0+incompatible github.com/Sirupsen/logrus =\u003e github.com/sirupsen/logrus v1.4.2 github.com/docker/docker =\u003e github.com/moby/moby v0.7.3-0.20190826074503-38ab9da00309 k8s.io/api =\u003e k8s.io/api v0.0.0-20191004120104-195af9ec3521 k8s.io/apiextensions-apiserver =\u003e k8s.io/apiextensions-apiserver v0.0.0-20191204090712-e0e829f17bab k8s.io/apimachinery =\u003e k8s.io/apimachinery v0.0.0-20191028221656-72ed19daf4bb k8s.io/apiserver =\u003e k8s.io/apiserver v0.0.0-20191109104512-b243870e034b k8s.io/cli-runtime =\u003e k8s.io/cli-runtime v0.0.0-20191004123735-6bff60de4370 k8s.io/client-go =\u003e k8s.io/client-go v0.0.0-20191016111102-bec269661e48 k8s.io/cloud-provider =\u003e k8s.io/cloud-provider v0.0.0-20191004125000-f72359dfc58e k8s.io/cluster-bootstrap =\u003e k8s.io/cluster-bootstrap v0.0.0-20191004124811-493ca03acbc1 k8s.io/code-generator =\u003e k8s.io/code-generator v0.0.0-20191004115455-8e001e5d1894 k8s.io/component-base =\u003e k8s.io/component-base v0.0.0-20191004121439-41066ddd0b23 k8s.io/cri-api =\u003e k8s.io/cri-api v0.0.0-20190828162817-608eb1dad4ac k8s.io/csi-translation-lib =\u003e k8s.io/csi-translation-lib v0.0.0-20191004125145-7118cc13aa0a k8s.io/gengo =\u003e k8s.io/gengo v0.0.0-20190822140433-26a664648505 k8s.io/heapster =\u003e k8s.io/heapster v1.2.0-beta.1 k8s.io/klog =\u003e github.com/stefanprodan/klog v0.0.0-20190418165334-9cbb78b20423 k8s.io/kube-aggregator =\u003e k8s.io/kube-aggregator v0.0.0-20191104231939-9e18019dec40 k8s.io/kube-controller-manager =\u003e k8s.io/kube-controller-manager v0.0.0-20191004124629-b9859bb1ce71 k8s.io/kube-openapi =\u003e k8s.io/kube-openapi v0.0.0-20190816220812-743ec37842bf k8s.io/kube-proxy =\u003e k8s.io/kube-proxy v0.0.0-20191004124112-c4ee2f9e1e0a k8s.io/kube-scheduler =\u003e k8s.io/kube-scheduler v0.0.0-20191004124444-89f3bbd82341 k8s.io/kubectl =\u003e k8s.io/kubectl v0.0.0-20191004125858-14647fd13a8b k8s.io/kubelet =\u003e k8s.io/kubelet v0.0.0-20191004124258-ac1ea479bd3a k8s.io/legacy-cloud-providers =\u003e k8s.io/legacy-cloud-providers v0.0.0-20191203122058-2ae7e9ca8470 k8s.io/metrics =\u003e k8s.io/metrics v0.0.0-20191004123543-798934cf5e10 k8s.io/node-api =\u003e k8s.io/node-api v0.0.0-20191004125527-f5592a7bd6b6 k8s.io/repo-infra =\u003e k8s.io/repo-infra v0.0.0-20181204233714-00fe14e3d1a3 k8s.io/sample-apiserver =\u003e k8s.io/sample-apiserver v0.0.0-20191028231949-ceef03","date":"2020-05-14","objectID":"/%E8%87%AA%E5%AE%9A%E4%B9%89-gloo-proxy-controller/:0:0","tags":["Gloo","原理"],"title":"自定义 Gloo Proxy Controller","uri":"/%E8%87%AA%E5%AE%9A%E4%B9%89-gloo-proxy-controller/"},{"categories":["Gloo"],"content":"3.2. Gloo API Clients func initGlooClients(ctx context.Context) (v1.UpstreamClient, v1.ProxyClient) { // root rest config restConfig, err := kubeutils.GetConfig( os.Getenv(\"KUBERNETES_MASTER_URL\"), os.Getenv(\"KUBECONFIG\")) must(err) // wrapper for kubernetes shared informer factory cache := kube.NewKubeCache(ctx) // initialize the CRD client for Gloo Upstreams upstreamClient, err := v1.NewUpstreamClient(\u0026factory.KubeResourceClientFactory{ Crd: v1.UpstreamCrd, Cfg: restConfig, SharedCache: cache, SkipCrdCreation: true, }) must(err) // registering the client registers the type with the client cache err = upstreamClient.Register() must(err) // initialize the CRD client for Gloo Proxies proxyClient, err := v1.NewProxyClient(\u0026factory.KubeResourceClientFactory{ Crd: v1.ProxyCrd, Cfg: restConfig, SharedCache: cache, SkipCrdCreation: true, }) must(err) // registering the client registers the type with the client cache err = proxyClient.Register() must(err) return upstreamClient, proxyClient } ","date":"2020-05-14","objectID":"/%E8%87%AA%E5%AE%9A%E4%B9%89-gloo-proxy-controller/:1:0","tags":["Gloo","原理"],"title":"自定义 Gloo Proxy Controller","uri":"/%E8%87%AA%E5%AE%9A%E4%B9%89-gloo-proxy-controller/"},{"categories":["Gloo"],"content":"3.3. Proxy Configuration // in this function we'll generate an opinionated // proxy object with a routes for each of our upstreams func makeDesiredProxy(upstreams v1.UpstreamList) *v1.Proxy { // each virtual host represents the table of routes for a given // domain or set of domains. // in this example, we'll create one virtual host // for each upstream. var virtualHosts []*v1.VirtualHost for _, upstream := range upstreams { upstreamRef := upstream.Metadata.Ref() // create a virtual host for each upstream vHostForUpstream := \u0026v1.VirtualHost{ // logical name of the virtual host, should be unique across vhosts Name: upstream.Metadata.Name, // the domain will be our \"matcher\". // requests with the Host header equal to the upstream name // will be routed to this upstream Domains: []string{upstream.Metadata.Name}, // we'll create just one route designed to match any request // and send it to the upstream for this domain Routes: []*v1.Route{{ // use a basic catch-all matcher Matchers: []*matchers.Matcher{ \u0026matchers.Matcher{ PathSpecifier: \u0026matchers.Matcher_Prefix{ Prefix: \"/\", }, }, }, // tell Gloo where to send the requests Action: \u0026v1.Route_RouteAction{ RouteAction: \u0026v1.RouteAction{ Destination: \u0026v1.RouteAction_Single{ // single destination Single: \u0026v1.Destination{ DestinationType: \u0026v1.Destination_Upstream{ // a \"reference\" to the upstream, which is a Namespace/Name tuple Upstream: \u0026upstreamRef, }, }, }, }, }, }}, } virtualHosts = append(virtualHosts, vHostForUpstream) } desiredProxy := \u0026v1.Proxy{ // metadata will be translated to Kubernetes ObjectMeta Metadata: core.Metadata{Namespace: \"gloo-system\", Name: \"my-cool-proxy\"}, // we have the option of creating multiple listeners, // but for the purpose of this example we'll just use one Listeners: []*v1.Listener{{ // logical name for the listener Name: \"my-amazing-listener\", // instruct envoy to bind to all interfaces on port 8080 BindAddress: \"::\", BindPort: 8080, // at this point you determine what type of listener // to use. here we'll be using the HTTP Listener // other listener types are currently unsupported, // but future ListenerType: \u0026v1.Listener_HttpListener{ HttpListener: \u0026v1.HttpListener{ // insert our list of virtual hosts here VirtualHosts: virtualHosts, }, }}, }, } return desiredProxy } ","date":"2020-05-14","objectID":"/%E8%87%AA%E5%AE%9A%E4%B9%89-gloo-proxy-controller/:2:0","tags":["Gloo","原理"],"title":"自定义 Gloo Proxy Controller","uri":"/%E8%87%AA%E5%AE%9A%E4%B9%89-gloo-proxy-controller/"},{"categories":["Gloo"],"content":"3.4. Event Loop // we received a new list of upstreams! regenerate the desired proxy // and write it as a CRD to Kubernetes func resync(ctx context.Context, upstreams v1.UpstreamList, client v1.ProxyClient) { desiredProxy := makeDesiredProxy(upstreams) // see if the proxy exists. if yes, update; if no, create existingProxy, err := client.Read( desiredProxy.Metadata.Namespace, desiredProxy.Metadata.Name, clients.ReadOpts{Ctx: ctx}) // proxy exists! this is an update, not a create if err == nil { // sleep for 1s as Gloo may be re-validating our proxy, which can cause resource version to change time.Sleep(time.Second) // ensure resource version is the latest existingProxy, err = client.Read( desiredProxy.Metadata.Namespace, desiredProxy.Metadata.Name, clients.ReadOpts{Ctx: ctx}) must(err) // update the resource version on our desired proxy desiredProxy.Metadata.ResourceVersion = existingProxy.Metadata.ResourceVersion } // write! written, err := client.Write(desiredProxy, clients.WriteOpts{Ctx: ctx, OverwriteExisting: true}) must(err) log.Printf(\"wrote proxy object: %+v\\n\", written) } ","date":"2020-05-14","objectID":"/%E8%87%AA%E5%AE%9A%E4%B9%89-gloo-proxy-controller/:3:0","tags":["Gloo","原理"],"title":"自定义 Gloo Proxy Controller","uri":"/%E8%87%AA%E5%AE%9A%E4%B9%89-gloo-proxy-controller/"},{"categories":["Gloo"],"content":"3.5. Main Function func main() { // root context for the whole thing ctx := context.Background() // initialize Gloo API clients upstreamClient, proxyClient := initGlooClients(ctx) // start a watch on upstreams. we'll use this as our trigger // whenever upstreams are modified, we'll trigger our sync function upstreamWatch, watchErrors, initError := upstreamClient.Watch(\"gloo-system\", clients.WatchOpts{Ctx: ctx}) must(initError) // our \"event loop\". an event occurs whenever the list of upstreams has been updated for { select { // if we error during watch, just exit case err := \u003c-watchErrors: must(err) // process a new upstream list case newUpstreamList := \u003c-upstreamWatch: // we received a new list of upstreams from our watch, resync(ctx, newUpstreamList, proxyClient) } } } ","date":"2020-05-14","objectID":"/%E8%87%AA%E5%AE%9A%E4%B9%89-gloo-proxy-controller/:4:0","tags":["Gloo","原理"],"title":"自定义 Gloo Proxy Controller","uri":"/%E8%87%AA%E5%AE%9A%E4%B9%89-gloo-proxy-controller/"},{"categories":["Gloo"],"content":"3.6. Run go run example/proxycontroller/proxycontroller.go ","date":"2020-05-14","objectID":"/%E8%87%AA%E5%AE%9A%E4%B9%89-gloo-proxy-controller/:5:0","tags":["Gloo","原理"],"title":"自定义 Gloo Proxy Controller","uri":"/%E8%87%AA%E5%AE%9A%E4%B9%89-gloo-proxy-controller/"},{"categories":["Gloo"],"content":"3.7. Test curl $(glooctl proxy url -n default --name my-cool-proxy)/api/pets -H \"Host: default-petstore-8080\" returns [{\"id\":1,\"name\":\"Dog\",\"status\":\"available\"},{\"id\":2,\"name\":\"Cat\",\"status\":\"pending\"}] ","date":"2020-05-14","objectID":"/%E8%87%AA%E5%AE%9A%E4%B9%89-gloo-proxy-controller/:6:0","tags":["Gloo","原理"],"title":"自定义 Gloo Proxy Controller","uri":"/%E8%87%AA%E5%AE%9A%E4%B9%89-gloo-proxy-controller/"},{"categories":["Gloo"],"content":"1. Gloo 简介 Gloo is a feature-rich, Kubernetes-native ingress controller, and next-generation API gateway. Gloo is uniquely designed to support hybrid applications, in which multiple technologies, architectures, protocols, and clouds can coexist. Gloo is exceptional in its function-level routing; its support for legacy apps, microservices and serverless; its discovery capabilities; its numerous features; its tight integration with leading open-source projects. 2. Gloo 功能 connect secure control All Workload TypesAuto Service DiscoveryHTTP RoutingTCP ProxygRPC WebCORSKubernetes ServicesConsul ServicesServerless FunctionsConfiguration ValidationRequest / Response TransformationService Mesh integration 社区版：TLSHashicorp Vault SecretsLet’s EncryptCustom Authentication (DIY)企业版：Data Loss PreventionWeb App Firewall (WAF)Basic AuthenticationAPI KeyJSON Web Token (JWT)LDAP SupportOAuth / OIDCOpen Policy Agent 社区版：Admin Dashboard (Read Only)Role DelegationAccess Logging \u0026 Usage StatsPrometheus and GrafanaTracingCircuit BreakingRetriesTimeoutsTraffic ShiftingTraffic ShadowingRate Limiting (DIY)企业版：Admin Dashboard (Full Access)Advanced Rate Limiting 3. Gloo 架构 Secret Watcher The Secret Watcher watches a secret store for updates to secrets (which are required for certain plugins such as the AWS Lambda Plugin . The secret storage could be using secrets management in Kubernetes, HashiCorp Vault, or some other secure secret storage system. Config Watcher The Config Watcher watches the storage layer for updates to user configuration objects, such as Upstreams and Virtual Services. The storage layer could be a custom resource in Kubernetes or an key/value entry in HashiCorp Consul. Reporter The Reporter receives a validation report for every Upstream and Virtual Service processed by the translator. Any invalid config objects are reported back to the user through the storage layer. Invalid objects are marked as Rejected with detailed error messages describing mistakes found in the configuration. Endpoint Discovery Endpoint Discovery watches service registries such as Kubernetes, Cloud Foundry, and Consul for IPs associated with services. Endpoint Discovery is plugin-specific, so each endpoint type will require a plug-in that supports the discovery functionality. For example, the Kubernetes Plugin runs its own Endpoint Discovery goroutine. Gloo Translator The Gloo Translator receives snapshots of the entire state, composed of the following configuration data: Artifacts Endpoints Proxies Upstreams UpstreamGroups Secrets AuthConfigs The translator takes all of this information and initiates a new translation loop with the end goal of creating a new Envoy xDS Snapshot. xDS Server The final snapshot is passed to the xDS Server, which notifies Envoy of a successful config update, updating the Envoy cluster with a new configuration to match the desired state set expressed by Gloo. Discovery Architecture Gloo is supported by a suite of optional discovery services that automatically discover and configure Gloo with Upstreams and functions to simplify routing for users and self-service. Discovery services act as automated Gloo clients, automatically populating the storage layer with Upstreams and functions to facilitate easy routing for users. Discovery is optional, but when enabled, it will attempt to discover available Upstreams and functions. The following discovery methods are currently supported: Kubernetes Service-Based Upstream Discovery AWS Lambda-Based Function Discovery Google Cloud Function-Based Function Discovery OpenAPI-Based Function Discovery Istio-Based Route Rule Discovery (Experimental) 参考： Announcing Gloo: The Function Gateway https://medium.com/solo-io/announcing-gloo-the-function-gateway-3f0860ef6600 ","date":"2020-05-14","objectID":"/gloo-gloo-%E7%AE%80%E4%BB%8B%E5%8A%9F%E8%83%BD%E5%92%8C%E6%9E%B6%E6%9E%84/:0:0","tags":["Gloo","原理"],"title":"Gloo Gloo 简介、功能和架构","uri":"/gloo-gloo-%E7%AE%80%E4%BB%8B%E5%8A%9F%E8%83%BD%E5%92%8C%E6%9E%B6%E6%9E%84/"},{"categories":["Gloo"],"content":"以下为个人理解，仅供参考！ Gloo Envoy Upgrades As we ship an extended version of Envoy, it is very important for us to stay close to upstream Envoy. To achieve this, our repository structure and CI closely resembles those of Envoy. Envoy master branch is always considered RC quality. We therefore make sure frequently that Gloo can be built with the latest master code. This ensures that we always have the latest Envoy features and optimizations, and can respond quickly if a security update is needed. Building Envoy on the 32-core CI we use takes about 10 minutes. servless 支持 In order to achieve Knative scale-from-zero, we use a Mixer out-of-process adapter to call the Autoscaler. Out-of-process adapters for Mixer allow developers to use any programming language and to build and maintain your extension as a stand-alone program without the need to build the Istio proxy. 好复杂的感觉。 只需创建一个 Aws 类型的 Upstream，so easy 的感觉。 Api 组合 Sqoop (formerly QLoo) is a GraphQL Server built on top of Gloo and the Envoy Proxy. Sqoop leverages Gloo’s function registry and Envoy’s advanced HTTP routing features to provide a GraphQL frontend for REST/gRPC applications and serverless functions. Sqoop routes requests to data sources via Envoy, leveraging Envoy HTTP filters for security, load balancing, and more. 个人觉得基于此可做Api组合的事情，一方面项目大起来之后，减少聚合层做的工作，另一方面，减少后端接口的数量。 Istio 暂无，也不会有，关注点不一样，是东西流量。 灰度发布 Istio 做法：哪一个服务要灰度，写哪一个服务的 VirtualService，个人觉得，灰度的配置管理，当前和流控，路由等的配置都集中在了目的服务的vs配置里，不便于管理。另一方面，服务多了之后，因为是为目标服务来做配置，这些配置文件如何管理。如下的这些灰度规则，个人 觉得都有点晕了。 Gloo 做法, 通过Upstream Group 统一起来，So Easy，至少视觉效果是这样 参考： https://medium.com/solo-io/building-a-control-plane-for-envoy-7524ceb09876 https://github.com/solo-io/sqoop https://istio.io/blog/2019/knative-activator-adapter/ ","date":"2020-05-14","objectID":"/gloo-%E5%88%86%E6%9E%90/:0:0","tags":["Gloo","原理"],"title":"Gloo 分析","uri":"/gloo-%E5%88%86%E6%9E%90/"},{"categories":["Gloo"],"content":"3个层：网关侦听器，虚拟服务和上游。通常，我们与虚拟服务进行交互，从而可以配置希望在网关上公开的API的详细信息以及如何路由到后端。 上游代表那些后端。网关对象可帮助我们控制侦听器的传入流量。 1. 部署 Pet Store kubectl apply -f https://raw.githubusercontent.com/solo-io/gloo/v1.2.9/example/petstore/petstore.yaml 验证 Pod 和 Svc 验证 Pet Store 上游服务 Gloo 服务发现会监视添加到 kubernetes 集群的新服务。当 petstore 服务被创建后，Gloo 会自动为 petstore 服务创建一个 Upstream。如果一切正常， Upstream STATUS 为 Accepted。 2. 标记命名空间 查看 upstream yaml 默认情况下，上游创建的过程非常简单。它代表特定的 kubernetes 服务。但是，petstore应用程序是一个 swagger 的服务。Gloo可以发现这种 swagger 规格，但是默认情况下，Gloo的功能发现功能已关闭，以提高性能。 要在我们的petstore上启用功能发现服务（fds），我们需要标记名称空间。 kubectl label namespace default discovery.solo.io/function_discovery=enabled 3. 配置路由 即使已创建上游，在我们向虚拟服务添加一些路由规则之前，Gloo仍不会将流量路由到该路由。现在，我们使用glooctl通过–prefix-rewrite标志为此上游创建一条基本路由，以在传入请求中重写路径以匹配我们的petstore应用程序期望的路径。 glooctl add route \\ --path-exact /all-pets \\ --dest-name default-petstore-8080 \\ --prefix-rewrite /api/pets petstore虚拟服务的初始状态为Pending。几秒钟后，应更改为“已接受”。让我们通过使用glooctl检索虚拟服务来验证这一点。 glooctl get virtualservice 路由与Gloo中的虚拟服务相关联。在上一步中创建路由时，我们没有提供虚拟服务，因此Gloo创建了一个名为default的虚拟服务并添加了路线。 glooctl get virtualservice --output kube-yaml 创建虚拟服务后，Gloo立即更新代理配置。由于此虚拟服务的状态为“已接受”，因此我们知道此路由现在处于活动状态。 至此，我们有了一个带有路由规则的虚拟服务，该路由规则将/ all-pets路径上的流量发送到/ api / pets路径上的上游petstore。 4. 测试 curl $(glooctl proxy url)/all-pets ","date":"2020-05-14","objectID":"/gloo-routing/:0:0","tags":["Gloo","原理"],"title":"Gloo Routing","uri":"/gloo-routing/"},{"categories":["Istio"],"content":"1. 环境准备 ","date":"2020-05-14","objectID":"/%E5%AE%9E%E9%AA%8C%E6%80%BB%E7%BB%93/:0:0","tags":["Istio","实验"],"title":"实验总结","uri":"/%E5%AE%9E%E9%AA%8C%E6%80%BB%E7%BB%93/"},{"categories":["Istio"],"content":"1.1. Kubernetes 搭建 ","date":"2020-05-14","objectID":"/%E5%AE%9E%E9%AA%8C%E6%80%BB%E7%BB%93/:1:0","tags":["Istio","实验"],"title":"实验总结","uri":"/%E5%AE%9E%E9%AA%8C%E6%80%BB%E7%BB%93/"},{"categories":["Istio"],"content":"1.2. Istio 部署 2. 流量监控 ","date":"2020-05-14","objectID":"/%E5%AE%9E%E9%AA%8C%E6%80%BB%E7%BB%93/:2:0","tags":["Istio","实验"],"title":"实验总结","uri":"/%E5%AE%9E%E9%AA%8C%E6%80%BB%E7%BB%93/"},{"categories":["Istio"],"content":"2.1. 预先准备：安装插件 ","date":"2020-05-14","objectID":"/%E5%AE%9E%E9%AA%8C%E6%80%BB%E7%BB%93/:3:0","tags":["Istio","实验"],"title":"实验总结","uri":"/%E5%AE%9E%E9%AA%8C%E6%80%BB%E7%BB%93/"},{"categories":["Istio"],"content":"2.2. 调用链跟踪 ","date":"2020-05-14","objectID":"/%E5%AE%9E%E9%AA%8C%E6%80%BB%E7%BB%93/:4:0","tags":["Istio","实验"],"title":"实验总结","uri":"/%E5%AE%9E%E9%AA%8C%E6%80%BB%E7%BB%93/"},{"categories":["Istio"],"content":"2.3. 指标监控 ","date":"2020-05-14","objectID":"/%E5%AE%9E%E9%AA%8C%E6%80%BB%E7%BB%93/:5:0","tags":["Istio","实验"],"title":"实验总结","uri":"/%E5%AE%9E%E9%AA%8C%E6%80%BB%E7%BB%93/"},{"categories":["Istio"],"content":"2.4. Grafana ","date":"2020-05-14","objectID":"/%E5%AE%9E%E9%AA%8C%E6%80%BB%E7%BB%93/:6:0","tags":["Istio","实验"],"title":"实验总结","uri":"/%E5%AE%9E%E9%AA%8C%E6%80%BB%E7%BB%93/"},{"categories":["Istio"],"content":"2.5. 服务网格监控 3. 灰度发布 ","date":"2020-05-14","objectID":"/%E5%AE%9E%E9%AA%8C%E6%80%BB%E7%BB%93/:7:0","tags":["Istio","实验"],"title":"实验总结","uri":"/%E5%AE%9E%E9%AA%8C%E6%80%BB%E7%BB%93/"},{"categories":["Istio"],"content":"3.1. 预先准备：将所有流量路由到各服务 v1 版本 ","date":"2020-05-14","objectID":"/%E5%AE%9E%E9%AA%8C%E6%80%BB%E7%BB%93/:8:0","tags":["Istio","实验"],"title":"实验总结","uri":"/%E5%AE%9E%E9%AA%8C%E6%80%BB%E7%BB%93/"},{"categories":["Istio"],"content":"3.2. 基于流量比例的路由 ","date":"2020-05-14","objectID":"/%E5%AE%9E%E9%AA%8C%E6%80%BB%E7%BB%93/:9:0","tags":["Istio","实验"],"title":"实验总结","uri":"/%E5%AE%9E%E9%AA%8C%E6%80%BB%E7%BB%93/"},{"categories":["Istio"],"content":"3.3. 基于请求内容的路由 ","date":"2020-05-14","objectID":"/%E5%AE%9E%E9%AA%8C%E6%80%BB%E7%BB%93/:10:0","tags":["Istio","实验"],"title":"实验总结","uri":"/%E5%AE%9E%E9%AA%8C%E6%80%BB%E7%BB%93/"},{"categories":["Istio"],"content":"3.4. 组合条件路由 ","date":"2020-05-14","objectID":"/%E5%AE%9E%E9%AA%8C%E6%80%BB%E7%BB%93/:11:0","tags":["Istio","实验"],"title":"实验总结","uri":"/%E5%AE%9E%E9%AA%8C%E6%80%BB%E7%BB%93/"},{"categories":["Istio"],"content":"3.5. 多服务灰度发布 ","date":"2020-05-14","objectID":"/%E5%AE%9E%E9%AA%8C%E6%80%BB%E7%BB%93/:12:0","tags":["Istio","实验"],"title":"实验总结","uri":"/%E5%AE%9E%E9%AA%8C%E6%80%BB%E7%BB%93/"},{"categories":["Istio"],"content":"3.6. TCP 服务灰度发布 ","date":"2020-05-14","objectID":"/%E5%AE%9E%E9%AA%8C%E6%80%BB%E7%BB%93/:13:0","tags":["Istio","实验"],"title":"实验总结","uri":"/%E5%AE%9E%E9%AA%8C%E6%80%BB%E7%BB%93/"},{"categories":["Istio"],"content":"3.7. 自动化灰度发布 4. 服务治理 ","date":"2020-05-14","objectID":"/%E5%AE%9E%E9%AA%8C%E6%80%BB%E7%BB%93/:14:0","tags":["Istio","实验"],"title":"实验总结","uri":"/%E5%AE%9E%E9%AA%8C%E6%80%BB%E7%BB%93/"},{"categories":["Istio"],"content":"4.1. 流量负载均衡 ","date":"2020-05-14","objectID":"/%E5%AE%9E%E9%AA%8C%E6%80%BB%E7%BB%93/:15:0","tags":["Istio","实验"],"title":"实验总结","uri":"/%E5%AE%9E%E9%AA%8C%E6%80%BB%E7%BB%93/"},{"categories":["Istio"],"content":"4.2. 会话保持 ","date":"2020-05-14","objectID":"/%E5%AE%9E%E9%AA%8C%E6%80%BB%E7%BB%93/:16:0","tags":["Istio","实验"],"title":"实验总结","uri":"/%E5%AE%9E%E9%AA%8C%E6%80%BB%E7%BB%93/"},{"categories":["Istio"],"content":"4.3. 故障注入 ","date":"2020-05-14","objectID":"/%E5%AE%9E%E9%AA%8C%E6%80%BB%E7%BB%93/:17:0","tags":["Istio","实验"],"title":"实验总结","uri":"/%E5%AE%9E%E9%AA%8C%E6%80%BB%E7%BB%93/"},{"categories":["Istio"],"content":"4.4. 超时 ","date":"2020-05-14","objectID":"/%E5%AE%9E%E9%AA%8C%E6%80%BB%E7%BB%93/:18:0","tags":["Istio","实验"],"title":"实验总结","uri":"/%E5%AE%9E%E9%AA%8C%E6%80%BB%E7%BB%93/"},{"categories":["Istio"],"content":"4.5. 重试 ","date":"2020-05-14","objectID":"/%E5%AE%9E%E9%AA%8C%E6%80%BB%E7%BB%93/:19:0","tags":["Istio","实验"],"title":"实验总结","uri":"/%E5%AE%9E%E9%AA%8C%E6%80%BB%E7%BB%93/"},{"categories":["Istio"],"content":"4.6. HTTP 重定向 ","date":"2020-05-14","objectID":"/%E5%AE%9E%E9%AA%8C%E6%80%BB%E7%BB%93/:20:0","tags":["Istio","实验"],"title":"实验总结","uri":"/%E5%AE%9E%E9%AA%8C%E6%80%BB%E7%BB%93/"},{"categories":["Istio"],"content":"4.7. HTTP 重写 ","date":"2020-05-14","objectID":"/%E5%AE%9E%E9%AA%8C%E6%80%BB%E7%BB%93/:21:0","tags":["Istio","实验"],"title":"实验总结","uri":"/%E5%AE%9E%E9%AA%8C%E6%80%BB%E7%BB%93/"},{"categories":["Istio"],"content":"4.8. 熔断 ","date":"2020-05-14","objectID":"/%E5%AE%9E%E9%AA%8C%E6%80%BB%E7%BB%93/:22:0","tags":["Istio","实验"],"title":"实验总结","uri":"/%E5%AE%9E%E9%AA%8C%E6%80%BB%E7%BB%93/"},{"categories":["Istio"],"content":"4.9. 限流 ","date":"2020-05-14","objectID":"/%E5%AE%9E%E9%AA%8C%E6%80%BB%E7%BB%93/:23:0","tags":["Istio","实验"],"title":"实验总结","uri":"/%E5%AE%9E%E9%AA%8C%E6%80%BB%E7%BB%93/"},{"categories":["Istio"],"content":"4.10. 服务隔离 ","date":"2020-05-14","objectID":"/%E5%AE%9E%E9%AA%8C%E6%80%BB%E7%BB%93/:24:0","tags":["Istio","实验"],"title":"实验总结","uri":"/%E5%AE%9E%E9%AA%8C%E6%80%BB%E7%BB%93/"},{"categories":["Istio"],"content":"4.11. 影子测试 5. 服务保护 ","date":"2020-05-14","objectID":"/%E5%AE%9E%E9%AA%8C%E6%80%BB%E7%BB%93/:25:0","tags":["Istio","实验"],"title":"实验总结","uri":"/%E5%AE%9E%E9%AA%8C%E6%80%BB%E7%BB%93/"},{"categories":["Istio"],"content":"5.1. 单向 TLS 加密网关 ","date":"2020-05-14","objectID":"/%E5%AE%9E%E9%AA%8C%E6%80%BB%E7%BB%93/:26:0","tags":["Istio","实验"],"title":"实验总结","uri":"/%E5%AE%9E%E9%AA%8C%E6%80%BB%E7%BB%93/"},{"categories":["Istio"],"content":"5.2. 双向 TLS 加密网关 ","date":"2020-05-14","objectID":"/%E5%AE%9E%E9%AA%8C%E6%80%BB%E7%BB%93/:27:0","tags":["Istio","实验"],"title":"实验总结","uri":"/%E5%AE%9E%E9%AA%8C%E6%80%BB%E7%BB%93/"},{"categories":["Istio"],"content":"5.3. SDS 加密网关 ","date":"2020-05-14","objectID":"/%E5%AE%9E%E9%AA%8C%E6%80%BB%E7%BB%93/:28:0","tags":["Istio","实验"],"title":"实验总结","uri":"/%E5%AE%9E%E9%AA%8C%E6%80%BB%E7%BB%93/"},{"categories":["Istio"],"content":"5.4. 访问控制之黑名单 ","date":"2020-05-14","objectID":"/%E5%AE%9E%E9%AA%8C%E6%80%BB%E7%BB%93/:29:0","tags":["Istio","实验"],"title":"实验总结","uri":"/%E5%AE%9E%E9%AA%8C%E6%80%BB%E7%BB%93/"},{"categories":["Istio"],"content":"5.5. 访问控制之白名单 ","date":"2020-05-14","objectID":"/%E5%AE%9E%E9%AA%8C%E6%80%BB%E7%BB%93/:30:0","tags":["Istio","实验"],"title":"实验总结","uri":"/%E5%AE%9E%E9%AA%8C%E6%80%BB%E7%BB%93/"},{"categories":["Istio"],"content":"5.6. 认证 ","date":"2020-05-14","objectID":"/%E5%AE%9E%E9%AA%8C%E6%80%BB%E7%BB%93/:31:0","tags":["Istio","实验"],"title":"实验总结","uri":"/%E5%AE%9E%E9%AA%8C%E6%80%BB%E7%BB%93/"},{"categories":["Istio"],"content":"5.7. 授权之命名空间级别的访问控制 ","date":"2020-05-14","objectID":"/%E5%AE%9E%E9%AA%8C%E6%80%BB%E7%BB%93/:32:0","tags":["Istio","实验"],"title":"实验总结","uri":"/%E5%AE%9E%E9%AA%8C%E6%80%BB%E7%BB%93/"},{"categories":["Istio"],"content":"5.8. 授权之服务级别访问控制 6. 多集群管理 ","date":"2020-05-14","objectID":"/%E5%AE%9E%E9%AA%8C%E6%80%BB%E7%BB%93/:33:0","tags":["Istio","实验"],"title":"实验总结","uri":"/%E5%AE%9E%E9%AA%8C%E6%80%BB%E7%BB%93/"},{"categories":["Istio"],"content":"6.1. 多集群管理之单控制平面服务路由感知 ","date":"2020-05-14","objectID":"/%E5%AE%9E%E9%AA%8C%E6%80%BB%E7%BB%93/:34:0","tags":["Istio","实验"],"title":"实验总结","uri":"/%E5%AE%9E%E9%AA%8C%E6%80%BB%E7%BB%93/"},{"categories":["Istio"],"content":"在多个集群中部署和管理应用，能带来更好的故障隔离性和扩展性。Istio 的多集群模型主要分为两类：多控制面模型和单控制面模型。 由于多控制面模型存在配置规则复杂等问题，而在集群间使用 VPN 直连的单控制面模型对网络的连通性又有较高的要求。所以如下实验 演示单控制平面服务路由的感知方案，这种方案不要求网络扁平，只要求 Pilot 可以访问所有集群的 Kube-apiserver。 1. 实验目标 在两个不同的集群中分别部署同一个服务 helloworld 的不同实例：helloworld-v1 和 helloworld-v2，从客户端程序 sleep 内访问 helloworld 服务的流量能够被路由到两个集群的不同实例。 2. 实验演练： （一）环境准备 （1）准备两个 kubernetes 集群：cluster1 和 cluster2，其中 cluster1 是主集群，即部署 Pilot 等组件的集群： kubectl config get-contexts （2）存储集群名称到环境变量： export CTX_CLUSTER1=$(kubectl config view -o jsonpath='{.contexts[0].name}') export CTX_CLUSTER2=$(kubectl config view -o jsonpath='{.contexts[1].name}') echo CTX_CLUSTER1 = ${CTX_CLUSTER1}, CTX_CLUSTER2 = ${CTX_CLUSTER2} （二）主集群配置： （1）部署 Istio 到 cluster1： kubectl create --context=$CTX_CLUSTER1 ns istio-system kubectl create --context=$CTX_CLUSTER1 secret generic cacerts -n istio-system --from-file=samples/certs/ca-cert.pem --from-file=samples/certs/ca-key.pem --from-file=samples/certs/root-cert.pem --from-file=samples/certs/cert-chain.pem istioctl manifest apply --context=$CTX_CLUSTER1 \\ -f install/kubernetes/operator/examples/multicluster/values-istio-multicluster-primary.yaml 查看部署： kubectl get pods --context=$CTX_CLUSTER1 -n istio-system （2）创建一个入口网关使 cluster2 的服务可以进人 cluster1： kubectl apply --context=$CTX_CLUSTER1 -f - \u003c\u003cEOF apiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: cluster-aware-gateway namespace: istio-system spec: selector: istio: ingressgateway servers: - port: number: 443 name: tls protocol: TLS tls: mode: AUTO_PASSTHROUGH hosts: - \"*.local\" EOF （3）确定 cluster1 的入口 IP 和端口： 1.将kubectl的当前上下文设置为CTX_CLUSTER1 export ORIGINAL_CONTEXT=$(kubectl config current-context) kubectl config use-context $CTX_CLUSTER1 2.确定入口 IP 和 端口： export INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.spec.ports[?(@.name==\"http2\")].nodePort}') export SECURE_INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.spec.ports[?(@.name==\"https\")].nodePort}') export INGRESS_HOST=$(kubectl get po -l istio=ingressgateway -n istio-system -o jsonpath='{.items[0].status.hostIP}') 3.恢复以前的kubectl上下文： kubectl config use-context $ORIGINAL_CONTEXT unset ORIGINAL_CONTEXT 4.打印 INGRESS_HOST 和 SECURE_INGRESS_PORT 的值： echo The ingress gateway of cluster1: address=$INGRESS_HOST, port=$SECURE_INGRESS_PORT （4）在网状网络配置中更新网关地址。编辑istio ConfigMap：注意，该地址显示在两个位置，第二个位于values.yaml： kubectl edit cm -n istio-system --context=$CTX_CLUSTER1 istio （三）部署 cluster2： （1）导出 cluster1 网关地址： export LOCAL_GW_ADDR=$(kubectl get po -l istio=ingressgateway -n istio-system -o jsonpath='{.items[0].status.hostIP}') \u0026\u0026 echo ${LOCAL_GW_ADDR} （2）部署 Istio 到 cluster2： kubectl create --context=$CTX_CLUSTER2 ns istio-system kubectl create --context=$CTX_CLUSTER2 secret generic cacerts -n istio-system --from-file=samples/certs/ca-cert.pem --from-file=samples/certs/ca-key.pem --from-file=samples/certs/root-cert.pem --from-file=samples/certs/cert-chain.pem CLUSTER_NAME=$(kubectl --context=$CTX_CLUSTER2 config view --minify=true -o jsonpath='{.clusters[].name}') istioctl manifest apply --context=$CTX_CLUSTER2 \\ --set profile=remote \\ --set values.global.mtls.enabled=true \\ --set values.gateways.enabled=true \\ --set values.security.selfSigned=false \\ --set values.global.controlPlaneSecurityEnabled=true \\ --set values.global.createRemoteSvcEndpoints=true \\ --set values.global.remotePilotCreateSvcEndpoint=true \\ --set values.global.remotePilotAddress=${LOCAL_GW_ADDR} \\ --set values.global.remotePolicyAddress=${LOCAL_GW_ADDR} \\ --set values.global.remoteTelemetryAddress=${LOCAL_GW_ADDR} \\ --set values.gateways.istio-ingressgateway.env.ISTIO_META_NETWORK=\"network2\" \\ --set values.global.network=\"network2\" \\ --set values.global.multiCluster.clusterName=${CLUSTER_NAME} \\ --set autoInjection.enabled=true （3）等待除 istio-ingressgateway 外的cluster2上的Istio Pod准备就绪，注意：将 cluster1 中的 Istio 控制平面配置为监视cluster2之前，istio-ingressgateway才准备就绪。 kubectl get pods --context=$CTX_CLUSTER2 -n istio-sys","date":"2020-05-14","objectID":"/%E5%A4%9A%E9%9B%86%E7%BE%A4%E7%AE%A1%E7%90%86/:0:0","tags":["Istio","实验"],"title":"多集群管理","uri":"/%E5%A4%9A%E9%9B%86%E7%BE%A4%E7%AE%A1%E7%90%86/"},{"categories":["Istio"],"content":"Istio 的安全功能十分强大，安全场景包括对网关的加密、服务间的访问控制、认证和授权。网关加密由 Ingress Gateway 实现，访问控制依赖 Mixer，认证和授权主要由 Citadel、Envoy 实现。 1. 网关加密 HTTPS 能最大化保证信息传输的安全，是当前互联网推荐的通讯方式。Istio 为 Gateway 提供了 HTTPS 加密支持。 ","date":"2020-05-14","objectID":"/%E6%9C%8D%E5%8A%A1%E4%BF%9D%E6%8A%A4/:0:0","tags":["Istio","实验"],"title":"服务保护","uri":"/%E6%9C%8D%E5%8A%A1%E4%BF%9D%E6%8A%A4/"},{"categories":["Istio"],"content":"1.1. 单向 TLS 网关 一般的 Web 应用都采用单向认证，即仅客户端验证服务端证书，无须在通信层做用户身份验证，而是在应用逻辑层保证 用户的合法登入。 ","date":"2020-05-14","objectID":"/%E6%9C%8D%E5%8A%A1%E4%BF%9D%E6%8A%A4/:1:0","tags":["Istio","实验"],"title":"服务保护","uri":"/%E6%9C%8D%E5%8A%A1%E4%BF%9D%E6%8A%A4/"},{"categories":["Istio"],"content":"1.1.1. 实验目标 在通过 HTTPS 访问 frontend 服务时只校验服务端，而服务端不校验客户端。对 Ingress Gateway 进行配置，为服务启用单向 TLS 保护，以 HTTPS 的形式对网格外部提供服务。 ","date":"2020-05-14","objectID":"/%E6%9C%8D%E5%8A%A1%E4%BF%9D%E6%8A%A4/:1:1","tags":["Istio","实验"],"title":"服务保护","uri":"/%E6%9C%8D%E5%8A%A1%E4%BF%9D%E6%8A%A4/"},{"categories":["Istio"],"content":"1.1.2. 实验演练 （1）使用工具生成客户端与服务端的证书与秘钥： git clone https://github.com/nicholasjackson/mtls-go-example.git cd mtls-go-example ./generate.sh www.weather.com mkdir ~/www.weather.com \u0026\u0026 mv 1_root/ 2_intermediate/ 3_application/ 4_client/ ~/www.weather.com/ （2）创建 Secret 对象，用于保存服务器的证书和秘钥： cd ~/www.weather.com kubectl create -n istio-system secret tls istio-ingressgateway-certs --key 3_application/private/www.weather.com.key.pem --cert 3_application/certs/www.weather.com.cert.pem 查看配置： （3）创建 Gateway 资源： kubectl apply -f chapter-files/security/gateway-tls-simple.yaml 查看配置： （4）创建 frontend 服务的 VirtualService 资源： kubectl apply -f chapter-files/security/vs-frontend-tls.yaml -n weather 查看配置： 可以看到这个 VirtualService 绑定了网关 weather-gateway，在 hosts 中添加了域名信息。外部访问 www.weather.com 的流量通过 Gateway 被路由到 frontend 的 v1 实例。 （5）先不使用 CA 证书，直接用 curl 命令向 www.weather.com 发送 HTTPS 请求，返回结果提示签发证书机构未经认证，无法识别： curl -v --resolve www.weather.com:443:10.105.28.142 https://www.weather.com -o /dev/null 使用 CA 证书再次发送 HTTPS 请求，收到成功的响应： curl -v --resolve www.weather.com:443:10.105.28.142 --cacert 2_intermediate/certs/ca-chain.cert.pem https://www.weather.com -o /dev/null 如果客户端不想校验服务端，可以使用 -k 或 –insecure 发送 HTTPS 请求忽略错误。 ","date":"2020-05-14","objectID":"/%E6%9C%8D%E5%8A%A1%E4%BF%9D%E6%8A%A4/:1:2","tags":["Istio","实验"],"title":"服务保护","uri":"/%E6%9C%8D%E5%8A%A1%E4%BF%9D%E6%8A%A4/"},{"categories":["Istio"],"content":"1.2. 双向 TLS 网关 双向 TLS 除了需要客户端认证服务端，还增加了服务端对客户端的认证 ","date":"2020-05-14","objectID":"/%E6%9C%8D%E5%8A%A1%E4%BF%9D%E6%8A%A4/:2:0","tags":["Istio","实验"],"title":"服务保护","uri":"/%E6%9C%8D%E5%8A%A1%E4%BF%9D%E6%8A%A4/"},{"categories":["Istio"],"content":"1.2.1. 实验目标 通过 HTTPS 访问 frontend 服务时，对服务端和客户端同时进行校验。 ","date":"2020-05-14","objectID":"/%E6%9C%8D%E5%8A%A1%E4%BF%9D%E6%8A%A4/:2:1","tags":["Istio","实验"],"title":"服务保护","uri":"/%E6%9C%8D%E5%8A%A1%E4%BF%9D%E6%8A%A4/"},{"categories":["Istio"],"content":"1.2.2. 实验演练 （1）创建一个 Kubernetes Secret，用于存储 CA 证书，服务端会使用这一证书来对客户端进行校验： cd ~/www.weather.com kubectl create -n istio-system secret generic istio-ingressgateway-ca-certs --from-file=2_intermediate/certs/ca-chain.cert.pem 查看配置 （2）创建 Gateway 资源： kubectl apply -f chapter-files/security/gateway-tls-mutual.yaml 查看配置：其中 tls 中 mode 字段值为 MUTUAL，并添加了 caCertificates 字段： ![image-20200514112250771](/Users/jiaheng/Library/Application Support/typora-user-images/image-20200514112250771.png) （3）创建 frontend 服务的 VirtualService 资源： kubectl apply -f chapter-files/security/vs-frontend-tls.yaml -n weather 查看配置： （4）不使用客户端证书和秘钥发送 HTTPS 请求，客户端校验没有通过： cd ~/www.weather.com curl -v --resolve www.weather.com:443:10.105.28.142 --cacert 2_intermediate/certs/ca-chain.cert.pem https://www.weather.com -o /dev/null 使用客户端证书及秘钥再次发送 HTTPS 请求，校验通过，返回成功： curl -v --resolve www.weather.com:443:10.105.28.142 --cacert 2_intermediate/certs/ca-chain.cert.pem --cert 4_client/certs/www.weather.com.cert.pem --key 4_client/private/www.weather.com.key.pem https://www.weather.com -o /dev/null ","date":"2020-05-14","objectID":"/%E6%9C%8D%E5%8A%A1%E4%BF%9D%E6%8A%A4/:2:2","tags":["Istio","实验"],"title":"服务保护","uri":"/%E6%9C%8D%E5%8A%A1%E4%BF%9D%E6%8A%A4/"},{"categories":["Istio"],"content":"1.3. SDS 加密网关 引入 Secret 发现服务为 Gateway 提供 HTTPS 的加密支持。 ","date":"2020-05-14","objectID":"/%E6%9C%8D%E5%8A%A1%E4%BF%9D%E6%8A%A4/:3:0","tags":["Istio","实验"],"title":"服务保护","uri":"/%E6%9C%8D%E5%8A%A1%E4%BF%9D%E6%8A%A4/"},{"categories":["Istio"],"content":"1.3.1. 实验目标 使用 Secret 发现服务配置入口网关的 TLS，在为主机名 www.weather.cn 更新证书和秘钥后，无需重启 Ingress 网关就能自动生效。 ","date":"2020-05-14","objectID":"/%E6%9C%8D%E5%8A%A1%E4%BF%9D%E6%8A%A4/:3:1","tags":["Istio","实验"],"title":"服务保护","uri":"/%E6%9C%8D%E5%8A%A1%E4%BF%9D%E6%8A%A4/"},{"categories":["Istio"],"content":"1.3.2. 实验演练 （1）使用 Helm 升级开启 Ingress 网关的 Secret 发现服务： helm upgrade istio install/kubernetes/helm/istio -n istio-system --set gateways.istio-ingressgateway.sds.enabled=true （2）新建证书和秘钥，并为网关创建新的 Secret： cd mtls-go-example ./generate.sh www.weather.cn mkdir ~/www.weather.cn \u0026\u0026 mv 1_root/ 2_intermediate/ 3_application/ 4_client/ ~/www.weather.cn/ cd ~/www.weather.cn kubectl create -n istio-system secret generic weather-credential --from-file=key=3_application/private/www.weather.cn.key.pem --from-file=cert=3_application/certs/www.weather.cn.cert.pem 查看配置： （3）更新 Gateway 资源： kubectl apply -f chapter-files/security/gateway-sds.yaml 查看配置： （4）更新 frontend 服务的 VirtualService 资源： kubectl apply -f chapter-files/security/vs-frontend-sds.yaml -n weather 查看配置： （5）使用 CA 证书发送 HTTPS 请求，收到成功响应： curl -v --resolve www.weather.cn:443:10.105.28.142 --cacert 2_intermediate/certs/ca-chain.cert.pem https://www.weather.cn -o /dev/null （6）更新证书和秘钥： kubectl -n istio-system delete secret weather-credential cd mtls-go-example ./generate.sh www.weather.cn mkdir ~/new.weather.cn \u0026\u0026 mv 1_root/ 2_intermediate/ 3_application/ 4_client/ ~/new.weather.cn/ kubectl create -n istio-system secret generic weather-credential --from-file=key=3_application/private/www.weather.cn.key.pem --from-file=3_application/certs/www.weather.cn.cert.pem 如果继续使用旧的 CA 证书发送 HTTPS 请求，期望访问失败，使用新的 CA 证书发送 HTTPS 请求，成功响应，事实相反，猜测 helm 升级失败，没有开启 sds 服务 。 cd ~ curl -v --resolve www.weather.cn:443:10.105.28.142 --cacert www.weather.cn/2_intermediate/certs/ca-chain.cert.pem https://www.weather.cn -o /dev/null curl -v --resolve www.weather.cn:443:10.105.28.142 --cacert new.weather.cn/2_intermediate/certs/ca-chain.cert.pem https://www.weather.cn -o /dev/null 2. 访问控制 访问控制是向应用程序注入安全构造的主要工具，对实际的代码实现没有影响。黑白名单是 Istio 实现访问控制的一种方式。 ","date":"2020-05-14","objectID":"/%E6%9C%8D%E5%8A%A1%E4%BF%9D%E6%8A%A4/:3:2","tags":["Istio","实验"],"title":"服务保护","uri":"/%E6%9C%8D%E5%8A%A1%E4%BF%9D%E6%8A%A4/"},{"categories":["Istio"],"content":"2.1. 黑名单 黑名单指拒绝特定条件的调用。 ","date":"2020-05-14","objectID":"/%E6%9C%8D%E5%8A%A1%E4%BF%9D%E6%8A%A4/:4:0","tags":["Istio","实验"],"title":"服务保护","uri":"/%E6%9C%8D%E5%8A%A1%E4%BF%9D%E6%8A%A4/"},{"categories":["Istio"],"content":"2.1.1. 实验目标 对 advertisement 服务创建一个黑名单，期望从 frontend 服务到 advertisement 服务的访问被拒绝。 ","date":"2020-05-14","objectID":"/%E6%9C%8D%E5%8A%A1%E4%BF%9D%E6%8A%A4/:4:1","tags":["Istio","实验"],"title":"服务保护","uri":"/%E6%9C%8D%E5%8A%A1%E4%BF%9D%E6%8A%A4/"},{"categories":["Istio"],"content":"2.1.2. 实验演练 （1）启用 Istio 策略检查： （2）启用黑名单配置： kubectl apply -f chapter-files/security/blacklist.yaml -n weather #查看配置 vim chapter-files/security/blacklist.yaml apiVersion: \"config.istio.io/v1alpha2\" kind: handler metadata: name: denycustomerhandler spec: compiledAdapter: denier params: status: code: 7 message: Not allowed --- apiVersion: \"config.istio.io/v1alpha2\" kind: instance metadata: name: denycustomerrequests spec: compiledTemplate: checknothing --- apiVersion: \"config.istio.io/v1alpha2\" kind: rule metadata: name: denycustomer spec: match: destination.labels[\"app\"] == \"advertisement\" \u0026\u0026 source.labels[\"app\"]==\"frontend\" actions: - handler: denycustomerhandler instances: [ denycustomerrequests ] （3）进入 frontend 容器访问 advertisement 服务： （4）清除策略： kubectl delete -f chapter-files/security/blacklist.yaml -n weather ","date":"2020-05-14","objectID":"/%E6%9C%8D%E5%8A%A1%E4%BF%9D%E6%8A%A4/:4:2","tags":["Istio","实验"],"title":"服务保护","uri":"/%E6%9C%8D%E5%8A%A1%E4%BF%9D%E6%8A%A4/"},{"categories":["Istio"],"content":"2.2. 白名单 白名单只允许特定属性的访问请求，拒绝不符合要求的访问请求。 ","date":"2020-05-14","objectID":"/%E6%9C%8D%E5%8A%A1%E4%BF%9D%E6%8A%A4/:5:0","tags":["Istio","实验"],"title":"服务保护","uri":"/%E6%9C%8D%E5%8A%A1%E4%BF%9D%E6%8A%A4/"},{"categories":["Istio"],"content":"2.2.1. 实验目标 advertisement 服务创建一个白名单，期望只有从 frontend 服务到 advertisement 服务的访问才被允许，来自其他源头对 advertisement 服务的访问都被拒绝。 ","date":"2020-05-14","objectID":"/%E6%9C%8D%E5%8A%A1%E4%BF%9D%E6%8A%A4/:5:1","tags":["Istio","实验"],"title":"服务保护","uri":"/%E6%9C%8D%E5%8A%A1%E4%BF%9D%E6%8A%A4/"},{"categories":["Istio"],"content":"2.2.2. 实验演练 （1）启用 Istio 策略检查： （2）启用白名单配置 kubectl apply -f chapter-files/security/whitelist.yaml -n weather #查看配置 vim chapter-files/security/whitelist.yaml apiVersion: config.istio.io/v1alpha2 kind: handler metadata: name: advertisementwhitelist spec: compiledAdapter: listchecker params: overrides: [\"frontend\"] blacklist: false --- apiVersion: config.istio.io/v1alpha2 kind: instance metadata: name: advertisementsource spec: compiledTemplate: listentry params: value: source.labels[\"app\"] --- apiVersion: config.istio.io/v1alpha2 kind: rule metadata: name: check spec: match: destination.labels[\"app\"] == \"advertisement\" actions: - handler: advertisementwhitelist instances: - advertisementsource （3）浏览器中访问前台页面，可以正常看到 advertisement 服务信息，说明 frontend 服务能正常访问 advertisement 服务。 进入 forecast 容器，访问 advertisement 服务，提示访问源不在白名单，访问被拒绝。 （4）清除策略： kubectl delete -f chapter-files/security/whitelist.yaml -n weather 3. 认证 Istio 通过双向 TLS 方式提供了从服务到服务的传输认证。 ","date":"2020-05-14","objectID":"/%E6%9C%8D%E5%8A%A1%E4%BF%9D%E6%8A%A4/:5:2","tags":["Istio","实验"],"title":"服务保护","uri":"/%E6%9C%8D%E5%8A%A1%E4%BF%9D%E6%8A%A4/"},{"categories":["Istio"],"content":"3.1. 实验目标 为 advertisement 服务设置认证策略和目的地规则，使得只有在网格内有 Sidecar 的服务才能访问 advertisement 服务，其他来源的访问都被拒绝。 ","date":"2020-05-14","objectID":"/%E6%9C%8D%E5%8A%A1%E4%BF%9D%E6%8A%A4/:6:0","tags":["Istio","实验"],"title":"服务保护","uri":"/%E6%9C%8D%E5%8A%A1%E4%BF%9D%E6%8A%A4/"},{"categories":["Istio"],"content":"3.2. 实验演练 （1）Kiali 容器（未注入 sidecar 的任意容器）中对 advertisement 服务发起请求，返回成功： （2）为 advertisement 服务启用双向 TLS 认证： kubectl apply -f chapter-files/security/advertisement-authentication-policy.yaml -n weather 查看配置： 此时 advertisement 服务仅接收使用 TLS 的加密请求，如果在 Kiali 容器中再次直接对 advertisement 服务发起请求，则由于请求没有加密，返回失败： （3）设置 advertisement 服务的 DestinationRule，指定访问 advertisement 服务的客户端需要使用双向 TLS： kubectl apply -f chapter-files/security/dr-advertisement-tls.yaml -n weather 查看配置： （4）进入 frontend 容器，对 advertisement 服务发起请求，由于 frontend 服务的 Proxy 会对请求加密后发出，所以返回成功。 从 Kiali 实时流量监控进一步确认，有流量从 frontend 服务发往 advertisement 服务，小锁图标表示流量经过加密。 4. 授权 Istio 使用基于角色的访问权限控制（RBAC 模型）来进行授权控制。 ","date":"2020-05-14","objectID":"/%E6%9C%8D%E5%8A%A1%E4%BF%9D%E6%8A%A4/:7:0","tags":["Istio","实验"],"title":"服务保护","uri":"/%E6%9C%8D%E5%8A%A1%E4%BF%9D%E6%8A%A4/"},{"categories":["Istio"],"content":"4.1. 命名空间级别的访问控制 Istio 能够轻松实现命名空间级别的访问控制，即对某个命名空间下的所有服务或部分服务设置策略，允许被其他命名空间的服务访问。 ","date":"2020-05-14","objectID":"/%E6%9C%8D%E5%8A%A1%E4%BF%9D%E6%8A%A4/:8:0","tags":["Istio","实验"],"title":"服务保护","uri":"/%E6%9C%8D%E5%8A%A1%E4%BF%9D%E6%8A%A4/"},{"categories":["Istio"],"content":"4.1.1. 实验目标 只允许 istio-system 命名空间下的服务（如 ingressgateway）访问 weather 命名空间下的服务，其他命名空间下的服务不能访问 weather 命名空间下的服务。 ","date":"2020-05-14","objectID":"/%E6%9C%8D%E5%8A%A1%E4%BF%9D%E6%8A%A4/:8:1","tags":["Istio","实验"],"title":"服务保护","uri":"/%E6%9C%8D%E5%8A%A1%E4%BF%9D%E6%8A%A4/"},{"categories":["Istio"],"content":"4.1.2. 实验演练 （1）为 weather 设置命名空间级别的认证策略： kubectl apply -f chapter-files/security/weather-authentication-policy.yaml 查看配置： 这时在浏览器中访问前台页面，显示 “upstream connect error or disconnect/reset before headers. reset reason: connection termination”，原因是认证策略要求对 weather 命名空间下的服务访问 必须是加密的请求。 （2）为 weather 命名空间下的 frontend、advertisement、forecast 和 recommendation 等服务设置 DestinationRule 的 TLS 策略，则请求端的 Proxy 会对这些目标服务的请求进行加密： kubectl apply -f chapter-files/security/dr-all-tls.yaml -n weather 以 frontend 服务为例，查看配置： 再次在浏览器中访问前台页面，查询天气，返回正常。 （3）启用 Istio 对 weather 命名空间下所有服务的访问控制： kubectl apply -f chapter-files/security/weather-rbac-config.yaml 查看配置： 浏览器访问前台服务，看到 “RBAC: access denied”，原因是 Istio 访问控制默认采用拒绝策略，这就要求必须显示声明授权策略才能成功访问到服务。 （4）配置授权策略： kubectl apply -f chapter-files/security/weather-authorization.yaml -n weather 查看配置： 完成配置后等待几秒，浏览器再次访问前台服务，一切正常。因为在浏览器访问时通过 istio-system 下的 ingressgateway 入口访问 weather 命名空间下的服务，根据授权策略，这是被允许的。 进入 default 命名空间下的 sleep 服务，对 weather 命名空间下的 advertisement 服务发起请求，根据授权策略，来自其他命名空间的访问都被拒绝。 ","date":"2020-05-14","objectID":"/%E6%9C%8D%E5%8A%A1%E4%BF%9D%E6%8A%A4/:8:2","tags":["Istio","实验"],"title":"服务保护","uri":"/%E6%9C%8D%E5%8A%A1%E4%BF%9D%E6%8A%A4/"},{"categories":["Istio"],"content":"4.2. 服务级别的访问控制 除了在命名空间范围内控制访问，Istio 还可以精确地对单个服务进行访问控制。 ","date":"2020-05-14","objectID":"/%E6%9C%8D%E5%8A%A1%E4%BF%9D%E6%8A%A4/:9:0","tags":["Istio","实验"],"title":"服务保护","uri":"/%E6%9C%8D%E5%8A%A1%E4%BF%9D%E6%8A%A4/"},{"categories":["Istio"],"content":"4.2.1. 实验目标 Istio 在服务级别设置访问策略，依次开放对 frontend 服务和 advertisement 服务的访问权限。 ","date":"2020-05-14","objectID":"/%E6%9C%8D%E5%8A%A1%E4%BF%9D%E6%8A%A4/:9:1","tags":["Istio","实验"],"title":"服务保护","uri":"/%E6%9C%8D%E5%8A%A1%E4%BF%9D%E6%8A%A4/"},{"categories":["Istio"],"content":"4.2.2. 实验演练 （1）参照上节设置命名空间级别的认证策略，设置 DestinationRule 的 TLS 策略，启用 Istio 对 weather 命名空间级别的访问控制。 （2）开放访问 frontend 服务的权限： kubectl apply -f chapter-files/security/frontend-authorization.yaml 查看授权策略： 等待几秒后，用浏览器可以正常访问前台服务，但不能访问广告服务。广告服务接口调用返回 “RBAC: access denied”。 （3）开放访问 advertisement 服务的权限： kubectl -n weather apply -f chapter-files/security/advertisement-authorization.yaml 查看授权策略： 等待几秒后，在浏览器再次访问前台服务和广告服务，均正常显示。 ","date":"2020-05-14","objectID":"/%E6%9C%8D%E5%8A%A1%E4%BF%9D%E6%8A%A4/:9:2","tags":["Istio","实验"],"title":"服务保护","uri":"/%E6%9C%8D%E5%8A%A1%E4%BF%9D%E6%8A%A4/"},{"categories":["Istio"],"content":"之前的灰度发布，包括策略配置和指标分析，都需要人工干预。在持续交付过程中，为了解决部署和管理的复杂性，需要通过自动化工具实现基于权重的灰度发布。 Flagger 是一个基于 Kubernetes 和 Istio 提供灰度发布、监控和告警等功能的开源软件，通过使用 Istio 的流量路由和 Prometheus 指标来分析应用程序的行为，从而实现灰度版本的自动部署，可以使用 Webhook 扩展 Canary 分析，以运行集成测试，压力测试或其他自定义测试。Flagger 将控制发布行为的参数定义在一个名为 Canary 的 CRD 资源中，逐渐将流量转移到灰度版本，同时测量关键的性能指标，例如 HTTP 请求成功率、请求平均持续时间和Pod 健康状况，并根据 KPI 分析逐步完成或取消灰度发布，并将结果发布给 Slack。Flagger 具体的灰度发布部署流程如下图： 1. 预先准备：Flagger 部署 helm repo add flagger https://flagger.app kubectl apply -f https://raw.githubusercontent.com/weaveworks/flagger/master/artifacts/flagger/crd.yaml helm upgrade -i flagger flagger/flagger \\--namespace=istio-system \\--set crd.create=false \\--set meshProvider=istio \\--set metricsServer=http://prometheus:9090 部署验证 2. 正常发布 ","date":"2020-05-13","objectID":"/%E8%87%AA%E5%8A%A8%E5%8C%96%E7%81%B0%E5%BA%A6%E5%8F%91%E5%B8%83/:0:0","tags":["Istio","实验"],"title":"自动化灰度发布","uri":"/%E8%87%AA%E5%8A%A8%E5%8C%96%E7%81%B0%E5%BA%A6%E5%8F%91%E5%B8%83/"},{"categories":["Istio"],"content":"2.1. 实验目标 使用 Flagger 对 ad 服务的 v2 版本进行灰度发布，自动调整流量比例，直至 v2 版本全部接管流量，完成灰度发布。 ","date":"2020-05-13","objectID":"/%E8%87%AA%E5%8A%A8%E5%8C%96%E7%81%B0%E5%BA%A6%E5%8F%91%E5%B8%83/:1:0","tags":["Istio","实验"],"title":"自动化灰度发布","uri":"/%E8%87%AA%E5%8A%A8%E5%8C%96%E7%81%B0%E5%BA%A6%E5%8F%91%E5%B8%83/"},{"categories":["Istio"],"content":"2.2. 实验演练 （1）部署 ad 服务的工作负载： kubectl apply -f chapter-files/canary-release/ad-deployment.yaml -n weather （2）创建 Canary 资源，其中定义了自动化发布的参数： kubectl apply -f chapter-files/canary-release/auto-canary.yaml -n weather 等待几秒，Flagger 会创建用于自动化灰度发布的相关资源。 查看 Canary 配置： （3）进入 frontend 容器，开始对 ad 服务发起连续的请求，将请求间隔设为 1 秒： kubectl exec -it frontend-v1-75d4648dc6-fqx9c -n weather bash for i in seq 1 1000; do curl http://ad.weather:3003/ad --silent --w \"Status: %{http_code}\\n\" -o /dev/null; sleep 1; done 新建一个 Bash 窗口，执行以下命令更新 ad 服务的 Deployment 镜像，触发对 v2 版本的灰度发布任务： kubectl -n weather set image deployment/ad ad=istioweather/advertisement:v2 Flagger 在检查到 Deployment 的镜像版本发生变化后，会部署一个镜像为 v2 版本的临时 Deployment，并根据检测到的 Metrics 配置逐步调整权重，实时的流量变化如下图所示： （4）执行如下命令查看 Canary 资源对象的 Events 字段，可以获得整个灰度发布过程的信息： kubectl -nweather describe canary ad 如果检测到的 Metrics 值始终低于设定的门限值，Flagger 就会按照设定的步长（20%）逐步增加 v2 版本的流量比例。在达到 100% 后，Flagger 会将 ad-primary 的 Deployment 的镜像改为 v2，删掉临时的 Deployment，完成对 v2 版本的灰度发布。 3. 异常发布 ","date":"2020-05-13","objectID":"/%E8%87%AA%E5%8A%A8%E5%8C%96%E7%81%B0%E5%BA%A6%E5%8F%91%E5%B8%83/:2:0","tags":["Istio","实验"],"title":"自动化灰度发布","uri":"/%E8%87%AA%E5%8A%A8%E5%8C%96%E7%81%B0%E5%BA%A6%E5%8F%91%E5%B8%83/"},{"categories":["Istio"],"content":"3.1. 实验目标 对 ad 服务发布一个有 Bug 的 v3 版本，这个版本会导致对 ad 服务的请求失败。在灰度发布过程中，Flagger 在检查到多次访问失败后会终止灰度发布任务，并且自动回滚到前一个版本。 ","date":"2020-05-13","objectID":"/%E8%87%AA%E5%8A%A8%E5%8C%96%E7%81%B0%E5%BA%A6%E5%8F%91%E5%B8%83/:3:0","tags":["Istio","实验"],"title":"自动化灰度发布","uri":"/%E8%87%AA%E5%8A%A8%E5%8C%96%E7%81%B0%E5%BA%A6%E5%8F%91%E5%B8%83/"},{"categories":["Istio"],"content":"3.2. 实验演练 （1）进入 frontend 容器，对 ad 服务发起连续的请求，将请求间隔设为 1 秒： kubectl exec -it frontend-v1-75d4648dc6-fqx9c -n weather bash for i in seq 1 1000; do curl http://ad.weather:3003/ad --silent --w \"Status: %{http_code}\\n\" -o /dev/null; sleep 1; done （2）新建一个 Bash 窗口，执行以下命令更新 ad 服务的 Deployment 镜像，触发对 v3 版本的灰度发布： kubectl -n weather set image deployment/ad ad=istioweather/advertisement:v3 （3）v3 版本是一个有 Bug 的版本，会随机返回 “500” 状态码。在灰度发布过程中，如果 Flagger 检测到错误率大于 5%，且这种失败的次数达到 3 次，流量就会被自动切回 Primary，并删掉 临时的 Deployment，宣布发布失败。 kubectl -nweather describe canary ad ","date":"2020-05-13","objectID":"/%E8%87%AA%E5%8A%A8%E5%8C%96%E7%81%B0%E5%BA%A6%E5%8F%91%E5%B8%83/:4:0","tags":["Istio","实验"],"title":"自动化灰度发布","uri":"/%E8%87%AA%E5%8A%A8%E5%8C%96%E7%81%B0%E5%BA%A6%E5%8F%91%E5%B8%83/"},{"categories":["Istio"],"content":"1. 预先准备：将所有流量路由到各个服务 v1 版本 将 fronted，advertisement 和 forecast 服务的 v1 版本部署到集群中，命名空间是 weather。执行如下命令确认 Pod 成功启动 kubectl get po -n weather ![image-20200513181844897](/Users/jiaheng/Library/Application Support/typora-user-images/image-20200513181844897.png) 对每个服务都创建各自的 VirtualService 和 DestinationRule 资源，将访问请求路由到所有服务 v1 版本 kubectl apply -f install/destination-rule-v1.yaml -n weather kubectl apply -f install/virtual-service-v1.yaml -n weather 查看配置的路由规则，以 forecast 服务为例： kubectl get vs -n weather forecast-route -o yaml 在浏览器多次加载前台页面，并查询城市天气信息，确认显示正常。各个服务调用关系如图所示： 2. 基于流量比例的路由 ","date":"2020-05-13","objectID":"/%E7%81%B0%E5%BA%A6%E5%8F%91%E5%B8%83/:0:0","tags":["Istio","实验"],"title":"灰度发布","uri":"/%E7%81%B0%E5%BA%A6%E5%8F%91%E5%B8%83/"},{"categories":["Istio"],"content":"2.1. 实验目标 用户需要软件能够根据不同的天气状况推荐合适的穿衣和运动信息。于是开发人员增加了 recommendation 新服务，并升级 forecast 服务到 v2 版本来调用 recommendation 服务。在新特性上线时，运维 人员首先部署 recommendation 服务和 forecast 服务的 v2 版本，并对 forecast 服务的 v2 版本进行灰度发布 ","date":"2020-05-13","objectID":"/%E7%81%B0%E5%BA%A6%E5%8F%91%E5%B8%83/:1:0","tags":["Istio","实验"],"title":"灰度发布","uri":"/%E7%81%B0%E5%BA%A6%E5%8F%91%E5%B8%83/"},{"categories":["Istio"],"content":"2.2. 实验演练 （1）部署 recommendation 服务和 forecast 服务的 v2 版本： kubectl apply -f install/recommendation-service/recommendation-all.yaml -n weather kubectl apply -f install/forecast-service/forecast-v2-deployment.yaml -n weather 执行如下命令确认部署成功： （2）执行如下命令更新 forecast 服务的 DestinationRule： kubectl apply -f install/forecast-service/forecast-v2-destination.yaml -n weather 查看下发成功的配置，可以看到增加了 v2 版本 subset 的定义： 这时在浏览器中查询天气，不会出现推荐信息，因为所有流量依然都被路由到 forecast 服务的 v1 版本，不会调用 recommendation 服务。 （3）执行如下命令配置 forecast 服务的路由规则： kubectl apply -f chapter-files/canary-release/vs-forecast-weight-based-50.yaml -n weather 查看 forecast 服务的 VirtualService 配置，其中的 weight 字段显示了相应服务的流量占比： 在浏览器中查看配置后的效果：多次刷新页面查询天气，可以发现在大约50%的情况下不显示推荐服务，表示调用了 forecast 服务的 v1 版本；在另外 50% 的情况下显示推荐服务，表示调用了 forecast 服务的 v2 版本。可以通过 Kiali 进一步确认流量数据，如下图： （4）逐步增加 forecast 服务的 v2 版本的流量占比，直到流量全部被路由到 v2 版本： kubectl apply -f chapter-files/canary-release/vs-forecast-weight-based-v2.yaml -n weather 查看 forecast 服务的 VirtualService 配置，可以看到 v2 版本的流量占比被设置为 100： 在浏览器中查看配置后的效果：多次刷新页面查询天气，每次都会出现推荐信息，说明访问请求都被路由到了 forecast 服务的 v2 版本。可以通过 Kiali 进一步确认流量数据，如下图： （5）保留 forecast 服务的老版本 v1 一段时间，在确认 v2 版本的各性能指标稳定后，删除老版本 v1 的所有资源，完成灰度发布。 3. 基于请求内容的路由 Istio 可以基于不同的请求内容将流量路由到不同的版本，这种策略一方面被应用于 AB 测试的场景中，另一方面配合基于流量比例的规则被应用于较复杂的灰度发布场景中，例如 组合条件路由。 ","date":"2020-05-13","objectID":"/%E7%81%B0%E5%BA%A6%E5%8F%91%E5%B8%83/:2:0","tags":["Istio","实验"],"title":"灰度发布","uri":"/%E7%81%B0%E5%BA%A6%E5%8F%91%E5%B8%83/"},{"categories":["Istio"],"content":"3.1. 实验目标 在生产环境中同时上线了 forecast 服务的 v1 和 v2 版本，产品经理期望让不同的终端用户访问不同的版本，例如：让使用 Chrome 留恋其的用户看到推荐信息，但让使用其他 终端的用户看不到推荐信息。 ","date":"2020-05-13","objectID":"/%E7%81%B0%E5%BA%A6%E5%8F%91%E5%B8%83/:3:0","tags":["Istio","实验"],"title":"灰度发布","uri":"/%E7%81%B0%E5%BA%A6%E5%8F%91%E5%B8%83/"},{"categories":["Istio"],"content":"3.2. 实验演练 更新 forecast 服务的 DestinationRule，执行如下命令配置 forecast 服务的路由规则： kubectl apply -f chapter-files/canary-release/vs-forecast-header-based.yaml -n weather 在浏览器中查看配置后的效果：用 Chrome 多次查询天气信息，发现始终显示推荐信息，说明访问到 forecast 服务的 v2 版本；用 Firefox 多次查询天气信息， 发现始终不显示推荐信息，说明访问到 forecast 服务的 v1 版本。 Chrome 效果： Firefox 效果： ","date":"2020-05-13","objectID":"/%E7%81%B0%E5%BA%A6%E5%8F%91%E5%B8%83/:4:0","tags":["Istio","实验"],"title":"灰度发布","uri":"/%E7%81%B0%E5%BA%A6%E5%8F%91%E5%B8%83/"},{"categories":["Istio"],"content":"3.3. 工作原理 4. 组合条件路由 一些复杂的灰度发布场景需要使用上面两种路由规则的组合形式 ","date":"2020-05-13","objectID":"/%E7%81%B0%E5%BA%A6%E5%8F%91%E5%B8%83/:5:0","tags":["Istio","实验"],"title":"灰度发布","uri":"/%E7%81%B0%E5%BA%A6%E5%8F%91%E5%B8%83/"},{"categories":["Istio"],"content":"4.1. 实验目标 在生产环境中同时上线了 frontend 服务的 v1 和 v2 版本，v1 版本的按钮颜色是绿色的，v2 版本的按钮颜色是蓝色的。产品经理期望使用 Android 操作系统的一半用户看到的是 v1 版本，另一半用户看到的是 v2 版本；使用其他操作系统的用户看到的总是 v1 版本。 ","date":"2020-05-13","objectID":"/%E7%81%B0%E5%BA%A6%E5%8F%91%E5%B8%83/:6:0","tags":["Istio","实验"],"title":"灰度发布","uri":"/%E7%81%B0%E5%BA%A6%E5%8F%91%E5%B8%83/"},{"categories":["Istio"],"content":"4.2. 实验演练 （1）部署 frontend 服务的 v2 版本： kubectl apply -f install/frontend-service/frontend-v2-deployment.yaml -n weather 执行如下命令确认部署成功： （2）执行如下命令更新 frontend 服务的 DestinationRule： kubectl apply -f install/frontend-service/frontend-v2-destination.yaml -n weather 查看下发的 DestinationRule，发现增加了 v2 版本 subset 的定义： （3）执行如下命令配置 frontend 服务的路由策略： kubectl apply -f chapter-files/canary-release/vs-frontend-combined-condition.yaml -n weather 查看配置后的效果：用 Android 手机多次查询前台页面，有一半概率显示绿色按钮，另一半概率显示蓝色按钮。在 Mac 操作系统上多次查询前台页面，始终显示绿色按钮。 ","date":"2020-05-13","objectID":"/%E7%81%B0%E5%BA%A6%E5%8F%91%E5%B8%83/:7:0","tags":["Istio","实验"],"title":"灰度发布","uri":"/%E7%81%B0%E5%BA%A6%E5%8F%91%E5%B8%83/"},{"categories":["Istio"],"content":"4.3. 工作原理 5. 多服务灰度发布 在一些系统中往往需要对同一应用的多个组件同时进行灰度发布，这时需要将这些服务串联起来。例如，只有测试账号才能访问这些服务的新版本并进行功能测试； 其他用户只能访问老版本，不能使用新功能。 ","date":"2020-05-13","objectID":"/%E7%81%B0%E5%BA%A6%E5%8F%91%E5%B8%83/:8:0","tags":["Istio","实验"],"title":"灰度发布","uri":"/%E7%81%B0%E5%BA%A6%E5%8F%91%E5%B8%83/"},{"categories":["Istio"],"content":"5.1. 实验目标 运维人员对 frontend 和 forecast 两个服务同时进行灰度发布，frontend 服务新增 v2 版本，界面的按钮变为蓝色，forecast 服务新增 v2 版本，增加了推荐信息。 测试人员在用账号 tester 访问天气应用时会看到这两个服务的 v2 版本，其他用户只能看到这两个服务的 v1 版本，不会出现服务版本交叉调用的情况。 ","date":"2020-05-13","objectID":"/%E7%81%B0%E5%BA%A6%E5%8F%91%E5%B8%83/:9:0","tags":["Istio","实验"],"title":"灰度发布","uri":"/%E7%81%B0%E5%BA%A6%E5%8F%91%E5%B8%83/"},{"categories":["Istio"],"content":"5.2. 实验演练 （1）如上标题2，在集群中部署 recommendation 服务和 forecast 服务的 v2 版本，并更新 forecast 服务的 DestinationRule，在 DestinationRule 中增加对 v2 版本 subset 的定义。如上标题4，在集群中部署 frontend 服务的 v2 版本，并更新 frontend 服务的 DestinationRule，增加对 v2 版本 subset 的定义。 （2）对非入口服务 forecast 使用 match 的 sourceLabels 创建 VirtualService kubectl apply -f chapter-files/canary-release/vs-forecast-multiservice-release.yaml -n weather （3）对入口服务 frontend 设置基于访问内容的规则 kubectl apply -f chapter-files/canary-release/vs-frontend-multiservice-release.yaml -n weather ","date":"2020-05-13","objectID":"/%E7%81%B0%E5%BA%A6%E5%8F%91%E5%B8%83/:10:0","tags":["Istio","实验"],"title":"灰度发布","uri":"/%E7%81%B0%E5%BA%A6%E5%8F%91%E5%B8%83/"},{"categories":["Istio"],"content":"5.3. 工作原理 查看 forecast 服务的路由配置： 上面的配置使得只有带 “version: v2” 标签的 Pod 实例的流量，才能进入 forecast 服务的新版本 v2 实例。 查看 frontend 服务的路由配置： 对于测试账号即 Cookie 带有 “user=tester” 信息的请求，Istio 会将这种特殊用户的流量导入 frontend 服务的 v2 版本 的 Pod 实例；根据 forecast 服务的路由规则，这些流量在访问 forecast 服务时会被路由到 forecast 服务的 v2 版本的 Pod 实例。 对于其他用户的请求，Istio 会将流量导入 frontend 服务的 v1 版本的 Pod 实例；这些流量在访问 forecast 服务时会被路由到 forecast 服务 的老版本 v1 的 Pod 实例。 综上，整个服务链路上的一次访问流量要么都被路由到两个服务的 v1 版本的 Pod 实例，要么都被路由到两个服务的 v2 版本的 Pod 实例。 6. TCP 服务灰度发布 如上的灰度发布场景主要针对 HTTP 服务，下面以 Istio 安装包中的 TCP Echo 服务为例进行 TCP 服务的灰度发布。 实验目标 在 Kubernetes 集群上部署 TCP Echo 服务的 v1 和 v2 版本，对两个版本实施基于流量比例的策略。 实验演练 （1）部署 TCP Echo 服务的 v1 版本： kubectl apply -f install/tcp-echo-service/tcp-echo-v1.yaml -n weather （2）配置对应的 DestinationRule 和 VirtualService： kubectl apply -f chapter-files/canary-release/vs-tcp-echo-weight-based-20.yaml -n weather 通过 Ingress Gateway 访问 TCP Echo 服务： （3）部署 TCP Echo 服务的 v2 版本： kubectl apply -f install/tcp-echo-service/tcp-echo-v2.yaml -n weather （4）对 TCP Echo 服务配置路由规则，使 80% 的请求流量访问 v1 版本，20% 的请求流量访问 v2 版本： kubectl apply -f chapter-files/canary-release/vs-tcp-echo-weight-based-20.yaml -n weather （5）对 TCP Echo 服务发起 10 次请求： 从返回结果看出，有 80% 的请求路由到了 TCP Echo 服务的 v1 版本，剩下 20% 的请求路由到了 TCP Echo 服务的 v2 版本。 ","date":"2020-05-13","objectID":"/%E7%81%B0%E5%BA%A6%E5%8F%91%E5%B8%83/:11:0","tags":["Istio","实验"],"title":"灰度发布","uri":"/%E7%81%B0%E5%BA%A6%E5%8F%91%E5%B8%83/"},{"categories":["Istio"],"content":"Istio 的 Proxy 通过拦截系统中所有网络的通信，来提供可用于获得整个应用的可观察性的指标和数据。比如：若一个服务响应过慢是由 CPU 占用率过高导致的，则可以采用弹性伸缩来解决问题；若一个服务响应过慢是由于访问量突然增大引起的，则可以采取熔断限流等措施。 1. 预先准备：安装插件 在使用 Istio 的流量功能前，需要在集群中安装 Jaeger、Prometheus、Grafana 和 Kiali 等插件。如果这些插件在部署时未启用，则可以使用 helm upgrade 命令进行启用。 Grafana：–set grafana.enabled=true Kiali：–set kiali.enabled=true Prometheus：–set prometheus.enabled=true Tracing：–set tracing.enabled=true 在登录 Kiali 时需要输入用户名和密码，执行以下命令创建 Kiali 的 Secret： kubectl apply -f chapter-files/telemetry/kiali-secret.yaml kiali-secret.yaml 保存了经过 Base64 编码的用户和密码信息，初始的用户名是 admin，密码也是 admin。可以在 Secret 中按需修改这部分信息， 在创建 Secret 后有些 Kiali 版本需要重启才能生效。 所有插件都安装成功后，为了在集群外的浏览器中打开界面，需配置每个插件的对外访问方式，如下命令使用 Gateway 设置 HTTP 访问各插件： kubectl apply -f chapter-files/telemetry/access-addons.yaml Gateway 资源创建完成后，在浏览器输入对应插件的地址进行访问： Kiali：http://:15029/kiali Prometheus：http://:15030/ Grafana：http://:15031/ Tracing：http://:15032/ 2. 调用链跟踪 调用链跟踪是一种用于分析和监控微服务应用的方法，OpenTracing 是分布式跟踪的 API 规范，提供了统一的接口，方便开发者在自己的服务中集成 一种或多种分布式追踪的实现。Istio 使用 Jaeger 作为调用链的引擎。 一次调用链跟踪（Trace）由若干个 Span 组成，Span 是请求数据的最小单位，包括这次跟踪的开始时间、结束时间、操作名称及一组标签和日志。尽管 Proxy 可以自动生成 Span，但是应用程序需要传播响应的 HTTP Header，这样这些 Span 才能被正确关联到一个跟踪。因此，需要应用程序手机传入请求 中的以下 HTTP Header 并将其传播出去。 x-request-id x-b3-traceid x-b3-spanid x-b3-parentspanid x-b3-sampled x-b3-flags x-ot-span-context 访问 Jaeger 页面：http://10.12.1.100:30655/ 在浏览器中访问前台页面查询天气，在这个过程中会产生调用链信息。从界面左边面板的 Service 下拉列表中选择 frontend.weather，并单击左下角 “Find Traces” 按钮检索调用链，右侧会显示调用链记录的列表： 调用链数据的频繁采集会对系统造成严重的性能损失，Istio 为了提高系统性能，设置默认采样率为 1%。如果遇到调用链不能正常获取的情况，则需要调整 跟踪取样率。Istio 的跟踪取样率被设置为 istio-pilot 的环境变量 PILOT_TRACE_SAMPLING，取值范围为 1.0 ~ 100.0： kubectl -n istio-system edit deploy istio-pilot 单击某一个跟踪记录，可以查看其包含的所有 Span 和每个 Span 的消耗时间。如下的跟踪信息有 4 个 Span，涉及 3 个服务，一共耗时 7.21 毫秒。 在跟踪（Trace）的详情页可以单击某个 Span 来查看其详细信息，包括被调用的 URL、HTTP 方法、响应状态和其他 Header 信息 此外，Jaeger 还提供了 Compare 功能（用来对比不同 Trace 信息）和 Dependencies 视图（展现各个服务如何相互调用）。 3. 指标监控 Metrics 是服务的监控指标数据，指标数据通常分为 4 类：Counter、Gauge、Histogram 和 Summary。Istio 内置了几种常见的度量标准类型， 也可以创建自定义指标。 ","date":"2020-05-11","objectID":"/%E6%B5%81%E9%87%8F%E7%9B%91%E6%8E%A7/:0:0","tags":["Istio","实验"],"title":"流量监控","uri":"/%E6%B5%81%E9%87%8F%E7%9B%91%E6%8E%A7/"},{"categories":["Istio"],"content":"Prometheus Istio 默认启用了预置的 Prometheus 组件，配置文件被定义在 istio-system 命名空间下的名称为 Prometheus 的 ConfigMap 中，并被挂载到 Prometheus 的 Pod 内。Prometheus 容器内的配置文件 “/etc/prometheus/prometheus.yaml” 如下： kubectlgetcmprometheus-nistio-system-oyamlapiVersion:v1data:prometheus.yml:|- global:scrape_interval:15sscrape_configs:- job_name:'istio-mesh'kubernetes_sd_configs:- role:endpointsnamespaces:names:- istio-systemrelabel_configs:- source_labels:[__meta_kubernetes_service_name,__meta_kubernetes_endpoint_port_name]action:keepregex:istio-telemetry;prometheus# Scrape config for envoy stats- job_name:'envoy-stats'metrics_path:/stats/prometheuskubernetes_sd_configs:- role:podrelabel_configs:- source_labels:[__meta_kubernetes_pod_container_port_name]action:keepregex:'.*-envoy-prom'- source_labels:[__address__,__meta_kubernetes_pod_annotation_prometheus_io_port]action:replaceregex:([^:]+)(?::\\d+)?;(\\d+)replacement:$1:15090target_label:__address__- action:labelmapregex:__meta_kubernetes_pod_label_(.+)- source_labels:[__meta_kubernetes_namespace]action:replacetarget_label:namespace- source_labels:[__meta_kubernetes_pod_name]action:replacetarget_label:pod_name- job_name:'istio-policy'kubernetes_sd_configs:- role:endpointsnamespaces:names:- istio-systemrelabel_configs:- source_labels:[__meta_kubernetes_service_name,__meta_kubernetes_endpoint_port_name]action:keepregex:istio-policy;http-monitoring- job_name:'istio-telemetry'kubernetes_sd_configs:- role:endpointsnamespaces:names:- istio-systemrelabel_configs:- source_labels:[__meta_kubernetes_service_name,__meta_kubernetes_endpoint_port_name]action:keepregex:istio-telemetry;http-monitoring- job_name:'pilot'kubernetes_sd_configs:- role:endpointsnamespaces:names:- istio-systemrelabel_configs:- source_labels:[__meta_kubernetes_service_name,__meta_kubernetes_endpoint_port_name]action:keepregex:istio-pilot;http-monitoring- job_name:'galley'kubernetes_sd_configs:- role:endpointsnamespaces:names:- istio-systemrelabel_configs:- source_labels:[__meta_kubernetes_service_name,__meta_kubernetes_endpoint_port_name]action:keepregex:istio-galley;http-monitoring- job_name:'citadel'kubernetes_sd_configs:- role:endpointsnamespaces:names:- istio-systemrelabel_configs:- source_labels:[__meta_kubernetes_service_name,__meta_kubernetes_endpoint_port_name]action:keepregex:istio-citadel;http-monitoring# scrape config for API servers- job_name:'kubernetes-apiservers'kubernetes_sd_configs:- role:endpointsnamespaces:names:- defaultscheme:httpstls_config:ca_file:/var/run/secrets/kubernetes.io/serviceaccount/ca.crtbearer_token_file:/var/run/secrets/kubernetes.io/serviceaccount/tokenrelabel_configs:- source_labels:[__meta_kubernetes_service_name,__meta_kubernetes_endpoint_port_name]action:keepregex:kubernetes;https# scrape config for nodes (kubelet)- job_name:'kubernetes-nodes'scheme:httpstls_config:ca_file:/var/run/secrets/kubernetes.io/serviceaccount/ca.crtbearer_token_file:/var/run/secrets/kubernetes.io/serviceaccount/tokenkubernetes_sd_configs:- role:noderelabel_configs:- action:labelmapregex:__meta_kubernetes_node_label_(.+)- target_label:__address__replacement:kubernetes.default.svc:443- source_labels:[__meta_kubernetes_node_name]regex:(.+)target_label:__metrics_path__replacement:/api/v1/nodes/${1}/proxy/metrics# Scrape config for Kubelet cAdvisor.## This is required for Kubernetes 1.7.3 and later, where cAdvisor metrics# (those whose names begin with 'container_') have been removed from the# Kubelet metrics endpoint. This job scrapes the cAdvisor endpoint to# retrieve those metrics.## In Kubernetes 1.7.0-1.7.2, these metrics are only exposed on the cAdvisor# HTTP endpoint; use \"replacement: /api/v1/nodes/${1}:4194/proxy/metrics\"# in that case (and ensure cAdvisor's HTTP server hasn't been disabled with# the --cadvisor-port=0 Kubelet flag).## This job is not necessary and should be removed in Kubernetes 1.6 and# earlier versions, or it will cause the metrics to be scraped twice.- job_name:'kubernetes-cadvisor'scheme:httpstls_config:ca_file:","date":"2020-05-11","objectID":"/%E6%B5%81%E9%87%8F%E7%9B%91%E6%8E%A7/:0:1","tags":["Istio","实验"],"title":"流量监控","uri":"/%E6%B5%81%E9%87%8F%E7%9B%91%E6%8E%A7/"},{"categories":["Istio"],"content":"Istio Mesh Dashboard 提供了网格的全局摘要视图，在多次访问应用产生流量后，可以在仪表盘实时看到全局的数据请求量、成功率，以及服务和工作负载 的列表等信息。 ","date":"2020-05-11","objectID":"/%E6%B5%81%E9%87%8F%E7%9B%91%E6%8E%A7/:0:2","tags":["Istio","实验"],"title":"流量监控","uri":"/%E6%B5%81%E9%87%8F%E7%9B%91%E6%8E%A7/"},{"categories":["Istio"],"content":"Istio Service Dashboard 提供了每个服务的 HTTP、gRPC 和 TCP 的请求和响应的度量指标，以及有关此服务的客户端和服务端工作负载的指标。 ","date":"2020-05-11","objectID":"/%E6%B5%81%E9%87%8F%E7%9B%91%E6%8E%A7/:0:3","tags":["Istio","实验"],"title":"流量监控","uri":"/%E6%B5%81%E9%87%8F%E7%9B%91%E6%8E%A7/"},{"categories":["Istio"],"content":"Istio Workload Dashboard 提供了每个工作负载请求流量的动态数据，以及入站和出站的相关指标。 ","date":"2020-05-11","objectID":"/%E6%B5%81%E9%87%8F%E7%9B%91%E6%8E%A7/:0:4","tags":["Istio","实验"],"title":"流量监控","uri":"/%E6%B5%81%E9%87%8F%E7%9B%91%E6%8E%A7/"},{"categories":["Istio"],"content":"Istio Performance Dashboard 用来监控 istio-proxy、istio-telemetry、istio-policy 和 istio-ingressgateway 的 vCPU、内存和每秒传输字节数等关键指标， 用来测量和评估 Istio 的整体性能表现。 ","date":"2020-05-11","objectID":"/%E6%B5%81%E9%87%8F%E7%9B%91%E6%8E%A7/:0:5","tags":["Istio","实验"],"title":"流量监控","uri":"/%E6%B5%81%E9%87%8F%E7%9B%91%E6%8E%A7/"},{"categories":["Istio"],"content":"自定义 Dashboard Istio 默认集成的仪表盘缺少对 Kubernetes Pod 的资源使用情况的展示，用户需要创建自定义的仪表盘来监控 Pod 的相关指标。导入 chapter-files/telemetry/Istio-USE-dashboard.json。 可以看到，这个 Dashboard 显示了指定命名空间下所有 Pod 的 CPU、内存和网络 I/O 的使用情况。也可以在 Grafana 官网查找其他开发 人员或组织公开发布的配置文件进行使用。如 “https://grafana.com/grafana/dashboards/1471\" 展示了指定命名空间下容器的关键 Metrics: request rate、error rate、response times、pod count、cpu 和 memory usage。 5. 服务网格监控 Istio 默认不安装 Kiali 组件，需启用 Kiali 并配置对外访问方式。 Kiali：http://10.12.1.100:32143/，输入用户名 admin 和密码 admin。 Kiali 总览视图展示了集群中所有命名空间的全局视图，以及各个命名空间下应用的数量、健康状态和其他功能视图的链接图标。 “Graph” 菜单项可以查看服务拓扑关系，深入了解服务间如何通讯。可从中获取实时的动态流量数据，包括 HTTP/TCP 的每秒请求数、流量比例、 成功率及不同返回码的占比。 Kiali 中，应用指具有相同标签的服务和工作负载的集合，是一个虚拟概念。如下为应用详情页，可以看到与应用关联的服务和工作负载、健康状况、 入站和出站流量的请求和响应指标等信息。 工作负载详情页包含了负载的标签、创建时间、健康状态、关联的 Pod 信息、Service 信息、Istio 资源对象 和 Metrics 等。 服务详情页展示了服务的标签、端口信息、工作负载、健康状态、Istio 资源对象 和 Metrics等。 用户通过上面的信息可以检查 Pod 和 服务是否满足 Istio 规范，如 Service 是否定义了包含协议的端口名称、 Deployment 是否带有正确的 app 和 version 标签等。 Istio Config 显示了网格中所有的 Istio 资源和规则，用户可以对单个配置进行查看、修改、删除操作。同时，Kiali 会对网格内的 Istio 规则进行在线的语法校验。如果出现网格范围内的配置冲突，Kiali 就会按照严重程度（Warning 或 Error）高亮显示这些冲突 提示用户。如 VituralService 绑定的 Gateway 不存在；Subset 没有定义；同一个主机存在定义了不同 Subset 的多个 DestinationRule 等。 ","date":"2020-05-11","objectID":"/%E6%B5%81%E9%87%8F%E7%9B%91%E6%8E%A7/:0:6","tags":["Istio","实验"],"title":"流量监控","uri":"/%E6%B5%81%E9%87%8F%E7%9B%91%E6%8E%A7/"},{"categories":["Istio"],"content":"1. 安装 Helm3 2. 安装 Istio 1.4 3. 修改 Istio 参数 vim install/kubernetes/helm/istio/values.yaml 参数 值 描述 grafana.enabled true 安装 Grafana 插件 tracing.enabled true 安装 Jaeger 插件 kiali.enabled true 安装 Kiali 插件 global.proxy.disablePolicyChecks false 启用策略检查功能 global.proxy.accessLogFile “/dev/stdout” 获取 Envoy 的访问日志 4. 修改 Istio Gateway 类型为 NodePort vim install/kubernetes/helm/istio/charts/gateways/values.yaml 5. 安装 Istio CRD 资源 helm install istio-init -n istio-system install/kubernetes/helm/istio-init/ 6. 安装 Istio 组件 helm install istio -n istio-system install/kubernetes/helm/istio/ 7. 验证 helm list --all-namespaces kubectl get po -n istio-system ","date":"2020-05-11","objectID":"/helm3%E9%83%A8%E7%BD%B2istio-1.4/:0:0","tags":["Istio","实验"],"title":"Helm3部署Istio 1.4","uri":"/helm3%E9%83%A8%E7%BD%B2istio-1.4/"},{"categories":["Istio"],"content":"1. 机器配置 OS 主机名 配置 ip Centos7(Docker 19.03.5) m-100 4C8G 10.12.1.100 Centos7(Docker 19.03.5) w-101 4C8G 10.12.1.101 Centos7(Docker 19.03.5) w-102 4C8G 10.12.1.102 2. 主节点和工作节点配置 ","date":"2020-05-11","objectID":"/kubeadm%E6%90%AD%E5%BB%BAkubernetes%E9%9B%86%E7%BE%A4/:0:0","tags":["Istio","实验"],"title":"Kubeadm搭建Kubernetes集群","uri":"/kubeadm%E6%90%AD%E5%BB%BAkubernetes%E9%9B%86%E7%BE%A4/"},{"categories":["Istio"],"content":"安装 kubeadm, kubelet 和 kubectl，其中 kubectl 工作节点可选择性安装 cat \u003c\u003cEOF \u003e /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64 enabled=1 gpgcheck=1 repo_gpgcheck=1 gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg EOF # Set SELinux in permissive mode (effectively disabling it) setenforce 0 sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config yum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes systemctl enable kubelet ","date":"2020-05-11","objectID":"/kubeadm%E6%90%AD%E5%BB%BAkubernetes%E9%9B%86%E7%BE%A4/:0:1","tags":["Istio","实验"],"title":"Kubeadm搭建Kubernetes集群","uri":"/kubeadm%E6%90%AD%E5%BB%BAkubernetes%E9%9B%86%E7%BE%A4/"},{"categories":["Istio"],"content":"设置 iptables cat \u003c\u003cEOF \u003e /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 EOF ","date":"2020-05-11","objectID":"/kubeadm%E6%90%AD%E5%BB%BAkubernetes%E9%9B%86%E7%BE%A4/:0:2","tags":["Istio","实验"],"title":"Kubeadm搭建Kubernetes集群","uri":"/kubeadm%E6%90%AD%E5%BB%BAkubernetes%E9%9B%86%E7%BE%A4/"},{"categories":["Istio"],"content":"关闭防火墙 systemctl stop firewalld systemctl disable firewalld ","date":"2020-05-11","objectID":"/kubeadm%E6%90%AD%E5%BB%BAkubernetes%E9%9B%86%E7%BE%A4/:0:3","tags":["Istio","实验"],"title":"Kubeadm搭建Kubernetes集群","uri":"/kubeadm%E6%90%AD%E5%BB%BAkubernetes%E9%9B%86%E7%BE%A4/"},{"categories":["Istio"],"content":"设置 cgroup 为 systemd cat \u003e /etc/docker/daemon.json \u003c\u003cEOF { \"exec-opts\": [\"native.cgroupdriver=systemd\"] } EOF systemctl daemon-reload systemctl enable docker systemctl restart docker ","date":"2020-05-11","objectID":"/kubeadm%E6%90%AD%E5%BB%BAkubernetes%E9%9B%86%E7%BE%A4/:0:4","tags":["Istio","实验"],"title":"Kubeadm搭建Kubernetes集群","uri":"/kubeadm%E6%90%AD%E5%BB%BAkubernetes%E9%9B%86%E7%BE%A4/"},{"categories":["Istio"],"content":"关闭 swapoff swapoff -a sed -i '/ swap / s/^\\(.*\\)$/#\\1/g' /etc/fstab ","date":"2020-05-11","objectID":"/kubeadm%E6%90%AD%E5%BB%BAkubernetes%E9%9B%86%E7%BE%A4/:0:5","tags":["Istio","实验"],"title":"Kubeadm搭建Kubernetes集群","uri":"/kubeadm%E6%90%AD%E5%BB%BAkubernetes%E9%9B%86%E7%BE%A4/"},{"categories":["Istio"],"content":"设置宿主机 dns vim /etc/hosts ","date":"2020-05-11","objectID":"/kubeadm%E6%90%AD%E5%BB%BAkubernetes%E9%9B%86%E7%BE%A4/:0:6","tags":["Istio","实验"],"title":"Kubeadm搭建Kubernetes集群","uri":"/kubeadm%E6%90%AD%E5%BB%BAkubernetes%E9%9B%86%E7%BE%A4/"},{"categories":["Istio"],"content":"集群初始化 [root@m-100 ~]# kubeadm init --pod-network-cidr=192.168.0.0/16 W0217 10:01:19.304156 10476 version.go:101] could not fetch a Kubernetes version from the internet: unable to get URL \"https://dl.k8s.io/release/stable-1.txt\": Get https://dl.k8s.io/release/stable-1.txt: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers) W0217 10:01:19.304321 10476 version.go:102] falling back to the local client version: v1.17.3 W0217 10:01:19.304755 10476 validation.go:28] Cannot validate kube-proxy config - no validator is available W0217 10:01:19.304772 10476 validation.go:28] Cannot validate kubelet config - no validator is available [init] Using Kubernetes version: v1.17.3 [preflight] Running pre-flight checks [WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service' [preflight] Pulling images required for setting up a Kubernetes cluster [preflight] This might take a minute or two, depending on the speed of your internet connection [preflight] You can also perform this action in beforehand using 'kubeadm config images pull' [kubelet-start] Writing kubelet environment file with flags to file \"/var/lib/kubelet/kubeadm-flags.env\" [kubelet-start] Writing kubelet configuration to file \"/var/lib/kubelet/config.yaml\" [kubelet-start] Starting the kubelet [certs] Using certificateDir folder \"/etc/kubernetes/pki\" [certs] Generating \"ca\" certificate and key [certs] Generating \"apiserver\" certificate and key [certs] apiserver serving cert is signed for DNS names [m-100 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 10.12.1.100] [certs] Generating \"apiserver-kubelet-client\" certificate and key [certs] Generating \"front-proxy-ca\" certificate and key [certs] Generating \"front-proxy-client\" certificate and key [certs] Generating \"etcd/ca\" certificate and key [certs] Generating \"etcd/server\" certificate and key [certs] etcd/server serving cert is signed for DNS names [m-100 localhost] and IPs [10.12.1.100 127.0.0.1 ::1] [certs] Generating \"etcd/peer\" certificate and key [certs] etcd/peer serving cert is signed for DNS names [m-100 localhost] and IPs [10.12.1.100 127.0.0.1 ::1] [certs] Generating \"etcd/healthcheck-client\" certificate and key [certs] Generating \"apiserver-etcd-client\" certificate and key [certs] Generating \"sa\" key and public key [kubeconfig] Using kubeconfig folder \"/etc/kubernetes\" [kubeconfig] Writing \"admin.conf\" kubeconfig file [kubeconfig] Writing \"kubelet.conf\" kubeconfig file [kubeconfig] Writing \"controller-manager.conf\" kubeconfig file [kubeconfig] Writing \"scheduler.conf\" kubeconfig file [control-plane] Using manifest folder \"/etc/kubernetes/manifests\" [control-plane] Creating static Pod manifest for \"kube-apiserver\" [control-plane] Creating static Pod manifest for \"kube-controller-manager\" W0217 10:01:24.604930 10476 manifests.go:214] the default kube-apiserver authorization-mode is \"Node,RBAC\"; using \"Node,RBAC\" [control-plane] Creating static Pod manifest for \"kube-scheduler\" W0217 10:01:24.606403 10476 manifests.go:214] the default kube-apiserver authorization-mode is \"Node,RBAC\"; using \"Node,RBAC\" [etcd] Creating static Pod manifest for local etcd in \"/etc/kubernetes/manifests\" [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory \"/etc/kubernetes/manifests\". This can take up to 4m0s [apiclient] All control plane components are healthy after 35.003335 seconds [upload-config] Storing the configuration used in ConfigMap \"kubeadm-config\" in the \"kube-system\" Namespace [kubelet] Creating a ConfigMap \"kubelet-config-1.17\" in namespace kube-system with the configuration for the kubelets in the cluster [upload-certs] Skipping phase. Please see --upload-certs [mark-control-plane] Marking the node m-100 as control-plane by adding the label \"node-role.kubernetes.io/master=''\" [mark-control-plane] Marking the node m-100 as ","date":"2020-05-11","objectID":"/kubeadm%E6%90%AD%E5%BB%BAkubernetes%E9%9B%86%E7%BE%A4/:0:7","tags":["Istio","实验"],"title":"Kubeadm搭建Kubernetes集群","uri":"/kubeadm%E6%90%AD%E5%BB%BAkubernetes%E9%9B%86%E7%BE%A4/"},{"categories":["Istio"],"content":"配置 kubectl mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config ","date":"2020-05-11","objectID":"/kubeadm%E6%90%AD%E5%BB%BAkubernetes%E9%9B%86%E7%BE%A4/:0:8","tags":["Istio","实验"],"title":"Kubeadm搭建Kubernetes集群","uri":"/kubeadm%E6%90%AD%E5%BB%BAkubernetes%E9%9B%86%E7%BE%A4/"},{"categories":["Istio"],"content":"验证 kubernetes 集群 kubectl get po -A 发现 coredns pending，因为还未安装网络插件。 启用 calico 网络插件 kubectl apply -f https://docs.projectcalico.org/v3.8/manifests/calico.yaml 此时，只有一个主节点，calico 网络运行不起来，执行如下命令 kubectl taint nodes --all node-role.kubernetes.io/master- 3. 工作节点配置 ","date":"2020-05-11","objectID":"/kubeadm%E6%90%AD%E5%BB%BAkubernetes%E9%9B%86%E7%BE%A4/:0:9","tags":["Istio","实验"],"title":"Kubeadm搭建Kubernetes集群","uri":"/kubeadm%E6%90%AD%E5%BB%BAkubernetes%E9%9B%86%E7%BE%A4/"},{"categories":["Istio"],"content":"添加工作节点 kubeadm join 10.12.1.100:6443 --token 0aizeu.ji5y0dooy8g658n1 \\ --discovery-token-ca-cert-hash sha256:1bc30ca4b0c09582bf0537ca2f516ae2c510becd5bdefe4ec866f9201f3519a5 ","date":"2020-05-11","objectID":"/kubeadm%E6%90%AD%E5%BB%BAkubernetes%E9%9B%86%E7%BE%A4/:0:10","tags":["Istio","实验"],"title":"Kubeadm搭建Kubernetes集群","uri":"/kubeadm%E6%90%AD%E5%BB%BAkubernetes%E9%9B%86%E7%BE%A4/"},{"categories":["Istio"],"content":"验证工作节点 kubectl get nodes kubectl get po -A -o wide ","date":"2020-05-11","objectID":"/kubeadm%E6%90%AD%E5%BB%BAkubernetes%E9%9B%86%E7%BE%A4/:0:11","tags":["Istio","实验"],"title":"Kubeadm搭建Kubernetes集群","uri":"/kubeadm%E6%90%AD%E5%BB%BAkubernetes%E9%9B%86%E7%BE%A4/"},{"categories":["Istio"],"content":"添加第二个工作节点 kubeadm join 10.12.1.100:6443 --token 0aizeu.ji5y0dooy8g658n1 \\ --discovery-token-ca-cert-hash sha256:1bc30ca4b0c09582bf0537ca2f516ae2c510becd5bdefe4ec866f9201f3519a5 ","date":"2020-05-11","objectID":"/kubeadm%E6%90%AD%E5%BB%BAkubernetes%E9%9B%86%E7%BE%A4/:0:12","tags":["Istio","实验"],"title":"Kubeadm搭建Kubernetes集群","uri":"/kubeadm%E6%90%AD%E5%BB%BAkubernetes%E9%9B%86%E7%BE%A4/"},{"categories":["Istio"],"content":"验证工作节点 kubectl get nodes kubectl get po -A -o wide ","date":"2020-05-11","objectID":"/kubeadm%E6%90%AD%E5%BB%BAkubernetes%E9%9B%86%E7%BE%A4/:0:13","tags":["Istio","实验"],"title":"Kubeadm搭建Kubernetes集群","uri":"/kubeadm%E6%90%AD%E5%BB%BAkubernetes%E9%9B%86%E7%BE%A4/"},{"categories":["Istio"],"content":"开始之前 首先要安装 Kiali 插件，然后使用 Web 界面来查看网格内的服务图以及 Istio 配置对象； 下面的介绍假设已经安装了 Helm，并使用 Helm 来安装 Kiali。 1.在 Istio 命名空间中创建一个 Secret，作为 Kiali 的认证凭据。修改并运行下列命令： $ cat \u003c\u003cEOF | kubectl apply -f - apiVersion: v1 kind: Secret metadata: name: kiali namespace: istio-system labels: app: kiali type: Opaque data: username: YWRtaW4= passphrase: YWRtaW4= EOF 2.开启Kiali Ingress 配置 $ vim install/kubernetes/helm/istio/charts/kiali/values.yaml 3.创建了 Kiali Secret 之后，根据 Helm 安装简介 使用 Helm 来安装 Kiali。在运行 helm 命令的时候必须使用 –set kiali.enabled=true 选项，例如： $ helm template install/kubernetes/helm/istio --name dc-istio --namespace istio-system \u003e install/kubernetes/helm/istio/istio.yaml $ kubectl apply -f install/kubernetes/helm/istio/istio.yaml 4.本文并未涉及 Jaeger 和 Grafana。如果已经在集群中部署了这两个组件，并且希望能够集成到 Kiali 之中，就必须在 helm 命令中增加参数： $ helm template \\ --set kiali.enabled=true \\ --set \"kiali.dashboard.jaegerURL=http://$(kubectl get svc tracing --namespace istio-system -o jsonpath='{.spec.clusterIP}'):80\" \\ --set \"kiali.dashboard.grafanaURL=http://$(kubectl get svc grafana --namespace istio-system -o jsonpath='{.spec.clusterIP}'):3000\" \\ install/kubernetes/helm/istio \\ --name istio --namespace istio-system \u003e install/kubernetes/helm/istio/istio.yaml $ kubectl apply -f install/kubernetes/helm/istio/istio.yaml ","date":"2020-05-11","objectID":"/%E7%BD%91%E6%A0%BC%E5%8F%AF%E8%A7%86%E5%8C%96/:0:1","tags":["Istio","实验"],"title":"网格可视化","uri":"/%E7%BD%91%E6%A0%BC%E5%8F%AF%E8%A7%86%E5%8C%96/"},{"categories":["Istio"],"content":"生成服务图 要验证服务是否在集群中正确运行，需要执行如下命令： $ kubectl -n istio-system get svc kiali 向网格发送流量，有三种方法： 用浏览器访问 http://$GATEWAY_URL/productpage 重复执行下面的命令： $ curl http://$GATEWAY_URL/productpage 如果系统中安装了watch命令，就可以用它来持续发送请求： $ watch -n 1 curl -o /dev/null -s -w %{http_code} $GATEWAY_URL/productpage 用浏览器打开 http://istio-kiali.daocloud.io/ 可以使用前面建立 Secret 时使用的用户名和密码在 Kiali 登录页上进行登录。如果使用的是上面的示例 Secret，那么用户名就是 admin，密码也是 admin。 登录后会显示 Overview 页面，这里可以浏览服务网格的概况。 Overview 页面中会显示网格里所有命名空间中的服务。例如下面的截图，概览示例: ​ 要查看指定命名空间的服务图，可以点击 Bookinfo 命名空间卡片，会显示类似的页面，服务图样例: 要查看指标的合计，可以在服务图上选择任何节点或者边缘，就会在右边的 Panel 上显示所选指标的详情。 如果希望用不同的图形方式来查看服务网格，可以从 Graph Type 下拉菜单进行选择。有多种不同的图形类别可供挑选：App、Versioned App、Workload 以及 Service。 App 类型会将同一应用的所有版本的数据聚合为单一的图形节点，下面的例子展示了一个 reviews 节点，其中包含三个版本的 Reviews 应用，应用图样例: ​ Versioned App 类型会把一个 App 的每个版本都用一个节点来展示，但是一个应用的所有版本会被汇总在一起，下面的示例中显示了一个在分组框中的 reviews 服务，其中包含了三个节点，每个节点都代表 reviews 应用的一个版本，分版本应用图样例: ​ Workload 类型的图会将网格中的每个工作负载都呈现为一个节点。 这种类型的图不需要读取工作负载的 app 和 version 标签。所以如果你的工作负载中没有这些标签，这种类型就是个合理选择了，工作负载图样例: Service 图类型为网格中的每个服务生成一个节点，但是会排除所有的应用和工作负载，服务图样例: 要验证 Istio 配置的详情，可以点击左边菜单栏上的 Applications、Workloads 或者 Services。下面的截图展示了 Bookinfo 应用的信息，详情样例： ​ ","date":"2020-05-11","objectID":"/%E7%BD%91%E6%A0%BC%E5%8F%AF%E8%A7%86%E5%8C%96/:0:2","tags":["Istio","实验"],"title":"网格可视化","uri":"/%E7%BD%91%E6%A0%BC%E5%8F%AF%E8%A7%86%E5%8C%96/"},{"categories":["Istio"],"content":"关于 Kiali 的 API Kiali API 提供了为服务图以及其它指标、健康状况以及配置信息生成 JSON 文件的能力。例如可以用浏览器打开 $KIALI_URL/api/namespaces/graph?namespaces=default\u0026graphType=app，会看到使用 JSON 格式表达的 app 类型的服务图。 Kiali API 来自于 Prometheus 查询，并依赖于标准的 Istio 指标配置。它还需要调用 Kubernetes API 来获取关于服务方面的附加信息。为了获得 Kiali 的最佳体验，工作负载应该像 Bookinfo 一样使用 app 和 version 标签。 ","date":"2020-05-11","objectID":"/%E7%BD%91%E6%A0%BC%E5%8F%AF%E8%A7%86%E5%8C%96/:0:3","tags":["Istio","实验"],"title":"网格可视化","uri":"/%E7%BD%91%E6%A0%BC%E5%8F%AF%E8%A7%86%E5%8C%96/"},{"categories":["Istio"],"content":"清理 $ kubectl delete all,secrets,sa,configmaps,deployments,ingresses,clusterroles,clusterrolebindings,virtualservices,destinationrules --selector=app=kiali -n istio-system ","date":"2020-05-11","objectID":"/%E7%BD%91%E6%A0%BC%E5%8F%AF%E8%A7%86%E5%8C%96/:0:4","tags":["Istio","实验"],"title":"网格可视化","uri":"/%E7%BD%91%E6%A0%BC%E5%8F%AF%E8%A7%86%E5%8C%96/"},{"categories":["Istio"],"content":"收集指标 配置 Istio，对网格内服务的遥测数据进行自动收集 新建一个 YAML 文件，用来配置新的指标以及数据流，Istio 将会进行自动生成和收集的工作。 $ kubectl apply -f samples/bookinfo/telemetry/metrics.yaml 向示例应用发送流量。 在浏览器中打开 Bookinfo 应用的 product 页面：http://$GATEWAY_URL/productpage 复查新指标的生成和收集情况。 开启Prometheus Ingress enabled配置，修改contextPath和hosts配置为如下截图 $ vim ~/istio/helm/istio/charts/prometheus/values.yaml 修改本地hosts文件，添加如下配置 ​ 172.16.99.181 istio-prometheus.daocloud.io ​ 打开 http://istio-prometheus.daocloud.io 并查询 istio_double_request_count 的值。Console 标签页会以表格形式进行数据展示，类似： istio_double_request_count{destination=\"details-v1\",instance=\"172.17.0.12:42422\",job=\"istio-mesh\",message=\"twice the fun!\",reporter=\"client\",source=\"productpage-v1\"} 8 istio_double_request_count{destination=\"details-v1\",instance=\"172.17.0.12:42422\",job=\"istio-mesh\",message=\"twice the fun!\",reporter=\"server\",source=\"productpage-v1\"} 8 istio_double_request_count{destination=\"istio-policy\",instance=\"172.17.0.12:42422\",job=\"istio-mesh\",message=\"twice the fun!\",reporter=\"server\",source=\"details-v1\"} 4 istio_double_request_count{destination=\"istio-policy\",instance=\"172.17.0.12:42422\",job=\"istio-mesh\",message=\"twice the fun!\",reporter=\"server\",source=\"istio-ingressgateway\"} 4 理解遥测配置 这个任务中使用 Istio 配置，让 Mixer 自动为所有的网格内流量生成和报告新的指标以及新的日志流。 配置中使用了三种 Mixer 功能： 从 Istio 属性中生成 instance（这里是指标值以及日志条目） 创建 handler（配置 Mixer 适配器），用来处理生成的 instance 根据一系列的 rule，把 instance 传递给 handler。 理解指标配置 在此任务中，您添加了 Istio 配置，指示 Mixer 自动生成并报告网格中所有流量的新度量标准。 添加的配置控制了三个 Mixer 功能： 从 Istio 属性生成 instances（在此示例中，度量值） 创建 handlers（配置的 Mixer 适配器），能够处理生成的 instances 根据一组 rule 将 instances 发送给 handlers 指标的配置让 Mixer 把指标数值发送给 Prometheus。其中包含三块内容：instance 配置、handler 配置以及 rule 配置。 1.kind: instance 为指标值（或者 instance）定义了结构，命名为 doublerequestcount。Instance 配置告诉 Mixer 如何为所有请求生成指标。指标来自于 Envoy 汇报的属性（然后由 Mixer 生成）。doublerequestcount 配置让 Mixer 给每个 instance 赋值为 2。因为 Istio 为每个请求都会生成 instance，这就意味着这个指标的记录的值等于收到请求数量的两倍。每个 doublerequestcount 都有一系列的 dimension。dimension 提供了一种为不同查询和需求对指标数据进行分割、聚合以及分析的方式。例如在对应用进行排错的过程中，可能只需要目标为某个服务的请求进行报告。这种配置让 Mixer 根据属性值和常量为 dimension 生成数值。例如 source 这个 dimension，他首先尝试从 source.service 属性中取值，如果取值失败，则会使用缺省值 “unknown”。而 message 这个 dimension，所有的 instance 都会得到一个常量值：“twice the fun!\"。 2.kind: handler 这一段定义了一个叫做 doublehandler 的 handler。spec 中配置了 Prometheus 适配器收到指标之后，如何将指标 instance 转换为 Prometheus 能够处理的指标数据格式的方式。配置中生成了一个新的 Prometheus 指标，取名为 double_request_count。Prometheus 适配器会给指标名称加上 istio_ 前缀，因此这个指标在 Prometheus 中会显示为 istio_double_request_count。指标带有三个标签，和 doublerequestcount.metric 的 dimension 配置相匹配。Mixer 中的 instance 通过 instance_name 来匹配 Prometheus 指标。instance_name 必须是一个全限定名称（例如：doublerequestcount.metric.istio-system） 3.kind: rule 部分定义了一个新的叫做 doubleprom 的 rule 对象。这个对象要求 Mixer 把所有的 doublerequestcount.metric 发送给 doublehandler.prometheus。因为 rule 中没有包含 match 字段，并且身处缺省配置的命名空间内（istio-system），所以这个 rule 对象对所有的网格内通信都会生效。 清理 移除遥测配置： $ kubectl delete -f samples/bookinfo/telemetry/metrics.yaml ","date":"2020-05-11","objectID":"/%E9%81%A5%E6%B5%8B/:0:1","tags":["Istio","实验"],"title":"遥测","uri":"/%E9%81%A5%E6%B5%8B/"},{"categories":["Istio"],"content":"查询指标 查询 Istio 度量标准 验证 prometheus 服务是否在您的集群中运行 在 Kubernetes 环境中，执行以下命令： $ kubectl -n istio-system get svc prometheus 将流量发送到服务网格。 对于 Bookinfo 示例，请在 Web 浏览器中访问 http://$GATEWAY_URL/productpage 打开 Prometheus UI，执行 Prometheus 查询，在网页顶部的 “Expression” 输入框中，输入文本： istio_request_count , 然后，单击 Execute 按钮。 结果将类似于： ","date":"2020-05-11","objectID":"/%E9%81%A5%E6%B5%8B/:0:2","tags":["Istio","实验"],"title":"遥测","uri":"/%E9%81%A5%E6%B5%8B/"},{"categories":["Istio"],"content":"使用 Grafana 可视化指标度量 查看 Istio 仪表盘 确认您的集群中 prometheus 服务正在运行。 在 Kubernetes 环境中，执行以下命令： $ kubectl -n istio-system get svc prometheus 验证 Grafana 服务是否在集群中运行。 在 Kubernetes 环境中，执行以下命令： $ kubectl -n istio-system get svc grafana 开启grafana配置，开启Prometheus Ingress enabled配置，修改contextPath和hosts配置为如下截图 $ vim install/kubernetes/helm/istio/values.yaml $ vim install/kubernetes/helm/istio/charts/grafana/values.yaml 在 Web 浏览器中访问 http://istio-grafana.daocloud.io/dashboard/db/istio-mesh-dashboard。 Istio 仪表盘看起来类似于： 向网格发送流量。 对于 Bookinfo 示例，请在 Web 浏览器中访问 http://$GATEWAY_URL/productpage 或发出以下命令： $ curl http://$GATEWAY_URL/productpage 刷新几次页面（或发送几次命令）以产生少量流量。 再次查看 Istio 仪表盘，它应该显示出生成的流量，看起来类似于： 这提供了网格的全局视图以及网格中的服务和工作负载。 您可以通过导航到特定仪表盘获得有关服务和工作负载的更多详细信息，如下所述。 可视化服务仪表盘。 从 Grafana 仪表盘的左上角导航菜单，您可以导航到 Istio 服务仪表盘或使用浏览器直接访问 http://istio-grafana.daocloud.io/dashboard/db/istio-service-dashboard。 Istio 服务仪表盘看起来类似于： 这提供了有关服务的指标的详细信息，进一步地提供了有关该服务的客户端工作负载（调用此服务的工作负载）和服务工作负载（提供此服务的工作负载）的详细信息。 可视化工作负载仪表盘。 从 Grafana 仪表盘的左上角导航菜单，您可以导航到 Istio 工作负载仪表盘或使用浏览器直接访问 http://istio-grafana.daocloud.io/dashboard/db/istio-workload-dashboard。 Istio 工作负载仪表盘看起来类似于： 这会提供有关每个工作负载的指标的详细信息，进一步地提供有关该工作负载的入站工作负载（向此工作负载发送请求的工作负载）和出站服务（此工作负载发送请求的服务）的指标。 关于 Grafana 插件 Grafana 附加组件是 Grafana 的预配置实例。 基础镜像（grafana/grafana:5.2.3）已修改为同时启动 Prometheus 数据源和 Istio 仪表盘。 Istio 的基本安装文件，特别是 Mixer，附带了全局（用于每个服务）指标的默认配置。 Istio 仪表盘被构建为与默认 Istio 度量配置和 Prometheus 后端一起使用。 Istio 仪表盘由三个主要部分组成： 网格摘要视图。此部分提供网格的全局摘要视图，并在网格中显示 HTTP/gRPC 和 TCP 工作负载。 单个服务视图。此部分提供有关网格中每个服务（HTTP/gRPC 和 TCP）的请求和响应的度量标准，同时还提供了有关此服务的客户端和服务工作负载的指标。 单个工作负载视图。此部分提供有关网格内每个工作负载（HTTP/gRPC 和 TCP）的请求和响应的指标，同时还提供了有关此工作负载的入站工作负载和出站服务的指标。 有关如何创建、配置和编辑仪表盘的更多信息，请参阅 Grafana 文档。 ","date":"2020-05-11","objectID":"/%E9%81%A5%E6%B5%8B/:0:3","tags":["Istio","实验"],"title":"遥测","uri":"/%E9%81%A5%E6%B5%8B/"},{"categories":["Istio"],"content":"分布式追踪 在完成这个任务后，您将了解如何将应用加入追踪，而不用关心其语言、框架或者您构建应用的平台。 这个任务使用 Bookinfo 作为示例应用程序。 了解发生了什么 虽然 Istio 代理能够自动发送 span，但仍然需要一些线索来将整个追踪衔接起来。应用程序需要分发合适的 HTTP header，以便当代理发送 span 信息时，span 可以被正确的关联到一个追踪中。 例如，如果您查看 Python 示例 productpage 服务，您将看到应用程序从 HTTP 请求中提取所需的头通过 OpenTracing 库： x-request-id x-b3-traceid x-b3-spanid x-b3-parentspanid x-b3-sampled x-b3-flags x-ot-span-context 如果您查看示例服务，可以看到 productpage service（Python）从 HTTP 请求中提取所需的 header： def getForwardHeaders(request): headers = {} # x-b3-*** headers can be populated using the opentracing span span = get_current_span() carrier = {} tracer.inject( span_context=span.context, format=Format.HTTP_HEADERS, carrier=carrier) headers.update(carrier) # ... incoming_headers = ['x-request-id'] # ... for ihdr in incoming_headers: val = request.headers.get(ihdr) if val is not None: headers[ihdr] = val return headers reviews 应用程序（Java）做了类似的事情： @GET @Path(\"/reviews/{productId}\") public Response bookReviewsById(@PathParam(\"productId\") int productId, @HeaderParam(\"end-user\") String user, @HeaderParam(\"x-request-id\") String xreq, @HeaderParam(\"x-b3-traceid\") String xtraceid, @HeaderParam(\"x-b3-spanid\") String xspanid, @HeaderParam(\"x-b3-parentspanid\") String xparentspanid, @HeaderParam(\"x-b3-sampled\") String xsampled, @HeaderParam(\"x-b3-flags\") String xflags, @HeaderParam(\"x-ot-span-context\") String xotspan) { if (ratings_enabled) { JsonObject ratingsResponse = getRatings(Integer.toString(productId), user, xreq, xtraceid, xspanid, xparentspanid, xsampled, xflags, xotspan); 在应用程序中进行下游调用时，请确保包含了这些 header。 追踪采样 Istio 默认捕获所有请求的追踪信息。例如，当使用上面的 Bookinfo 示例应用程序时，每次访问 /productpage 时，都会看到相应的追踪仪表板。此采样率适用于测试或低流量网格。对于高流量网格，您可以以两种方式之一来降低追踪采样百分比： 在安装网格时，使用 pilot.traceSampling Helm 选项来设置追踪采样百分比。 在一个运行中的网格中，编辑 istio-pilot deployment，通过下列步骤改变环境变量： 运行以下命令打来文本编辑器并加载 deployment 配置文件： $ kubectl -n istio-system edit deploy istio-pilot 找到 PILOT_TRACE_SAMPLING 环境变量，并修改 value: 为期望的百分比。 在这两种情况下，有效值都是 0.0 到 100.0，精度为 0.01，默认值为1，即默认1%的采样率。 开始之前 使用 Helm chart 进行安装时，设置 –set tracing.enabled=true 选项以启用追踪。 在启用追踪功能时，您可以通过 pilot.traceSampling 选项设置 Istio 追踪所使用的采样率。默认采样率为 1%。 访问仪表盘 如要使用 Kubernetes ingress，请指定 Helm chart 选项 –set tracing.ingress.enabled=true，–set tracing.ingress.hosts=istio-tracing.daocloud.io。 使用 Bookinfo 示例产生追踪 当 Bookinfo 应用程序启动并运行时，访问 http://$GATEWAY_URL/productpage 一次或多次以生成追踪信息。 要查看追踪数据，您必须向您的服务发送请求。请求的数量取决于 Istio 的采样率。 您在安装 Istio 时设置过这个采样速率参数，默认采样率为 1%。 在第一个追踪可见之前，您需要发送至少 100 个请求。 要向 productpage 服务发送 100 个请求，请使用以下命令： $ for i in seq 1 100; do curl -s -o /dev/null http://$GATEWAY_URL/productpage; done 访问http://istio-tracing.daocloud.io/jaeger，从仪表盘左边面板的 Service 下拉列表中选择 productpage 并点击 Find Traces： 点击位于最上面的最近一次追踪，查看对应最近一次访问 /productpage 的详细信息： 追踪信息由一组 span 组成，每个 span 对应一个 Bookinfo service。这些 service 在 /productpage 请求或 Istio 内部组件（例如：istio-ingressgateway、istio-mixer、istio-policy）执行时被调用。 ","date":"2020-05-11","objectID":"/%E9%81%A5%E6%B5%8B/:0:4","tags":["Istio","实验"],"title":"遥测","uri":"/%E9%81%A5%E6%B5%8B/"},{"categories":["Istio"],"content":"请求路由 首先会把 Bookinfo 应用的进入流量导向 reviews 服务的 v1 版本。接下来会把特定用户的请求发送给 v2 版本，其他用户则不受影响。 Istio Bookinfo 示例包含四个独立的微服务，每个微服务都有多个版本。 其中一个微服务 reviews 的三个不同版本已经部署并同时运行。 在浏览器中访问 Bookinfo 应用程序的 /productpage 并刷新几次，会注意到，有时书评的输出包含星级评分，有时则不包含。 这是因为没有明确的默认服务版本路由，Istio 将以循环方式请求路由到所有可用版本。 应用 virtual service 要仅路由到一个版本，请应用为微服务设置默认版本的 virtual service。在这种情况下，virtual service 将所有流量路由到每个微服务的 v1 版本。 1.如果还没有应用 destination rule，请先应用缺省目标规则。在使用 Istio 控制 Bookinfo 版本路由之前，你需要在目标规则中定义好可用的版本，命名为 subsets 。等待几秒钟，等待目标规则生效。 如果不需要启用双向TLS，请执行以下命令： $ kubectl apply -f samples/bookinfo/networking/destination-rule-all.yaml 如果需要启用双向 TLS，请执行以下命令： $ kubectl apply -f samples/bookinfo/networking/destination-rule-all-mtls.yaml 2.运行以下命令以应用 virtual service： $ kubectl apply -f samples/bookinfo/networking/virtual-service-all-v1.yaml 3.使用以下命令显示已定义的路由： $ kubectl get virtualservices -o yaml 您还可以使用以下命令显示相应的 subset 定义： $ kubectl get destinationrules -o yaml 您已将 Istio 配置为路由到 Bookinfo 微服务的 v1 版本， reviews 服务的 v1 版本。 测试新的路由配置 您可以通过再次刷新 Bookinfo 应用程序的 /productpage 轻松测试新配置。 在浏览器中打开 Bookinfo 站点。 URL 为 http://$GATEWAY_URL/productpage，其中 GATEWAY_URL 是外部的入口 IP 地址 请注意，无论您刷新多少次，页面的评论部分都不会显示评级星标。这是因为您将 Istio 配置为将评论服务的所有流量路由到版本 reviews:v1，并且此版本的服务不访问星级评分服务。 您已成功完成此任务的第一部分：将流量路由到一个版本的服务。 ","date":"2020-05-11","objectID":"/%E6%B5%81%E9%87%8F%E7%AE%A1%E7%90%86/:0:1","tags":["Istio","实验"],"title":"流量管理","uri":"/%E6%B5%81%E9%87%8F%E7%AE%A1%E7%90%86/"},{"categories":["Istio"],"content":"基于用户身份的路由 接下来，您将更改路由配置，以便将来自特定用户的所有流量路由到特定服务版本。在这种情况下，来自名为 Jason 的用户的所有流量将被路由到服务 reviews:v2。 请注意，Istio 对用户身份没有任何特殊的内置机制。这个例子的基础在于， productpage 服务在所有针对 reviews 服务的调用请求中 都加自定义的 HTTP header，从而达到在流量中对最终用户身份识别的这一效果。 请记住，reviews:v2 是包含星级评分功能的版本。 1.运行以下命令以启用基于用户的路由： $ kubectl apply -f samples/bookinfo/networking/virtual-service-reviews-test-v2.yaml 2.确认规则已创建： $ kubectl get virtualservice reviews -o yaml 3.在 Bookinfo 应用程序的 /productpage 上，以用户 jason 身份登录。 刷新浏览器。你看到了什么？星级评分显示在每个评论旁边。 4.以其他用户身份登录（选择您想要的任何名称）。 刷新浏览器。现在星星消失了。这是因为除了 Jason 之外，所有用户的流量都被路由到 reviews:v1。 您已成功配置 Istio 以根据用户身份路由流量。 ","date":"2020-05-11","objectID":"/%E6%B5%81%E9%87%8F%E7%AE%A1%E7%90%86/:0:2","tags":["Istio","实验"],"title":"流量管理","uri":"/%E6%B5%81%E9%87%8F%E7%AE%A1%E7%90%86/"},{"categories":["Istio"],"content":"清除 删除应用程序 virtual service。 $ kubectl delete -f samples/bookinfo/networking/virtual-service-all-v1.yaml ","date":"2020-05-11","objectID":"/%E6%B5%81%E9%87%8F%E7%AE%A1%E7%90%86/:0:3","tags":["Istio","实验"],"title":"流量管理","uri":"/%E6%B5%81%E9%87%8F%E7%AE%A1%E7%90%86/"},{"categories":["Istio"],"content":"故障注入 使用 Istio 测试 Bookinfo 应用的弹性，具体方式就是在 reviews:v2 和 ratings 之间进行延迟注入。接下来以测试用户的角度观察后续行为，我们会注意到 reviews 服务的 v2 版本有一个 Bug。注意所有其他用户都不会感知到正在进行的测试过程。 通过首先执行请求路由任务或运行以下命令来初始化应用程序版本路由： $ kubectl apply -f samples/bookinfo/networking/virtual-service-all-v1.yaml $ kubectl apply -f samples/bookinfo/networking/virtual-service-reviews-test-v2.yaml 通过上面的配置，下面是请求的流程： productpage → reviews:v2 → ratings (jason 用户) productpage → reviews:v1 → ratings (其他用户) 使用 HTTP 延迟进行故障注入 为了测试微服务应用程序 Bookinfo 的弹性，我们将为用户 jason 在 reviews:v2 和 ratings 服务之间注入一个 7 秒的延迟。 这个测试将会发现故意引入 Bookinfo 应用程序中的错误。 由于 reviews:v2 服务对其 ratings 服务的调用具有 10 秒的硬编码连接超时，比我们设置的 7s 延迟要大，因此我们期望端到端流程是正常的（没有任何错误）。 创建故障注入规则以延迟来自用户 jason（我们的测试用户）的流量 $ kubectl apply -f samples/bookinfo/networking/virtual-service-ratings-test-delay.yaml 确认已创建规则： $ kubectl get virtualservice ratings -o yaml 延时配置测试 通过浏览器打开 Bookinfo 应用。 使用用户 jason 登陆到 /productpage 界面。 你期望 Bookinfo 主页在大约 7 秒钟加载完成并且没有错误。但是，出现了一个问题，Reviews 部分显示了错误消息： Error fetching product reviews! Sorry, product reviews are currently unavailable for this book. 查看页面的返回时间： 打开浏览器的 开发工具 菜单 打开 网络 标签 重新加载 productpage 页面，你会看到页面实际上用了大约 6s。 清理 删除应用程序路由规则： $ kubectl delete -f samples/bookinfo/networking/virtual-service-all-v1.yaml ","date":"2020-05-11","objectID":"/%E6%B5%81%E9%87%8F%E7%AE%A1%E7%90%86/:0:4","tags":["Istio","实验"],"title":"流量管理","uri":"/%E6%B5%81%E9%87%8F%E7%AE%A1%E7%90%86/"},{"categories":["Istio"],"content":"流量迁移 本任务将演示如何逐步将流量从一个版本的微服务迁移到另一个版本。例如，您可以将流量从旧版本迁移到新版本。 一个常见的用例是将流量从一个版本的微服务逐渐迁移到另一个版本。在 Istio 中，您可以通过配置一系列规则来实现此目标，这些规则将一定百分比的流量路由到一个或另一个服务。在此任务中，您将 50％ 的流量发送到 reviews:v1，另外 50％ 的流量发送到 reviews:v3。然后将 100％ 的流量发送到 reviews:v3 来完成迁移。 最后，会使用 Istio 将所有用户的流量从 reviews 的 v2 版本转移到 v3 版本之中，以此来规避 v2 版本中 Bug 造成的影响。 1.首先，运行此命令将所有流量路由到 v1 版本的各个微服务。 $ kubectl apply -f samples/bookinfo/networking/virtual-service-all-v1.yaml 2.在浏览器中打开 Bookinfo 站点。 URL 为 http://$GATEWAY_URL/productpage。 请注意，不管刷新多少次，页面的评论部分都不会显示评级星号。这是因为 Istio 被配置为将 reviews 服务的的所有流量都路由到了 reviews:v1 版本， 而该版本的服务不会访问带星级的 ratings 服务。 使用下面的命令把 50% 的流量从 reviews:v1 转移到 reviews:v3： $ kubectl apply -f samples/bookinfo/networking/virtual-service-reviews-50-v3.yaml 等待几秒钟以让新的规则传播到代理中生效。 3.确认规则已被替换: $ kubectl get virtualservice reviews -o yaml 4.刷新浏览器中的 /productpage 页面，大约有 50% 的几率会看到页面中出带红色星级的评价内容。这是因为 v3 版本的 reviews 访问了带星级评级的 ratings 服务，但 v1 版本却没有。 在目前的 Envoy sidecar 实现中，可能需要刷新 /productpage 很多次–可能 15 次或更多–才能看到流量分发的效果。您可以通过修改规则将 90% 的流量路由到 v3，这样能看到更多带红色星级的评价。 如果您认为 reviews:v3 微服务已经稳定，你可以通过应用此 virtual service 将 100% 的流量路由到 reviews:v3： $ kubectl apply -f samples/bookinfo/networking/virtual-service-reviews-v3.yaml 现在，当您刷新 /productpage 时，您将始终看到带有红色星级评分的书评。 清理 删除应用程序路由规则。 $ kubectl delete -f samples/bookinfo/networking/virtual-service-all-v1.yaml ","date":"2020-05-11","objectID":"/%E6%B5%81%E9%87%8F%E7%AE%A1%E7%90%86/:0:5","tags":["Istio","实验"],"title":"流量管理","uri":"/%E6%B5%81%E9%87%8F%E7%AE%A1%E7%90%86/"},{"categories":["Istio"],"content":"部署一个样例应用，它由四个单独的微服务构成，用来演示多种 Istio 特性。这个应用模仿在线书店的一个分类，显示一本书的信息。页面上会显示一本书的描述，书籍的细节，以及关于这本书的一些评论。 Bookinfo 应用分为四个单独的微服务： productpage：productpage 微服务会调用 details 和 reviews 两个微服务，用来生成页面。 details：这个微服务包含了书籍的信息。 reviews：这个微服务包含了书籍相关的评论。它还会调用 ratings 微服务。 ratings：ratings 微服务中包含了由书籍评价组成的评级信息。 reviews 微服务有 3 个版本： v1 版本不会调用 ratings 服务。 v2 版本会调用 ratings 服务，并使用 1 到 5 个黑色星形图标来显示评分信息。 v3 版本会调用 ratings 服务，并使用 1 到 5 个红色星形图标来显示评分信息。 下图展示了这个应用的端到端架构。 Bookinfo 是一个异构应用，几个微服务是由不同的语言编写的。这些服务对 Istio 并无依赖，但是构成了一个有代表性的服务网格的例子：它由多个服务、多个语言构成，并且 reviews 服务具有多个版本。 ","date":"2020-05-11","objectID":"/bookinfo-%E5%BA%94%E7%94%A8/:0:0","tags":["Istio","实验"],"title":"Bookinfo 应用","uri":"/bookinfo-%E5%BA%94%E7%94%A8/"},{"categories":["Istio"],"content":"开始之前 如果还没开始，首先要遵循 Istio安装 的指导，根据所在平台完成 Istio 的部署工作。 ","date":"2020-05-11","objectID":"/bookinfo-%E5%BA%94%E7%94%A8/:0:1","tags":["Istio","实验"],"title":"Bookinfo 应用","uri":"/bookinfo-%E5%BA%94%E7%94%A8/"},{"categories":["Istio"],"content":"部署应用 要在 Istio 中运行这一应用，无需对应用自身做出任何改变。我们只要简单的在 Istio 环境中对服务进行配置和运行，具体一点说就是把 Envoy sidecar 注入到每个服务之中。这个过程所需的具体命令和配置方法由运行时环境决定，而部署结果较为一致，如下图所示： 所有的微服务都和 Envoy sidecar 集成在一起，被集成服务所有的出入流量都被 sidecar 所劫持，这样就为外部控制准备了所需的 Hook，然后就可以利用 Istio 控制平面为应用提供服务路由、遥测数据收集以及策略实施等功能。 ","date":"2020-05-11","objectID":"/bookinfo-%E5%BA%94%E7%94%A8/:0:2","tags":["Istio","实验"],"title":"Bookinfo 应用","uri":"/bookinfo-%E5%BA%94%E7%94%A8/"},{"categories":["Istio"],"content":"如果在 Kubernetes 中运行 1.进入 Istio 安装目录。 Istio 默认启用自动 Sidecar 注入，为 default 命名空间打上标签 istio-injection=enabled。 $ kubectl label namespace default istio-injection=enabled 2.使用 kubectl 部署简单的服务 $ kubectl apply -f samples``/bookinfo/platform/kube/bookinfo``.yaml 3.（可选）如果在安装过程中禁用了自动 Sidecar 注入并依赖 手工 Sidecar 注入, istioctl kube-inject 命令用于在在部署应用之前修改 bookinfo.yaml。 $ kubectl apply -f \u003c(istioctl kube-inject -f samples``/bookinfo/platform/kube/bookinfo``.yaml) 上面的命令会启动 bookinfo` 应用程序架构图中全部的四个服务，其中也包括了 reviews 服务的三个版本（v1、v2 以及 v3） 在实际部署中，微服务版本的启动过程需要持续一段时间，并不是同时完成的。 4.确认所有的服务和 Pod 都已经正确的定义和启动： $ kubectl get services $ kubectl get pods 5.要确认 Bookinfo 应用程序正在运行，请通过某个 pod 中的 curl 命令向其发送请求，例如来自 ratings： $ kubectl exec -it $(kubectl get pod -l app=ratings -o jsonpath='{.items[0].metadata.name}') -c ratings -- curl productpage:9080/productpage | grep -o \"\u003ctitle\u003e.*\u003c/title\u003e\" 确定 Ingress 的 IP 和端口 现在 Bookinfo 服务启动并运行中，你需要使应用程序可以从外部访问 Kubernetes 集群，例如使用浏览器。一个 Istio Gateway 应用到了目标中。 1.为应用程序定义入口网关： $ kubectl apply -f samples``/bookinfo/networking/bookinfo-gateway``.yaml 2.确认网关创建完成： $ kubectl get gateway 3.根据部署istio时候istio-ingressgateway组件的http NodePort，访问bookinfo服务，如 http://172.16.99.181:8500/productpage ","date":"2020-05-11","objectID":"/bookinfo-%E5%BA%94%E7%94%A8/:0:3","tags":["Istio","实验"],"title":"Bookinfo 应用","uri":"/bookinfo-%E5%BA%94%E7%94%A8/"},{"categories":["Istio"],"content":"确认应用在运行中 可以用 curl 命令来确认 Bookinfo 应用的运行情况： $ curl -s http://${GATEWAY_URL}/productpage | grep -o \"\u003ctitle\u003e.*\u003c/title\u003e\" 还可以用浏览器打开网址 http://$GATEWAY_URL/productpage，来浏览应用的 Web 页面。如果刷新几次应用的页面，就会看到 productpage 页面中会随机展示 reviewe 服务的不同版本的效果（红色、黑色的星形或者没有显示）。reviews 服务出现这种情况是因为我们还没有使用 Istio 来控制版本的路由。 ","date":"2020-05-11","objectID":"/bookinfo-%E5%BA%94%E7%94%A8/:0:4","tags":["Istio","实验"],"title":"Bookinfo 应用","uri":"/bookinfo-%E5%BA%94%E7%94%A8/"},{"categories":["Istio"],"content":"清理 结束对 Bookinfo 示例应用的体验之后，就可以使用下面的命令来完成应用的删除和清理了。 1.删除路由规则，并终结应用的 Pod $ samples/bookinfo/platform/kube/cleanup.sh 2.确认应用已经关停 $ kubectl get virtualservices #-- there should be no virtual services $ kubectl get destinationrules #-- there should be no destination rules $ kubectl get gateway #-- there should be no gateway $ kubectl get pods #-- the Bookinfo pods should be deleted ","date":"2020-05-11","objectID":"/bookinfo-%E5%BA%94%E7%94%A8/:0:5","tags":["Istio","实验"],"title":"Bookinfo 应用","uri":"/bookinfo-%E5%BA%94%E7%94%A8/"},{"categories":["Istio"],"content":"三种方式 1.Quick Start 参考官网：https://istio.io/docs/setup/install/kubernetes/ 这里无需安装 Helm，只使用基本的 Kubernetes 命令，就能设置一个预配置的 Istio demo。 要正式在生产环境上安装 Istio，官网推荐使用 Helm 进行安装，其中包含了大量选项，可以对 Istio 的具体配置进行选择和管理，来满足特定的使用要求。 2.Helm + Tiller 3.Helm template 使用 helm template 进行安装 Helm Tiller 或 Helm template 差别不大，这里选择Helm template 方式。 1.为 Istio 组件创建命名空间 istio-system： $ kubectl create namespace istio-system 2.使用 kubectl apply 安装所有的 Istio CRD，命令执行之后，会隔一段时间才能被 Kubernetes API Server 收到。这里为了后续研究Istio特性，把Helm及部署yaml单独拷贝出来，放在一个名为istio目录内。 $ helm template install/kubernetes/helm/istio-init --name dc-istio-init --namespace istio-system \u003e install/kubernetes/helm/istio-init/istio-init.yaml //当前目录为解压istio-1.3.4.tar.gz后的根目录 $ kubectl apply -f install/kubernetes/helm/istio-init/istio-init.yaml 3.使用如下命令以确保全部 23 个 Istio CRD 被提交到 Kubernetes api-server： $ kubectl get crds | grep 'istio.io' | wc -l 4.选择一个配置文件，接着部署与你选择的配置文件相对应的 Istio 的核心组件，官网建议在生产环境部署中使用 default 配置文件: $ helm template install/kubernetes/helm/istio --name dc-istio --namespace istio-system \u003e install/kubernetes/helm/istio/istio.yaml //当前目录为解压istio-1.3.4.tar.gz后的根目录 $ vim install/kubernetes/helm/istio/charts/gateways/value.yaml //修改istio-ingressgateway.type=NodePort $ kubectl apply -f install/kubernetes/helm/istio/istio.yaml 你可以添加一个或多个 –set = 来进一步自定义 helm 命令的安装选项，或者采用更改配置文件的方式。 注：默认情况下，Istio 使用 LoadBalancer 服务类型，而有些平台是不支持 LoadBalancer 服务的。对于缺少 LoadBalancer 支持的平台，执行下面的安装步骤时，可以在 Helm 命令中加入 –set gateways.istio-ingressgateway.type=NodePort 选项，使用 NodePort 来替代 LoadBalancer 服务类型，也可以如上述第三条命令所示，进入相应的文件修改。 注：后续所有文章当前所处目录都为解压istio-1.3.4.tar.gz后的根目录，就不再注明 验证安装 确认 Istio Kubernetes services 服务是否正常，确保部署了相应的 Kubernetes pod 并且 STATUS 是 Running $ kubectl get svc -n istio-system $ kubectl get po -n istio-system 卸载 $ kubectl delete -f install/kubernetes/helm/istio/istio.yaml $ kubectl delete ns istio-system $ kubectl delete -f install/kubernetes/helm/istio-init/files 趟过的坑: 1.对于二进制方式部署的kubernetes集群，kube-apiserver 的准入控制参数，需添加MutatingAdmissionWebhook,ValidatingAdmissionWebhook 这两个webhook。 2.kubernetes集群master节点需部署kube-proxy，kubelet这两个组件，否则sidecar注入不进去。 ","date":"2020-05-11","objectID":"/istio-%E5%AE%89%E8%A3%85/:0:0","tags":["Istio","实验"],"title":"Istio 安装","uri":"/istio-%E5%AE%89%E8%A3%85/"},{"categories":["Envoy"],"content":" Listeners Discovery Service API — LDS to publish ports on which to listen for traffic Endpoints Discovery Service API- EDS for service discovery, Routes Discovery Service API- RDS for traffic routing decisions Clusters Discovery Service- CDS for backend services to which we can route traffic Secrets Discovery Service — SDS for distributing secrets (certificates and keys) xDS REST and gRPC protocol：https://github.com/envoyproxy/data-plane-api/blob/master/xds_protocol.rst xDS 控制面开发 SDK：https://github.com/envoyproxy/go-control-plane 参考： Guidance for Building a Control Plane to Manage Envoy Proxy at the edge, as a gateway, or in a mesh https://medium.com/solo-io/guidance-for-building-a-control-plane-to-manage-envoy-proxy-at-the-edge-as-a-gateway-or-in-a-mesh-badb6c36a2af Guidance for Building a Control Plane for Envoy Part 5: Deployment Tradeoffs https://medium.com/solo-io/guidance-for-building-a-control-plane-for-envoy-part-5-deployment-tradeoffs-a6ef55c06327 ","date":"2020-05-11","objectID":"/xds-api/:0:0","tags":["Envoy","原理"],"title":"XDS API","uri":"/xds-api/"},{"categories":["Envoy"],"content":"目标 根据 HTTP 请求头蓝绿部署 调配 20% 流量到新的版本 启动3个版本上游服务 docker run -d katacoda/docker-http-server:v1 docker run -d katacoda/docker-http-server:v1 docker run -d katacoda/docker-http-server:v2 docker run -d katacoda/docker-http-server:v2 docker run -d katacoda/docker-http-server:v3 docker ps -q | xargs -n 1 docker inspect --format '{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}} {{ .Config.Hostname }} {{ .Config.Image }}' | sed 's/ \\// /' 启动 Envoy envoy.yaml static_resources:listeners:- name:listener_httpaddress:socket_address:{address: 0.0.0.0, port_value:8080}filter_chains:- filters:- name:envoy.http_connection_managerconfig:codec_type:autostat_prefix:ingress_httproute_config:virtual_hosts:- name:backenddomains:- \"*\"routes:- match:prefix:\"/service/1\"route:cluster:service1- match:prefix:\"/service/2\"headers:- name:\"x-canary-version\"exact_match:\"service2a\"route:cluster:service2a- match:prefix:\"/service/2\"route:cluster:service2- match:prefix:\"/service/3\"route:weighted_clusters:clusters:- name:service3aweight:80- name:service3bweight:20http_filters:- name:envoy.routerconfig:{}clusters:- name:service1connect_timeout:0.25stype:strict_dnslb_policy:round_robinhosts:- socket_address:address:172.17.0.2port_value:80- name:service2connect_timeout:0.25stype:strict_dnslb_policy:round_robinhosts:- socket_address:address:172.17.0.3port_value:80- name:service2aconnect_timeout:0.25stype:strict_dnslb_policy:round_robinhosts:- socket_address:address:172.17.0.4port_value:80- name:service3aconnect_timeout:0.25stype:strict_dnslb_policy:round_robinhosts:- socket_address:address:172.17.0.5port_value:80- name:service3bconnect_timeout:0.25stype:strict_dnslb_policy:round_robinhosts:- socket_address:address:172.17.0.6port_value:80admin:access_log_path:/tmp/admin_access.logaddress:socket_address:address:0.0.0.0port_value:8001 docker run -d --name proxy1 -p 80:8080 -v /root/envoy-1.12.2/examples/blue-green/:/etc/envoy envoyproxy/envoy 测试 ","date":"2020-05-06","objectID":"/%E5%AE%9E%E7%8E%B0%E8%93%9D%E7%BB%BF%E9%83%A8%E7%BD%B2/:0:0","tags":["Envoy","实验"],"title":"实现蓝绿部署","uri":"/%E5%AE%9E%E7%8E%B0%E8%93%9D%E7%BB%BF%E9%83%A8%E7%BD%B2/"},{"categories":["Envoy"],"content":"目标 暴露 Envoy 的 Metrics 到 Prometheus 使用 Prometheus 数据源在 Grafana 展示 配置 Envoy 发送 traces 到 Jaeger 启动上游服务 docker run -d katacoda/docker-http-server:healthy docker run -d katacoda/docker-http-server:healthy 启动 Envoy admin:access_log_path:/tmp/admin_access.logaddress:socket_address:{address: 0.0.0.0, port_value:9901}static_resources:listeners:- name:listener_0address:socket_address:{address: 0.0.0.0, port_value:10000}filter_chains:- filters:- name:envoy.http_connection_managerconfig:codec_type:autostat_prefix:ingress_httpgenerate_request_id:truetracing:operation_name:egressroute_config:name:local_routevirtual_hosts:- name:backenddomains:- \"*\"routes:- match:prefix:\"/\"route:cluster:targetClusterhttp_filters:- name:envoy.routerclusters:- name:targetClusterconnect_timeout:0.25stype:STRICT_DNSdns_lookup_family:V4_ONLYlb_policy:ROUND_ROBINhosts:[{socket_address:{address: 172.17.0.2, port_value:80}},{socket_address:{address: 172.17.0.3, port_value:80}}]health_checks:- timeout:1sinterval:10sinterval_jitter:1sunhealthy_threshold:6healthy_threshold:1http_health_check:path:\"/health\"- name:jaegerconnect_timeout:1stype:strict_dnslb_policy:round_robinload_assignment:cluster_name:jaegerendpoints:- lb_endpoints:- endpoint:address:socket_address:address:172.18.0.6port_value:9411tracing:http:name:envoy.zipkinconfig:collector_cluster:jaegercollector_endpoint:\"/api/v1/spans\"shared_span_context:false prometheus.yml global:scrape_interval:15sevaluation_interval:15sscrape_configs:- job_name:'envoy'metrics_path:/stats/prometheusstatic_configs:- targets:['172.18.0.2:9901']labels:group:'envoy' //todo ","date":"2020-05-06","objectID":"/%E5%AE%9E%E7%8E%B0metrics%E5%92%8Ctracing%E7%9A%84%E8%83%BD%E5%8A%9B/:0:0","tags":["Envoy","实验"],"title":"实现Metrics和Tracing的能力","uri":"/%E5%AE%9E%E7%8E%B0metrics%E5%92%8Ctracing%E7%9A%84%E8%83%BD%E5%8A%9B/"},{"categories":["Envoy"],"content":"目标 配置EDS 使用 REST API 为集群添加端点 使用 REST API 删除端点 当上游被定义在配置中，Envoy 需要知道如何解析集群成员，这时需要用到服务发现。EDS 是一个运行在gRPC or REST-JSON API 。 服务端的xDS服务，Envoy 用该服务来获取集群成员。Envoy项目在 Java 和 Go 中都提供了EDS和其他发现服务的参考gRPC实现。 在接下来的步骤中，我们将更改配置以使用Endpoint Discovery Service（EDS），从而允许基于来自REST-JSON API的数据来动态添加节点。 启动 Envoy envoy.yaml admin:access_log_path:/dev/nulladdress:socket_address:address:127.0.0.1port_value:9000node:cluster:myclusterid:test-idstatic_resources:listeners:- name:listener_0address:socket_address:{address: 0.0.0.0, port_value:10000}filter_chains:- filters:- name:envoy.http_connection_managerconfig:stat_prefix:ingress_httpcodec_type:AUTOroute_config:name:local_routevirtual_hosts:- name:local_servicedomains:[\"*\"]routes:- match:{prefix:\"/\"}route:{cluster:targetCluster}http_filters:- name:envoy.routerclusters:- name:targetClustertype:EDSconnect_timeout:0.25seds_cluster_config:service_name:myserviceeds_config:api_config_source:api_type:RESTcluster_names:[eds_cluster]refresh_delay:5s- name:eds_clustertype:STATICconnect_timeout:0.25shosts:[{socket_address:{address: 172.18.0.4, port_value:8080}}] 启动 Envoy： docker run --name=api-eds -d -p 80:10000 -p 9901:9901 -v /root/envoy-1.12.2/examples/api-bases-dr/:/etc/envoy envoyproxy/envoy 启动上游服务 docker run -p 8081:8081 -d -e EDS_SERVER_PORT='8081' katacoda/docker-http-server:v4 验证服务：curl http://localhost:8081 -i 启动 EDS 服务 docker run -p 8080:8080 -d katacoda/eds_server 测试：上游服务未注册到 EDS 服务时 注册上游服务到 EDS 服务 curl -X POST --header 'Content-Type: application/json' --header 'Accept: application/json' -d '{ \"hosts\": [ { \"ip_address\": \"172.17.0.2\", \"port\": 8081, \"tags\": { \"az\": \"us-central1-a\", \"canary\": false, \"load_balancing_weight\": 50 } } ] }' http://localhost:8080/edsservice/myservice 测试：上游服务注册到 EDS 服务后 为集群添加新的端点 for i in 8082 8083 8084 8085 do docker run -d -e EDS_SERVER_PORT=$i katacoda/docker-http-server:v4; sleep .5 done 注册新的端点到 EDS 服务 curl -X PUT --header 'Content-Type: application/json' --header 'Accept: application/json' -d '{ \"hosts\": [ { \"ip_address\": \"172.17.0.2\", \"port\": 8081, \"tags\": { \"az\": \"us-central1-a\", \"canary\": false, \"load_balancing_weight\": 50 } }, { \"ip_address\": \"172.17.0.5\", \"port\": 8082, \"tags\": { \"az\": \"us-central1-a\", \"canary\": false, \"load_balancing_weight\": 50 } }, { \"ip_address\": \"172.17.0.6\", \"port\": 8083, \"tags\": { \"az\": \"us-central1-a\", \"canary\": false, \"load_balancing_weight\": 50 } }, { \"ip_address\": \"172.17.0.7\", \"port\": 8084, \"tags\": { \"az\": \"us-central1-a\", \"canary\": false, \"load_balancing_weight\": 50 } }, { \"ip_address\": \"172.17.0.8\", \"port\": 8085, \"tags\": { \"az\": \"us-central1-a\", \"canary\": false, \"load_balancing_weight\": 50 } } ] }' http://localhost:8080/edsservice/myservice 测试：共计5个上游服务 while true; do curl http://localhost; sleep .5; printf '\\n'; done 删除端点 curl -X PUT --header 'Content-Type: application/json' --header 'Accept: application/json' -d '{ \"hosts\": [ ] }' http://localhost:8080/edsservice/myservice 断联 EDS 服务 重新注册 curl -X PUT --header 'Content-Type: application/json' --header 'Accept: application/json' -d '{ \"hosts\": [ { \"ip_address\": \"172.17.0.2\", \"port\": 8081, \"tags\": { \"az\": \"us-central1-a\", \"canary\": false, \"load_balancing_weight\": 50 } }, { \"ip_address\": \"172.17.0.5\", \"port\": 8082, \"tags\": { \"az\": \"us-central1-a\", \"canary\": false, \"load_balancing_weight\": 50 } }, { \"ip_address\": \"172.17.0.6\", \"port\": 8083, \"tags\": { \"az\": \"us-central1-a\", \"canary\": false, \"load_balancing_weight\": 50 } }, { \"ip_address\": \"172.17.0.7\", \"port\": 8084, \"tags\": { \"az\": \"us-central1-a\", \"canary\": false, \"load_balancing_weight\": 50 } }, { \"ip_address\": \"172.17.0.8\", \"port\": 8085, \"tags\": { \"az\": \"us-central1-a\", \"canary\": false, \"load_balancing_weight\": 50 } } ] }' http://localhost:8080/edsservice/myservice 测试 删除 EDS 服务 docker ps -a | awk '{ print $1,$2 }' | grep katacoda/eds_server | awk '{print $1 }' | xargs -I {} docker stop {}; docker ps -a | awk '{ print $1,$2 }' | grep katacoda/eds_server | awk '{print $1 }' | xargs -I {} docker rm {} 测试：请求全部正常 结论：即使Envoy与EDS服务器断开连接，它也继续对请求做出响应。 ","date":"2020-05-06","objectID":"/%E5%9F%BA%E4%BA%8Eapi%E7%9A%84%E5%8A%A8%E6%80%81%E8%B7%AF%E7%94%B1/:0:0","tags":["Envoy","实验"],"title":"基于API的动态路由","uri":"/%E5%9F%BA%E4%BA%8Eapi%E7%9A%84%E5%8A%A8%E6%80%81%E8%B7%AF%E7%94%B1/"},{"categories":["Envoy"],"content":"目标 如何使用 Envoy 提供的基于文件的动态配置 介绍 前面介绍了静态配置，使得在需要更改时很难重新加载配置。使用动态配置，进行更改时，Envoy将自动重新加载更改并将其应用于配置和流量路由。 Envoy supports different parts of the configuration as dynamic. The APIs available are: EDS: The Endpoint Discovery Service (EDS) API provides a way Envoy can discover members of an upstream cluster. This allows you to dynamically add and remove servers handling the traffic. CDS: The Cluster Discovery Service (CDS) API layers on a mechanism by which Envoy can discover upstream clusters used during routing. RDS: The Route Discovery Service (RDS) API layers on a mechanism by which Envoy can discover the entire route configuration for an HTTP connection manager filter at runtime. This would enable concepts such as dynamically changing traffic shifting and blue/green releases. LDS: The Listener Discovery Service (LDS) layers on a mechanism by which Envoy can discover entire listeners at runtime. SDS: The Secret Discovery Service (SDS) layers on a mechanism by which Envoy can discover cryptographic secrets (certificate plus private key, TLS session ticket keys) for its listeners, as well as the configuration of peer certificate validation logic (trusted root certs, revocations, etc). The value for configuration can come from the filesystem, REST-JSON or gRPC endpoints. 配置 envoy.yaml node:id:id_1cluster:testadmin:access_log_path:\"/dev/null\"address:socket_address:address:0.0.0.0port_value:9901dynamic_resources:cds_config:path:\"/etc/envoy/cds.conf\"lds_config:path:\"/etc/envoy/lds.conf\" lds.conf { \"version_info\": \"0\", \"resources\": [{ \"@type\": \"type.googleapis.com/envoy.api.v2.Listener\", \"name\": \"listener_0\", \"address\": { \"socket_address\": { \"address\": \"0.0.0.0\", \"port_value\": 10000 } }, \"filter_chains\": [ { \"filters\": [ { \"name\": \"envoy.http_connection_manager\", \"config\": { \"stat_prefix\": \"ingress_http\", \"codec_type\": \"AUTO\", \"route_config\": { \"name\": \"local_route\", \"virtual_hosts\": [ { \"name\": \"local_service\", \"domains\": [ \"*\" ], \"routes\": [ { \"match\": { \"prefix\": \"/\" }, \"route\": { \"cluster\": \"newTargetCluster\" } } ] } ] }, \"http_filters\": [ { \"name\": \"envoy.router\" } ] } } ] } ] }] } cds.conf { \"version_info\": \"0\", \"resources\": [{ \"@type\": \"type.googleapis.com/envoy.api.v2.Cluster\", \"name\": \"targetCluster\", \"connect_timeout\": \"0.25s\", \"lb_policy\": \"ROUND_ROBIN\", \"type\": \"EDS\", \"eds_cluster_config\": { \"service_name\": \"localservices\", \"eds_config\": { \"path\": \"/etc/envoy/eds.conf\" } } }, { \"@type\": \"type.googleapis.com/envoy.api.v2.Cluster\", \"name\": \"newTargetCluster\", \"connect_timeout\": \"0.25s\", \"lb_policy\": \"ROUND_ROBIN\", \"type\": \"EDS\", \"eds_cluster_config\": { \"service_name\": \"localservices\", \"eds_config\": { \"path\": \"/etc/envoy/eds1.conf\" } } }] } eds.conf { \"version_info\": \"0\", \"resources\": [{ \"@type\": \"type.googleapis.com/envoy.api.v2.ClusterLoadAssignment\", \"cluster_name\": \"localservices\", \"endpoints\": [{ \"lb_endpoints\": [{ \"endpoint\": { \"address\": { \"socket_address\": { \"address\": \"172.17.0.3\", \"port_value\": 80 } } } },{ \"endpoint\": { \"address\": { \"socket_address\": { \"address\": \"172.17.0.2\", \"port_value\": 80 } } } }] }] }] } eds1.conf { \"version_info\": \"0\", \"resources\": [{ \"@type\": \"type.googleapis.com/envoy.api.v2.ClusterLoadAssignment\", \"cluster_name\": \"localservices\", \"endpoints\": [{ \"lb_endpoints\": [{ \"endpoint\": { \"address\": { \"socket_address\": { \"address\": \"172.17.0.6\", \"port_value\": 80 } } } }, { \"endpoint\": { \"address\": { \"socket_address\": { \"address\": \"172.17.0.5\", \"port_value\": 80 } } } }] }] }] } 操作 ","date":"2020-05-06","objectID":"/%E5%9F%BA%E4%BA%8E%E6%96%87%E4%BB%B6%E7%9A%84%E5%8A%A8%E6%80%81%E8%B7%AF%E7%94%B1/:0:0","tags":["Envoy","实验"],"title":"基于文件的动态路由","uri":"/%E5%9F%BA%E4%BA%8E%E6%96%87%E4%BB%B6%E7%9A%84%E5%8A%A8%E6%80%81%E8%B7%AF%E7%94%B1/"},{"categories":["Envoy"],"content":"启动 Envoy docker run -p 81:10000 -v /root/envoy-1.12.2/examples/file-based-dr/:/etc/envoy -v /root/envoy-1.12.2/examples/file-based-dr/envoy1.yaml:/ect/envoy/envoy.yaml envoyproxy/envoy ","date":"2020-05-06","objectID":"/%E5%9F%BA%E4%BA%8E%E6%96%87%E4%BB%B6%E7%9A%84%E5%8A%A8%E6%80%81%E8%B7%AF%E7%94%B1/:0:1","tags":["Envoy","实验"],"title":"基于文件的动态路由","uri":"/%E5%9F%BA%E4%BA%8E%E6%96%87%E4%BB%B6%E7%9A%84%E5%8A%A8%E6%80%81%E8%B7%AF%E7%94%B1/"},{"categories":["Envoy"],"content":"启动上游服务 docker run -d katacoda/docker-http-server docker run -d katacoda/docker-http-server docker run -d katacoda/docker-http-server docker run -d katacoda/docker-http-server 测试 ","date":"2020-05-06","objectID":"/%E5%9F%BA%E4%BA%8E%E6%96%87%E4%BB%B6%E7%9A%84%E5%8A%A8%E6%80%81%E8%B7%AF%E7%94%B1/:0:2","tags":["Envoy","实验"],"title":"基于文件的动态路由","uri":"/%E5%9F%BA%E4%BA%8E%E6%96%87%E4%BB%B6%E7%9A%84%E5%8A%A8%E6%80%81%E8%B7%AF%E7%94%B1/"},{"categories":["Envoy"],"content":"访问 Envoy 结果：轮询访问上游服务 ","date":"2020-05-06","objectID":"/%E5%9F%BA%E4%BA%8E%E6%96%87%E4%BB%B6%E7%9A%84%E5%8A%A8%E6%80%81%E8%B7%AF%E7%94%B1/:0:3","tags":["Envoy","实验"],"title":"基于文件的动态路由","uri":"/%E5%9F%BA%E4%BA%8E%E6%96%87%E4%BB%B6%E7%9A%84%E5%8A%A8%E6%80%81%E8%B7%AF%E7%94%B1/"},{"categories":["Envoy"],"content":"更改 lds 路由配置 “route”: { “cluster”: “targetCluster” } =\u003e “route”: { “cluster”: “newTargetCluster” } 结果：更新配置后，无需重启 Envoy，访问新的上游服务，达到动态更新 注：Based on how Docker handles file inode tracking, sometimes the filesystem change isn’t triggered and detected. Force the change with the command mv lds.conf tmp; mv tmp lds.conf ","date":"2020-05-06","objectID":"/%E5%9F%BA%E4%BA%8E%E6%96%87%E4%BB%B6%E7%9A%84%E5%8A%A8%E6%80%81%E8%B7%AF%E7%94%B1/:0:4","tags":["Envoy","实验"],"title":"基于文件的动态路由","uri":"/%E5%9F%BA%E4%BA%8E%E6%96%87%E4%BB%B6%E7%9A%84%E5%8A%A8%E6%80%81%E8%B7%AF%E7%94%B1/"},{"categories":["Envoy"],"content":"目标 使用 Envoy 基于请求的 URL 代理流量到不同的Python服务 实验代码部署架构如图所示： Clone Envoy 仓库，并启动所有的容器 git clone https://github.com/envoyproxy/envoy.git 测试 ","date":"2020-05-06","objectID":"/envoy-%E5%89%8D%E7%AB%AF%E4%BB%A3%E7%90%86/:0:0","tags":["Envoy","实验"],"title":"Envoy 前端代理","uri":"/envoy-%E5%89%8D%E7%AB%AF%E4%BB%A3%E7%90%86/"},{"categories":["Envoy"],"content":"测试 Envoy 路由能力 ","date":"2020-05-06","objectID":"/envoy-%E5%89%8D%E7%AB%AF%E4%BB%A3%E7%90%86/:1:0","tags":["Envoy","实验"],"title":"Envoy 前端代理","uri":"/envoy-%E5%89%8D%E7%AB%AF%E4%BB%A3%E7%90%86/"},{"categories":["Envoy"],"content":"测试Envoy负载均衡能力 ","date":"2020-05-06","objectID":"/envoy-%E5%89%8D%E7%AB%AF%E4%BB%A3%E7%90%86/:2:0","tags":["Envoy","实验"],"title":"Envoy 前端代理","uri":"/envoy-%E5%89%8D%E7%AB%AF%E4%BB%A3%E7%90%86/"},{"categories":["Envoy"],"content":"通过管理端口查看相关信息 ","date":"2020-05-06","objectID":"/envoy-%E5%89%8D%E7%AB%AF%E4%BB%A3%E7%90%86/:3:0","tags":["Envoy","实验"],"title":"Envoy 前端代理","uri":"/envoy-%E5%89%8D%E7%AB%AF%E4%BB%A3%E7%90%86/"},{"categories":["Envoy"],"content":"目标 对比Envoy 和 Nginx 配置 Nginx配置 Nginx 配置三要素： 全局配置 Nginx 服务器，日志记录结构，Gzip 功能等 配置 Nginx 在 10000 端口接收对 one.example.com 主机的请求 配置如何处理到达不同 URL 的流量的目标地址 user www www; pid /var/run/nginx.pid; worker_processes 2; events { worker_connections 2000; } http { gzip on; gzip_min_length 1100; gzip_buffers 4 8k; gzip_types text/plain; log_format main '$remote_addr - $remote_user [$time_local] ' '\"$request\" $status $bytes_sent ' '\"$http_referer\" \"$http_user_agent\" ' '\"$gzip_ratio\"'; log_format download '$remote_addr - $remote_user [$time_local] ' '\"$request\" $status $bytes_sent ' '\"$http_referer\" \"$http_user_agent\" ' '\"$http_range\" \"$sent_http_content_range\"'; upstream targetCluster { 172.18.0.3:80; 172.18.0.4:80; } server { listen 10000; server_name one.example.com www.one.example.com; access_log /var/log/nginx.access_log main; error_log /var/log/nginx.error_log info; location / { proxy_pass http://targetCluster/; proxy_redirect off; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; } } } Envoy 配置 Envoy 配置四要素： Listeners：定义了 Envoy 如何接收传入的请求。当前，Envoy 仅支持基于 TCP 的监听器，建立连接后，Envoy 会将流量传递到一组过滤器进行处理 Filters：处理入站和出站数据。比如Gzip过滤器，该过滤器会将数据发送到客户端前压缩数据 Routers: 将流量转发到所需的目的地（定义为集群） Clusters: 定义流量的目的端点和配置设置，如负载均衡策略 static_resources:#静态API配置listeners:#监听器配置- name:listener_0address:socket_address:protocol:TCPaddress:0.0.0.0port_value:10000filter_chains:#过滤器链配置- filters:- name:envoy.http_connection_managertyped_config:\"@type\": type.googleapis.com/envoy.config.filter.network.http_connection_manager.v2.HttpConnectionManagerstat_prefix:ingress_http##连接管理器发出统计信息时使用的可读性前缀access_log:- name:envoy.file_access_logconfig:path:\"/dev/stdout\"format:'[%START_TIME%] \"%REQ(:METHOD)% %REQ(X-ENVOY-ORIGINAL-PATH?:PATH)% %PROTOCOL%\" %RESPONSE_CODE% %RESP(X-ENVOY-UPSTREAM-SERVICE-TIME)% \"%REQ(X-REQUEST-ID)%\" \"%REQ(:AUTHORITY)%\" \"%UPSTREAM_HOST%\"\\n'#json_format: {\"protocol\": \"%PROTOCOL%\", \"duration\": \"%DURATION%\", \"request_method\": \"%REQ(:METHOD)%\"}route_config:name:local_routevirtual_hosts:- name:local_service#domains: [\"*\"] ##虚拟主机配置domains:- \"one.example.com\"- \"www.one.example.com\"routes:##如果虚拟主机匹配，则检查路由- match:prefix:\"/\"route:cluster:targetCluster##处理匹配的路由的集群名称http_filters:- name:envoy.router##该过滤器允许Envoy在处理请求时适应和修改请求clusters:##当请求与过滤器匹配时，该请求将传递到集群。- name:targetClusterconnect_timeout:0.25stype:STRICT_DNS# Comment out the following line to test on v6 networksdns_lookup_family:V4_ONLYlb_policy:ROUND_ROBINhosts:[{socket_address:{address: 172.17.0.4, port_value:80}},{socket_address:{address: 172.17.0.5, port_value:80}}] 测试：通过 Envoy 流量转发，轮询的负载均衡到后端两个服务 按照如上Envoy配置，启动 Envoy 容器。后端两个服务未启动时，返回503，属于正常现象。 启动后端两个服务 通过 Envoy 流量转发，轮询的负载均衡到后端两个服务 ","date":"2020-05-06","objectID":"/envoy-vs-nginx/:0:0","tags":["Envoy","实验"],"title":"Envoy VS Nginx","uri":"/envoy-vs-nginx/"},{"categories":["Envoy"],"content":"目标 配置Envoy代理转发流量到外部网址 通过控制流量目的地址，演示基于路径的路由 准备 Envoy 的配置文件 如下所示，文件名 baidu_com_proxy.v2.yaml。 admin:##Envoy提供的管理视图，可以查看配置，统计信息，日志和其他内部Envoy数据access_log_path:/tmp/admin_access.logaddress:socket_address:protocol:TCPaddress:0.0.0.0port_value:9901static_resources:#静态API配置listeners:#监听器配置- name:listener_0address:socket_address:protocol:TCPaddress:0.0.0.0port_value:10000filter_chains:#过滤器链配置，路由所有流量到www.baidu.com- filters:- name:envoy.http_connection_managertyped_config:\"@type\": type.googleapis.com/envoy.config.filter.network.http_connection_manager.v2.HttpConnectionManagerstat_prefix:ingress_http##连接管理器发出统计信息时使用的可读性前缀route_config:name:local_routevirtual_hosts:- name:local_servicedomains:[\"*\"]##虚拟主机配置routes:##如果虚拟主机匹配，则检查路由- match:prefix:\"/\"route:host_rewrite:www.baidu.com##更改HTTP请求的入站主机头cluster:service_baidu##处理匹配的路由的集群名称http_filters:- name:envoy.router##该过滤器允许Envoy在处理请求时适应和修改请求clusters:##当请求与过滤器匹配时，该请求将传递到集群。- name:service_baiduconnect_timeout:0.25stype:LOGICAL_DNS# Comment out the following line to test on v6 networksdns_lookup_family:V4_ONLYlb_policy:ROUND_ROBINload_assignment:cluster_name:service_baiduendpoints:- lb_endpoints:- endpoint:address:socket_address:address:www.baidu.comport_value:443transport_socket:name:envoy.transport_sockets.tlstyped_config:\"@type\": type.googleapis.com/envoy.api.v2.auth.UpstreamTlsContextsni:www.baidu.com 启动 Envoy 挂载配置文件 baidu_com_proxy.v2.yaml 到 /etc/envoy/envoy.yaml。 docker run --name=proxy-with-admin -d -p 9901:9901 -p 10000:10000 -v $(pwd)/baidu_com_proxy.v2.yaml:/etc/envoy/envoy.yaml envoyproxy/envoy:latest 测试 curl localhost:10000 管理控制台访问地址：http://172.16.99.100:9901/ （注：后续测试宿主机地址都为172.16.99.100） ","date":"2020-04-30","objectID":"/hello-envoy/:0:0","tags":["Envoy","实验"],"title":"Hello Envoy","uri":"/hello-envoy/"},{"categories":null,"content":"  LoveIt 是一个由 Dillon 开发的简洁、优雅且高效的 Hugo 博客主题。 它的原型基于 LeaveIt 主题 和 KeepIt 主题。 Hugo 主题 LoveItHugo 主题 LoveIt \" Hugo 主题 LoveIt ","date":"2019-08-02","objectID":"/about/:0:0","tags":null,"title":"关于 LoveIt","uri":"/about/"},{"categories":null,"content":"特性 性能和 SEO  性能优化：在 Google PageSpeed Insights 中， 99/100 的移动设备得分和 100/100 的桌面设备得分  使用基于 JSON-LD 格式 的 SEO SCHEMA 文件进行 SEO 优化  支持 Google Analytics  支持 Fathom Analytics  支持搜索引擎的网站验证 (Google, Bind, Yandex and Baidu)  支持所有第三方库的 CDN  基于 lazysizes 自动转换图片为懒加载 外观和布局 / 响应式布局 / 浅色/深色 主题模式  全局一致的设计语言  支持分页  易用和自动展开的文章目录  支持多语言和国际化  美观的 CSS 动画 社交和评论系统  支持 Gravatar 头像  支持本地头像  支持多达 64 种社交链接  支持多达 28 种网站分享  支持 Disqus 评论系统  支持 Gitalk 评论系统  支持 Valine 评论系统  支持 Facebook 评论系统  支持 Telegram comments 评论系统  支持 Commento 评论系统  支持 Utterances 评论系统 扩展功能  支持基于 Lunr.js 或 algolia 的搜索  支持 Twemoji  支持代码高亮  一键复制代码到剪贴板  支持基于 lightgallery.js 的图片画廊  支持 Font Awesome 图标的扩展 Markdown 语法  支持上标注释的扩展 Markdown 语法  支持分数的扩展 Markdown 语法  支持基于 $ \\KaTeX $ 的数学公式  支持基于 mermaid 的图表 shortcode  支持基于 ECharts 的交互式数据可视化 shortcode  支持基于 Mapbox GL JS 的 Mapbox shortcode  支持基于 APlayer 和 MetingJS 的音乐播放器 shortcode  支持 Bilibili 视频 shortcode  支持多种注释的 shortcode  支持自定义样式的 shortcode  支持自定义脚本的 shortcode  支持基于 TypeIt 的打字动画 shortcode  支持基于 Smooth Scroll 的滚动动画  支持基于 cookieconsent 的 Cookie 许可横幅 … ","date":"2019-08-02","objectID":"/about/:0:1","tags":null,"title":"关于 LoveIt","uri":"/about/"},{"categories":null,"content":"许可协议 LoveIt 根据 MIT 许可协议授权。 更多信息请查看 LICENSE 文件。 LoveIt 主题中用到了以下项目，感谢它们的作者： normalize.css Font Awesome Simple Icons Animate.css Smooth Scroll autocomplete.js Lunr.js algoliasearch lazysizes object-fit-images Twemoji lightgallery.js clipboard.js Sharer.js TypeIt $ \\KaTeX $ mermaid ECharts Mapbox GL JS APlayer MetingJS Gitalk Valine cookieconsent ","date":"2019-08-02","objectID":"/about/:0:2","tags":null,"title":"关于 LoveIt","uri":"/about/"}]